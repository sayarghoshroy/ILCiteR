{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8416d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cbc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 1\n",
    "datasets = ['peerread', 'acl', 'refseer', 'arxiv']\n",
    "dataset = datasets[dataset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef5869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./' + dataset + '/' + 'papers.json', 'r+') as f:\n",
    "    papers = json.load(f)\n",
    "    \n",
    "with open('./' + dataset + '/' + 'contexts.json', 'r+') as f:\n",
    "    contexts = json.load(f)\n",
    "\n",
    "mapping_types = ['train', 'val', 'test']\n",
    "\n",
    "with open('./' + dataset + '/' + mapping_types[2] + '.json', 'r+') as f:\n",
    "    raw_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ec741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    tokens = [token for token in nltk.word_tokenize(text) if token != ' '\n",
    "                                                          and token != 'OTHERCIT'\n",
    "                                                          and token != 'mcOTHERCIT']\n",
    "    return tokens\n",
    "\n",
    "def isolate_sentence(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    main_sent = ''\n",
    "    for sent in sents:\n",
    "        if 'TARGETCIT' in sent:\n",
    "            main_sent = sent\n",
    "            break\n",
    "            \n",
    "    tokens = [token for token in nltk.word_tokenize(main_sent) if token != ''\n",
    "                                                               and token != 'TARGETCIT'\n",
    "                                                               and token != 'OTHERCIT']\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6021b5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9585/9585 [00:03<00:00, 3035.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "\n",
    "def fill_test(datapoints, papers, contexts):\n",
    "    global test\n",
    "    for unit in tqdm.tqdm(datapoints):\n",
    "        context_id = unit['context_id']\n",
    "        paper_ids = unit['positive_ids']\n",
    "\n",
    "        paper_node = papers[paper_ids[0]]\n",
    "        context_text = contexts[context_id]['masked_text']\n",
    "\n",
    "        blob = {}\n",
    "        blob['cite_span'] = tokens_to_string(isolate_sentence(context_text))\n",
    "        blob['cite_context'] = tokens_to_string(process(context_text))\n",
    "        blob['raw_context_id'] = context_id\n",
    "        blob['paper_id'] = paper_ids[0]\n",
    "\n",
    "        blob['paper'] = paper_node\n",
    "\n",
    "        test.append(blob)\n",
    "    \n",
    "fill_test(raw_test, papers, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7edd4671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 39771/39771 [00:00<00:00, 2940345.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(index):\n",
    "    # Create Candidate Citation Spans from constructed index\n",
    "    candidate_texts = []\n",
    "    \n",
    "    for blob in tqdm.tqdm(index):\n",
    "        candidate_texts.append(blob['cite_context'])\n",
    "    \n",
    "    return candidate_texts\n",
    "\n",
    "with open('./' + dataset + '/' + 'index.json', 'r+') as f:\n",
    "    index = json.load(f)\n",
    "    \n",
    "candidates = get_candidates(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ffe20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e and target languages are annotated with a coarse parts-of-speech tagset which is shared across languages . Such tagsets are commonly used in multilingual parsing ( ; McOTHERCIT ; Søgaard , 2011 ; TARGETCIT . The key feature of our model is a two-tier approach that separates the selection of dependents from their ordering : 1 . Selection Component : Determines the dependent tags given the parent tag . 2 . Ord',\n",
       " 'ms with a brevity penalty ) is computed with human abstracts as reference . BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems ( TARGETCIT ; ) . 4We use SVMlight ( ) with RBF kernel by default parameters for SVM-based classifiers and regressor . 5The four types of meetings in AMI are : project kick-off ( 35 meetings ) , function',\n",
       " 'ng the sets of elementary dependency triples ( Oepen and Lønning , 2006 ) extracted from the returned and gold MRS . These annotations are similar in spirit to those used in the PARC 700 Dependency Bank ( TARGETCIT and other semantic dependency evaluation schemes . • Unlabeled Elementary Dependencies ( UED ) - identical to LED , except ignoring all labeling information other than the input positions involved . • Leaf',\n",
       " 'ase that is semantically equivalent , novel , or more/less informative with respect to the content of the other phrase . We set this problem as a variant of the Textual Entailment ( TE ) recognition task ( TARGETCIT b ; ) . We build an entailment graph for each topic cluster , where nodes are the extracted phrases and edges are the entailment relations between nodes . Given two phrases ( ph1 and ph2 ) , we aim a',\n",
       " 'lts are suggestive , they remain preliminary . A more detailed follow-up will need to look at the specific realization of the mentions and the kind of local coherence relations that link them ( , TARGETCIT , and to investigate the different aspects of referential chains with larger corpora and more varying genres . 3 . MT Success at Conveying Cohesion To evaluate the impact of this difference in expressed',\n",
       " 'r rules without any requirement for linguistic annotation of the training data . In subsequent work , the approach has been extended to incorporate linguistic annotation on the target side ( as in SAMT ( TARGETCIT ) or on both sides ( ) . In contrast , GHKM places target-side syntactic structure at the heart of the rule extraction process , producing extended tree transducer rules that map between strings a',\n",
       " 'g use of similar surface features . To the best of our knowledge , the only piece of work to have gone beyond sentence boundaries and tackle the problem of article-wide E-E temporal classification is by TARGETCIT . Making use of integer linear programming ( ILP ) , they built a joint inference model which is capable of classifying temporal relationships between any event pair within a given document . They also sh',\n",
       " 'T ) , semantic similarity computed from online resources ( ) , named entity type features , gender and number match using the dataset of , and features from unsupervised clusters ( ; TARGETCIT . In this section , we consider the following subset of these information sources : • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from • Named entity types',\n",
       " 'icular brand of political idealism . The term subjectivity is introduced by . Work on subjectivity in computational linguistics is initially due to Wiebe , Wilson , and collaborators ( Wiebe 1994 ; TARGETCIT ; ; Wilson 2008 ; ) and focuses on learning subjectivity from corpora . As Wiebe et al . ( 2004 , page 279 ) put it : Subjective language is language used to express private states in the con',\n",
       " 'variety of applications , such as online monitoring of mood ( ) , the classification of opinions or comments ( ) and their extraction ( Hu an ) and the semantic analysis of texts ( TARGETCIT . In ( ) , a bilingual lexicon and a manually translated parallel corpus are used to generate a sentence classifier according to their level of subjectivity for Romanian . Although many recent st',\n",
       " 'P12 : This dataset was distributed by . It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11 ( TARGETCIT . These MT systems range from online and customised SMT systems to commercial rule-based systems . Translations were post-edited by humans while time was recorded . The labels are the number of seconds',\n",
       " 'on of dependency length in English , German and Chinese has shown that rigidity of word order , consistency of dependency direction , and language type ( isolating , inflecting ) impact on a language ’ s MDD . TARGETCIT , and have looked into the relationship between dependency length and adjacency . These preliminary findings are difficult to interpret and more work needs to be done on this in the f',\n",
       " '-of-speech ( POS ) tag and a word . of s0 into stack1 . Dynamic programming merges equivalent states in the same step if they have the same feature values . We add the feature templates shown in Table 1 to TARGETCIT ’ s feature templates . Dynamic programming not only makes the shiftreduce parser with beam search more efficient but also produces a packed forest that encodes an exponential number of dependency trees',\n",
       " 's. 2 Related Work Event extraction techniques have largely focused on detecting event “ triggers ” with their arguments for extracting role fillers . Classical methods are either pattern-based ( ; TARGETCIT b ; ) or classifierbased ( e.g. , ( ) ) . Recently , several approaches have been proposed to address the insufficiency of using only local context to identify role fillers . Some approaches l',\n",
       " 's/ mt/2009/ResultsRelease/indexISC.html outputs were detokenized before computing case insensitive BLEU scores . Statistical significance was computed for each pairwise comparison using bootstrapping ( TARGETCIT . Aligner Decode Oracle tune test tune test GIZA 60.06 57.95 75.06 74.47 iTER 59.74 58.63† 73.84 73.20 iTERp 60.18 59.05† 76.43 75.58 iIHMM 60.51 59.27†‡ 76.50 76.17 iITGp 60.65 59.37†‡ 76.53 76.05 Ta',\n",
       " 'ence , in which we must select a parse for a sentence given the learned model . Previous work has shown that highquality unlexicalized PCFGs can be learned from a treebank , either by manual annotation ( TARGETCIT or automatic state splitting ( ) . In particular , we demonstrated in that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs ( ) . We adopted here a multil',\n",
       " 'longing to named-entity types such as persons , organizations and locations ( ) . Generic NER rules have been shown to work reasonably well-out-of-the-box , and with further domain customization ( TARGETCIT b ) , achieve quality surpassing state-of-the-art results . Table 1 System Dataset FQ=1 Generic Customized GATE ACE2002 57.8 82.2 ACE 2005 57.32 88.95 SystemT CoNLL 2003 64.15 91.77 Enron 76.53 85.29 Tab',\n",
       " 'pture stylistic patterns in sentence structure , as would targeting syntactic structures in syntax-based translation . A weakness of our approach is its computational expense ; by contrast , the method of TARGETCIT obtains diverse translations more efficiently by extracting them from a single decoding of an input sentence ( albeit with a wide beam ) . We expect their ideas to be directly applicable to our setting i',\n",
       " 'both the Explicit/Non-Explicit feature source and the intra-cell feature source are joined together , it also produces bigram features such as E.Comp.Arg2+-+Temp.Arg1 . 4.3 Predicting Readability Scores TARGETCIT used the SVMlight ( ) package with the preference ranking configuration . To train the model , each source text and one of its permutations form a training pair , where the source text is given a',\n",
       " 'We follow the same experiment set-up as DMV , and report the results on the section 23 , using the best grammar tested on the development set ( section 22 ) from 5 random runs for each algorithm . We adopt TARGETCIT ’ s methods to process the data : right binarising and replacing infrequent words with the generic unknown word marker for English , and to initialise : adding 1 % randomness to the parameters B0 to start',\n",
       " 'are typically run on news articles and assume an adult audience . They show that word co-occurrence ( ) , subtopic structure ( ) , discourse relations ( ) and coreference patterns ( TARGETCIT learn from large corpora can be used to predict coherence . But prior metrics are not proposed as unique to any genre . Some metrics using word patterns ( ) are domaindependent in that they requi',\n",
       " 'roaches . Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations , as introduced by Hasegawa et al . ( TARGETCIT ( this is the system we use in this work as our baseline ) . Hasegawa et al . used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering ( HAC ) wi',\n",
       " 'of STS test data is transformed into a latent vector using Equation 2 . Then sentence pair similarity is computed by the cosine similarity of the two latent vectors . We employ the parameters used in ( TARGETCIT b ) ( A = 20 , wm = 0.01 ) . 5.2 Results Table 3 summarizes the overall performance of WTMF on the concatenation of 5 data sets followed by the corresponding rank among all participating systems.5 There ar',\n",
       " 'g can provide accuracy benefits ( about 2 % ) in domain adaptation , although these results are not directly comparable to ours because they refer to constituency parsing rather than dependency parsing . ( TARGETCIT obtain even better results ( 5 % f-score gain ) using a more sophisticated form of self-training , involving n-best generative parsing and discriminative reranking . ( ) obtain similar gains ( about',\n",
       " 'to 2 phrase-tables , so we did not use it in this evaluation . 2.2 Modified Moore-Lewis Filtering In last year ’ s evaluation ( ) we had some success with modified Moore-Lewis filtering ( ; TARGETCIT of the training data . This year we conducted experiments in most of the language pairs using MML filtering , and also experimented using instance weighting ( ) using the ( exponential of ) the MML',\n",
       " 'verview of applications and speficic models ) . In recent years , Semantic Vector Spaces have also seen applications in more traditional domains of linguistics , like diachronic lexical studies ( ; TARGETCIT , or the study of lexical variation ( ) . In this paper , we want to show how Semantic Vector Spaces can further aid the linguistic analysis of lexical semantics , provided that they are made acce',\n",
       " 'consistent increase in BLEU score ( ) . 2 French-English System In addition to the baseline system , we also trained separate systems for News and Non-News genres for applying ensemble decoding ( TARGETCIT . The news genre system was trained only using the news-commentary corpus ( about 137K sen1Alternately systems add sentence boundary markers ( < s > and < /s > ) to the training data so that they are explici',\n",
       " 'lasses obtained in unsupervised semantic role labeling tasks . We then evaluate them with a classical metric used to evaluate these classes in unsupervised SRL ( as done for instance in ( ) and ( TARGETCIT ) : purity and collocation . Purity measures the degree to which each cluster contains instances that share the same gold class , while collocation measures the degree to which instances with the same go',\n",
       " 'ier with the additional label NO-REL . Hernault et al . obtained 93.8 % F-score for EDU segmentation , 85.0 % accuracy for structure classification , and 66.8 % accuracy for 18-class relation classification . TARGETCIT attempted to recognize implicit discourse relations ( discourse relations which are not signaled by explicit connectives ) in PDTB by using four classes of features — contextual features , constituent pa',\n",
       " 'mbination of data or model parameters from a set of source languages , as languages tend to share varied typological traits ; this critical insight is discussed further in §4 . To account for this issue , TARGETCIT recently introduced a novel generative model of dependency parsing , in which the generative process is factored into separate steps for the selection of dependents and their ordering . The parameters u',\n",
       " 'e rewrite the c distributions as c ( ej | e0i , sign ( j − i ) ) and denote this model by CCV1 . 578 We note that we can also include constraints in the sum over possible parents and still preserve concavity . TARGETCIT found that adding parentchild constraints to a grammar induction system can improve performance dramatically . We employ one simple rule : roots are likely to be verbs.2 We modify CCV1 to restrict the s',\n",
       " 'hose corpora with different annotation format , there still has a well-performed parser to fit the specific structure for the data . In this work , we adopt an existing powerful parser , Stanford parser ( TARGETCIT , which has shown its effectiveness in English . We make the necessary modifications for parsing Chinese and apply it to the shared task . In this evaluation , we use TCT Treebank as the developing and e',\n",
       " 'ng relations between common nouns ( rather than just namedentities ) —future work should explore this . 8 Related Work Our work builds on recent progress in monolingual distributional semantics ( ; TARGETCIT by 688 Source Le Princess Elizabeth arrive a ` Dunkerque le 3 aoˆut 1999 Machine translation 1-best Le Princess Elizabeth is to manage to Dunkirk on 3 August 1999 Reranked translation The Princess Eliz',\n",
       " 'han 100M tokens , using the approach by . These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition ( TARGETCIT . As features in the T/V classification of a given sentence , we simply count for each class the number of tokens in this class present in the current sentence . For illustration , Table 2 shows the thre',\n",
       " 'ed from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity ( ) , paraphrase recognition , textual entailment ( ) , and pseudoword generation ( TARGETCIT . Formally , we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present',\n",
       " 'number of signals that attempt to gauge translation quality : the translation models attempt to capture fidelity of translation ; language models focus on fluency ; etc . We use techniques such as MERT ( TARGETCIT and PRO ( ) to tune the relative weight of these signals . Why not tune indicators from linguists in the same manner ? When our linguists mark a mapping as +Like or +DontLike , we track that throu',\n",
       " '4 , which gave the best results on the development set . The UAS improves by 0.24 when we do joint tagging and parsing . The POS accuracy improves slightly by 0.12 Parser TLAS UAS LAS POS McOTHERCIT 90.9 TARGETCIT 91.5 92.1 92.1 93.04 92.9 93.26 † 93.16 † 93.5 † 93.79 Baseline ( k = 1 ) , b1 = 40 89.42 92.79 91.71 97.28 Best dev setting , b1 =',\n",
       " '8 bit Morris-style counter of base 2 . Larger reservoir sizes lead to better approximation , at higher cost in bits . to provide a streaming extension to the Bloom-filter based count-storage mechanism of TARGETCIT a ) and . See ( ) for a detailed analysis of Morrisstyle counting . 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy',\n",
       " 'uated translation output using case-insensitive IBM BLEU ( ) . Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from TARGETCIT , including k-best lists of size 1500 and 25 training iterations.11 MERT uses k-best lists of size 100 and was run to convergence . For both MERT and PRO , previous iterations ’ k-best lists were merged',\n",
       " 'ther hand , the two arguments of an Expansion relation are relatively likely to belong to the same polarity states ( e.g. , PositivePositive or Neutral-Neutral ) . Discourse relation recognition ( ; TARGETCIT and sentiment analysis ( ) have attracted much attention recently . Due to the limitation of the resources , the research on Chinese discourse relation analysis is relatively rare . In our previou',\n",
       " '. Our approach uses label propagation ( LP ) ( ) to infer tag distributions on unlabeled tokens . We then apply a novel weighted variant of the model minimization procedure originally developed by TARGETCIT to estimate sequence and word-tag frequency information from an unlabeled corpus by approximating the minimal set of tag bigrams needed to explain the data . This combination of techniques turns a tiny',\n",
       " 'of the most significant challenges : coordination scope errors , and verb argument selection . To see which tagging confusions contribute to which error reductions , we adapt the POS ablation approach of TARGETCIT . We consider the POS tag pairs shown in Table 3 . To isolate the effects of each confusion we start from the gold tags and introduce the output of the Stanford tagger whenever it returns one of the tw',\n",
       " 'ms “ Offizielle Prognosen sind von ” , then “ Prognosen sind von nur ” , etc . 5 Experiments 5.1 Setup The baseline system for our experiments is the syntax-based component of the Moses opensource toolkit of TARGETCIT and . We use linguistic syntactic annotation on both the source and the target language side ( tree-to-tree ) . Our contrastive system is the eMBOT-based translation system presented here . We pro',\n",
       " 'zes the score of an entire action sequence . A local learner trains a classifier which distinguishes between single actions . 4 As shown in ( ) , most transitionbased dependency parsers ( ; TARGETCIT ; ) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence . Features of ( ) for p in pi-1 , pi , pi+1 wp-vlp , wp-vrp , tp-vlp , tp-vrp ,',\n",
       " 't formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora ( ) . Current methods , including phrase-based ( ; TARGETCIT and hierarchical models ( ) , typically start by word-aligning a bilingual parallel corpus ( ) . They extract multiword phrases that are consistent with the Viterbi word alignments and use',\n",
       " 'R is in fact the first framework to address the non-net structures . In spite of that , CFUR is quite restricted and does not allow for adding new constraints after semantic composition . In recent work , TARGETCIT suggest using Regular Tree Grammars for scope underspecification , a probabilistic version of which could be used to find the best reading . The framework goes beyond the formalisms discussed in this pa',\n",
       " 'complete dictionaries ) . We thus build on our previous approach , which exploits extremely sparse , human-generated annotations that are produced without knowledge of which words appear in the test set ( TARGETCIT . This approach generalizes a small initial tag dictionary to include unannotated word types appearing in raw data . It estimates word/tag pair and tag-transition frequency information using modelminim',\n",
       " 'her work on the distributional semantics of grammatical elements.3 2 Composition models Interest in compositional DSMs has skyrocketed in the last few years , particularly since the influential work of TARGETCIT ; 2009 ; 2010 ) , who proposed three simple but effective composition models . In these models , the composed vectors are obtained through componentwise operations on the constituent vectors . Given input v',\n",
       " 'ms like { diseases , damage , contamination } . 2.2 Knowledge Extraction Procedure For our study , we have used the general Webbased class instance and relation extraction framework introduced by ( ; TARGETCIT . The procedure is minimally supervised and achieves high accuracy of the produced extractions . Term Extraction : To initiate the learning process , the user must provide as input a seed term Y and a re',\n",
       " 'he best case work in O ( n3 ) time complexity with O ( n3 ) memory use for sentences with of length n. We believe that the output of this model can also improve DMV.2 In addition , we use punctuation clues ( TARGETCIT c ) , tying feature similarity in the transition system configuration , and 1The other study is in that is for constituency parsing ( phrase structure extraction ) . 2For the effect of model initia',\n",
       " 's a sequence of words grammatical or not ? We evaluate on two tasks of differing granularity : the first , a coarse-grain classification , follows ; the other , a fine-grain analogue , is built upon TARGETCIT . 4.1 Datasets For the coarse-grained task , we use the BLLIP5- inspired dataset , as in , which discriminates between BLLIP sentences and KneyserNey trigram generated sentences ( of equal length',\n",
       " 'performs the mapping . volve tree transformations either between two trees or a tree and a string . The tree transducer , a formalism from automata theory which has seen interest in machine translation ( TARGETCIT ; ) and has potential applications in many other areas , is well suited to formalizing such tree transformation based models . Yet , while many semantic parsing systems resemble the formalism , ea',\n",
       " ': //nlp.cs.berkeley.edu . these will be necessary for an automatic system to fully solve the problem . Acknowledging this complexity , coreference systems , either learning-based ( ) or rule-based ( TARGETCIT ; ) , draw on diverse information sources and complex heuristics to resolve pronouns , model discourse , determine anaphoricity , and identify semantically compatible mentions . However , this leads',\n",
       " 's of records , logical form , and expert system knowledge bases ( ) . A variety of concept-totext generation systems have been engineered over the years , with considerable success ( e.g. , , TARGETCIT ) . Unfortunately , it is often difficult to adapt them across different domains as they rely mostly on handcrafted components . In this paper we present a data-driven approach to concept-to-text generat',\n",
       " 'h Wikipedia has been previously used for many text simplification approaches ( ) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception ( TARGETCIT b ; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2 : Summary counts for the simple-normal article aligned data set consisting of 60K article pairs . ) . We d',\n",
       " 'better performance . came up with Conditional Random Fields ( CRFs ) , a probabilistic model for segmenting and labeling sequence data and showed it to be successful with POS tagging experiment . TARGETCIT used CRFs for shallow parsing tasks such as noun phrase chunking . McOTHERCIT did named entity tagging using CRFs , feature induction and web enhanced lexicons . CRFs based Named Entity tagging was done',\n",
       " 'on and decoding . Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents ( ) . The syntax-augmented translation model of TARGETCIT annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs . This results in exacerbated data sparsity pro',\n",
       " ', and the sentences are aligned using ( ) . As illustrated in Fig . 2 , the text in one language is fed to the Bing translator , Google Translate , and an in-house SMT system4 implemented based on ( TARGETCIT by ourselves for obtaining sentences translated by SMT systems . Due to a severe limitation on the number of requests to the APIs , we randomly subsample sentences before sending them to these SMT syste',\n",
       " 'y non-syntactic phrases . Then , we annotate alignment information to the phrasal nodes labeled dependency tree T , as shown in Figure 4 . For description convenience , we make use of the notion of spans ( TARGETCIT ; ) . Given a node n in the source phrasal nodes labeled T with word alignment information , the spans of n induced by the word alignment are consecutive sequences of words in the target sentenc',\n",
       " 'nually-aligned parallel text ( ) . Although manually-aligned data is very valuable , it is only available for a small number of language pairs . Other models are unsupervised like the IBM models ( TARGETCIT ; Grac¸a et al. , 2010 ; ) , but have not been as widely adopted as GIZA++ has . In this paper , we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models , is as',\n",
       " 'ework we are proposing in this paper . We adopt a neo-Davidsonian approach to semantics by a formalism that bears similarity to existing frameworks such as ( R ) MRS ( Robust Minimal Recursion Semantics ) ( TARGETCIT . However , this paper is intended to explore what architecture is minimally required to augment the PLTAG syntactic framework , so we do not adopt these existing frameworks wholesale . Our examples such',\n",
       " '( 1004 seconds to complete decoding the 2489 sentences , versus 2551 seconds with the standard setup ) . 3 J-PRO : Pairwise Ranking Optimization in Joshua Pairwise ranking optimization ( PRO ) proposed by ( TARGETCIT is a new method for discriminative parameter tuning in statistical machine translation . It is reported to be more stable than the popular MERT algorithm ( ) and is more scalable with regard to',\n",
       " 'our strategy is sufficient for many applications , such as chunking or named entity recognition ; many applications such as sentiment analysis ( , §4.2.3 ) , open information extraction ( ; TARGETCIT , and information retrieval ( ) use POS 14See “ Tense and aspect ” examples in http : //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example , wtf has compositional behavior in',\n",
       " 'rther methods is our toppriority goal . In particular , several distributional models of word meaning in context share important similarities with composition models , and we plan to add them to DISSECT . TARGETCIT show , for example , that well-performing , simplified variants of the method in and Erk and Pad´o ( 2008 ) can be reduced to relatively simple matrix operations , making them particularly suitable',\n",
       " 't the number of phrases is far larger than the number of words , this approach suffers from sparsity and computational problems , as it requires training a classifier for each entry of the phrase table . TARGETCIT introduced a way to modify the rule weights of a hierarchical translation system to reflect the predictions of their WSD system . While their approach and ours are built on the same intuition ( an adapt',\n",
       " 'n , and over 300 patterns for security.9 These patterns were added to the system through an elaborate pattern-acquisition process , where semi-supervised pattern acquisition for English text was used , ( TARGETCIT ; ) , to bootstrap many pattern candidates from raw text based on a small set of seed patterns ; the candidates were subsequently checked manually and included in the system . Many of these patte',\n",
       " 'ries for a specific database , but means the semantic representations do not generalize to other datasets . There have been several attempts to annotate larger corpora with semantics—such as Ontonotes ( TARGETCIT or the Groningen Meaning Bank ( ) . These typically map words onto senses in ontologies such as WordNet , VerbNet ( ) and FrameNet ( ) . However , limitations of these ontologies mean',\n",
       " 'ed between different layers of linguistic analysis , contrary to evidence that the layers interact ( ) . As an alternative , one can take an integrated “ lexicalized approach , ” following ( ; TARGETCIT ; ) , in which each lexical unit ’ s syntactic , semantic , and pragmatic contributions are represented as a lexical entry . Lexicalized approaches presume that lexical entries can be designed to co',\n",
       " 'nlabeled attachment score ( UAS ) and 11 % relative error reduction . 2 Model The projected lexical features that we propose in this work are based on lexicalized versions of features found in MSTParser ( TARGETCIT , an edge-factored discriminative parser . We take MSTParser to be our underlying parsing model and use it as a testbed on which to evaluate the effectiveness of our method for various data conditions .',\n",
       " 'the issue of the correlation between BLEU and MAP in Section 4 . Both of the proposed approaches rely on the phrase-based SMT ( PBMT ) model ( ) implemented in the Open Source SMT toolkit MOSES ( TARGETCIT . 3.1 Tuning for genre adaptation First , we propose to adapt the PBMT model by tuning the model ’ s weights on a parallel set of queries . This approach addresses the first aspect of the problem , which i',\n",
       " 'approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process ( ) , or by using native language-specific error statistics ( TARGETCIT b ; ) . Both approaches show improvements over the models trained solely on well-formed native text . Training a model on error-tagged non-native text is expensive , as it requires large amounts o',\n",
       " 'the extracted sentences may confuse readers . argued that when the sentences of a text are randomly ordered , the text becomes difficult to understand , as its discourse structure is disturbed . TARGETCIT validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures . This prior work leads u',\n",
       " 't improvements have been made in the field of language processing in general , and improved learning techniques have pushed the state of the art in coreference resolution forward ( ; McOTHERCIT ; TARGETCIT ; ) . Researchers have continued to find novel ways of exploiting ontologies such as WordNet . Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited ( OTH',\n",
       " 'erization by collecting and exploiting such common-sense knowledge . Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes , e.g . ( TARGETCIT ; ; Pas¸ca and ) . We learn the distinguishing attributes of different demographic groups ( Section 3 ) , and then automatically assign users to these groups whenever they refer to a disti',\n",
       " 's. 5 Related Work Sense similarity measures have been the core component in many unsupervised WSD systems and lexical semantics research/applications . To date , elesk is the most popular such measure ( TARGETCIT ; ) . Sometimes people use jcn to obtain similarity of noun-noun and verb-verb pairs ( ) . Our similarity measure wmfvec exploits the same information ( sense definitions ) elesk and ldavec',\n",
       " 'to the same entity in reality ( ; ) . It is a core component in natural language processing and information extraction . Both rule-based approach ( ) and statistic-based approach ( ; TARGETCIT ; ) are proposed in coreference resolution study . Besides the frequently used syntactic and semantic features , the more linguistic features are exploited in recent works ( ) . CoNLL-2012',\n",
       " 'THERCIT ) . Therefore , many approaches have been proposed to learn word segmentation suitable for SMT . These approaches were either complicated ( ) , or of high computational complexity ( ; TARGETCIT . Moreover , they implicitly assumed that WSA and WSR should be equal . This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for transl',\n",
       " 'd test sets . The results 6nlp.cs.lth.se/software/treebank_converter . It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by TARGETCIT . However , to facilitate comparison with other parsers and dependency schemes , we did not use it in our experiments . We ran the converter with the rightBranching=false option to indicate that we are u',\n",
       " 'War for estimating inter-annotator agreement . appropriate entity classes will vary widely by domain ; occurrence rates for entity classes are quite different in news text vs. Wikipedia , for instance ( TARGETCIT . This is abundantly clear in technical and scientific discourse , where much of the terminology is domain-specific , but it holds elsewhere . Non-POL entities in the history domain , for instance , includ',\n",
       " 's for Discriminative Model 5.2.1 Local Features While the inference algorithm is a simple Viterbi algorithm , the discriminative model can use all trisibling features and some grand-sibling features2 ( TARGETCIT as a local scoring factor in addition to the first- and sibling second-order graphbased features . This is because the first stage shiftreduce parser uses features described in Section 2 and this infor',\n",
       " '-English 3.51M 1,894 1,664 1,357 ( mt05 ) ( mt06 ) ( mt08 ) Arabic-English 1.49M 1,663 1,360 1,313 ( mt06 ) ( mt08 ) ( mt09 ) Table 1 : Datasets for the two experimental conditions . 5 Experimental Design Following TARGETCIT , our experimental setup utilizes both real and synthetic data . The motivation for using synthetic data is that it is a way of gauging the quality of optimization methods , since the data is constructe',\n",
       " 'we adapt the main insights of such work to the machine translation setting and share results on two language pairs . Some recent works have attempted to relax the linearity assumption on MT features ( TARGETCIT , by defining non-parametric models on complete translation hypotheses , for use in an nbest re-ranking setting . In this paper we develop a framework for inducing non-linear features in the form of reg',\n",
       " 'e in developing an unsupervised segmentation model . The effectiveness of lexical cohesion has been demonstrated by TextTiling ( ) , c99 ( ) , MinCut ( ) , PLDA ( ) , Bayesseg ( TARGETCIT , TopicTiling ( ) , etc . Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods ( ) , and is based on Bayesian segmentation methods ( ) u',\n",
       " '651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , pages 651–657 , Sofia , Bulgaria , August 4-9 2013. c�2013 Association for Computational Linguistics topic . Z. TARGETCIT b ) used part-of-speech information and word clustering techniques , while F. added this information to the TFIDF method so as to consider both word dependency and semantic information . However',\n",
       " 'n beat a Random or LESK ( ) baseline ( see ) and can compete with or even beat the Most Frequent Sense ( MFS ) baseline in certain contexts which is by no means an easy task ( see , TARGETCIT a ) ) . 2 Methodology PD is a framework for knowledge-based WSD approaches that employ semantic graphs . However before we can elaborate we must first cover the fundamental resources it is built upon . 2.1',\n",
       " 'which uses a binary classifier for pairwise comparisons ) . Their features included some of the metric outputs ( BLEU , ROUGE , etc . ) for SVM-RANK and similarity scores between bags of n-grams for Tesla ( TARGETCIT . 5 Conclusions We introduced the TerrorCat metric , which performs pairwise comparison of translation hypotheses based on frequencies of automatically obtained error categories using a binary classifi',\n",
       " 'velopment newstest2012 test12 3,003 Table 1 : Used bilingual and monolingual corpora only available at that time ) . 2.3 Alignment and Translation Models All parallel corpora were aligned using MGIZA++ ( TARGETCIT . Our translation models are phrase-based models ( PBMs ) built with MOSES using default settings . Weights of LM , phrase table and lexicalized reordering model scores were optimized on test12 , thanks to',\n",
       " 'fluent or meaningful compressions ; thus , compression systems often assemble output text from units that are larger than single tokens such as n-grams ( McOTHERCIT ) or edges in a dependency structure ( TARGETCIT ; ) . These systems implicitly rely on a structural representation of text—as a sequence of tokens or as a dependency tree respectively—to to underpin the generation of an output sentence . In t',\n",
       " 'lexicalized parsing to Chinese and the results on English and Chinese are far from equal . Adapting unlexicalized parsers appears to be equally difficult : ( ) adapt the unlexicalized parser of ( TARGETCIT to Chinese . Automatically splitting grammars like the one of Matsuzaki it al . ( 2005 ) and require a Treebank not additionally hand tailored to English . ( ) exhibited a very accurate cat',\n",
       " 'seamlessly with CCG theories of formal semantics . Typing predicates—for example , determining that writing is a relation between people and books— has become standard in relation clustering ( ; TARGETCIT ; ) . We demonstate how to build a typing model into the CCG derivation , by subcategorizing all terms representing entities in the logical form with a more detailed type . These types are also i',\n",
       " 'the vehicle ( V ) . An example ( adapted from the BNC ) of a simile involving all of the above would be : [ He T ] [ looked E ] [ like C ] [ a broiled frog V ] , [ hunched P ] over his desk , grinning and satisfied . TARGETCIT used constituent parsing with GLARF ( ) transformations in order to match several hand-written comparison patterns . While the normalizations performed by GLARF allow for more general patterns (',\n",
       " 'd segmentation standards ( ) , the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing ( ) , and the strategies of hybrid or stacked modeling ( TARGETCIT ; ) . In parsing , proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank . It uses partial bracketing information',\n",
       " '.4 Grandparent-Sibling Scoring A further widening of the factorization takes grandparents and siblings simultaneously : For projective parsing , dynamic programming for this factorization was derived in TARGETCIT ( Model 1 in that paper ) , and for nonprojective parsing , dual decomposition was used for this factorization in . This factorization should combine all the benefits of the sibling and grandparen',\n",
       " 'f one word from each of the discourse relation arguments , have been identified as a very useful feature for the classification of implicit discourse relations in the Penn Discourse Treebank ( ; TARGETCIT , and , quite surprisingly , also for smaller datasets such as the discourse relations in the RST Discourse Treebank targeted by or the ambiguous connective dataset used by OTHERCIT.5 Because o',\n",
       " ') as the bitext from which to extract the AUTOMATIC bilingual lexicons . For each target language , we produce one-to-one alignments on the Englishtarget bitext by running the Berkeley Aligner ( TARGETCIT with five iterations of IBM Model 1 and 8Results do not degrade much if one simply uses Sections 2- 21 of the Penn treebank instead . Coverage of rare words in the treebank is less important when a giv',\n",
       " 'e used for tuning ( 1,000 sentences each ) and evaluation ( 5,000 sentences each ) . On these corpora we train twelve French-to-English and twelve English-to-French PB-SMT systems using the Moses toolkit ( TARGETCIT . We use the same GIZA++ configuration and phrase table pruning as in the Europarl experiments . We also reuse the English and French language models . French-to-English MT systems are tuned and tested',\n",
       " 'parsing is mapping natural language phrases ( e.g. , “ attend ” ) to logical predicates ( e.g. , Education ) . While limited-domain semantic parsers are able to learn the lexicon from per-example supervision ( TARGETCIT ; ) , at large scale they have inadequate coverage ( ) . Previous work on semantic parsing on Freebase uses a combination of manual rules ( ) , distant supervision ( ) , and s',\n",
       " 'significantly improve translation accuracy . 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation ( SMT ) ( TARGETCIT b ; ) . In this paper , we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT . Given a source side predicate-argument structure , we attempt to tr',\n",
       " 'or political changes . Nevertheless , many such imperfect rules can be learned and combined to yield useful KB completions , as demonstrated in particular with the Path-Ranking Algorithm ( PRA ) ( ; TARGETCIT , which learns such rules on heterogenous graphs for link prediction tasks . Alternatively , we may attempt to fill KB gaps by applying relation extraction rules to free text . For instance , and',\n",
       " 'ation tasks , uses Dirichlet Process Mixture Models for verb clustering , and uses a hierarchical Levin-style clustering to cluster verbs . Previous word sense induction work ( ; TARGETCIT ; ) relates to our work in that these approaches discover word senses automatically through clustering , even using multilingual parallel corpora . However , our task of clustering multiple words',\n",
       " 'plained by the fact that different authors aimed to identify a different scope of linguistic phenomena and thus interpreted the concept of “ light verb construction ” slightly differently . For instance , TARGETCIT and focused only on true light verb constructions while only object–verb pairs are considered in other studies ( ) . Several other studies report results only on light verb construction',\n",
       " 'robability distribution ( ) , and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations . Recently , event paraphrases ( TARGETCIT have been explored to deal with the diversity of event descriptions . However , the New Event Detection task differs from our event recognition task because we want to find all stories describing a cert',\n",
       " 'scribed in Table 3 below . # Sentences # Words Training 94926 1235163 Tuning 1446 23600 Test 500 9792 Table 3 : Corpus distribution The baseline system was setup by using the phrase-based model ( ; TARGETCIT . For the language model , we carried out experiments and found on comparison that 5-gram model with modified Kneser-Ney smoothing ( ) to be the best performing . Target Hindi corpus from the tra',\n",
       " 'HPBT ) setups , we employ the open source translation toolkit Jane ( ) , which has been developed at RWTH and is freely available for non-commercial use . In hierarchical phrase-based translation ( TARGETCIT , a weighted synchronous context-free grammar is induced from parallel text . 304 Proceedings of the 7th Workshop on Statistical Machine Translation , pages 304–311 , Montr´eal , Canada , June 7-8 , 2012. c',\n",
       " 'thm is the only exact algorithm widely adopted in the NLP applications . proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence . The staggered decoding ( TARGETCIT forms the basis for our work on iterative based decoding algorithms . Apart from the exact decoding , approximate decoding algorithms such as beam search are also related to our work . proposed',\n",
       " 'ing approach and show its effectiveness . introduce an alternating structure optimization based approach . Most of the works mentioned above focus on determiner and preposition errors . Besides , TARGETCIT propose a method to correct verb form errors through combining the features of parse trees and n-gram counts . To our knowledge , no one focused on noun form errors in specific researches . In this paper',\n",
       " 'Republic of Korea , 8-14 July 2012. c�2012 Association for Computational Linguistics Source-to-target Examples ( partial ) Decoding Rules Parser tree-to-tree ( ) ↓ dep.-to-dep . DG forest-to-tree ( TARGETCIT a ) ↓ ↑↓ tree-to-tree PCFG tree-to-string ( ) ↑ tree-to-string PCFG ( ) ↑ dep.-to-string DG forest-to-string ( ) ↓ ↑↓ tree-to-string PCFG ( ) ↓ ↑↓ tree-to-string HPSG string',\n",
       " 'cture , can we recover the reply structure of messages in the conversation ? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence ( TARGETCIT ; ) . We take a more practical approach and find the optimal structure as part of our inference procedure . We do this by treating the parent of each block as a hidden variable to be inferred . T',\n",
       " 'onaries derived from tagged corpora . 3.1 Labeled corpora and Universal tags We collected part-of-speech tagged corpora for 9 languages , from CoNLL-X and CoNLL-2007 shared tasks on dependency parsing ( TARGETCIT ; ) . In this work we use the Universal POS tag set ( ) that defines 12 universal categories with a relatively stable functional definition across languages . These categories include NOU',\n",
       " 'THERCIT 77.7 74.8 79.6 78.5 79.6 79.0 – – – 73.2 86.7 79.3 69.9 82.7 75.8 74.5 88.3 80.8 ACE2004-nwire This paper 75.1 84.6 79.6 74.1 87.3 80.2 75.9 77.0 76.5 74.5 79.4 76.9 TARGETCIT 70.5 71.3 70.9 – – – 58.5 78.7 67.1 65.2 86.8 74.5 MUC6-Test This paper 69.1 90.6 78.4 63.1 90.6 74.4 77.3 87.2 81.9 67.3 84.7 75.0 75.8 83.0 79.2 – – – 55.1 89.7 6',\n",
       " 'data ( Utrain ) . The context is approximated using the previous two utterances ( one from each speaker ) . This model does not use the contents of the utterance ui itself . 3.2 Cross-lingual Relevance Model TARGETCIT model P as a cross-lingual relevance model . This model takes into account the content of the utterance ui as well as the content of the context . It does not impose any restriction on Upossible , but in',\n",
       " 'can be retrieved . A similar approach has been used recently by ( ) to compute text semantic similarity in recognizing textual entailment , and also as a solution for word sense disambiguation ( TARGETCIT . We have used the UKB software from this last citation to generate the PPVs used in our system . Random walk algorithms are inspired originally by the Google PageRank algorithm ( ) . The idea be',\n",
       " 'the bias , i.e . number of examples generated for each CUI . Performance is compared against various alternative approaches . Two supervised approaches are included . The first , most frequent sense ( MFS ) ( TARGETCIT , is widely used baseline for supervised WSD systems . It consists of assigning each ambiguous term the meaning that is more frequently observed in the training data . The second supervised approach 7ht',\n",
       " 'and McOTHERCIT further improved performance by capturing correlations between events and enforcing consistency across arguments . Temporal event-event relations have been extensively studied ( ; TARGETCIT ; ; McOTHERCIT ; D ’ ) , and we leverage such techniques in our work ( Section 3.1 ) . However , we extend beyond temporal relations alone , and strongly rely on dependencies between process ev',\n",
       " 'ata . We use 5-grams for all language models ( LMs ) implemented using the SRILM toolkit ( ) . For English language modeling , we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit ( TARGETCIT . All experiments are conducted using the Moses phrase-based SMT system ( ) . We use MERT ( ) for decoding weight 4LDC Catalog IDs : LDC2005E83 , LDC2006E24 , LDC2006E34 , LDC2006E85 , LDC2006',\n",
       " 'sed w = 6.0 , since , for a few instances , lower values of w resulted in prohibitive runtime . For beam search we used k = 150 , since higher val3http : //lucene.apache.org 4www.ark.cs.cmu.edu/ARKref/ See ( TARGETCIT ues of k did not improve the proof cost on the training dataset . The value of d in LLGS was set to 3. d = 4 yielded the same proof costs , but was about 3 times slower . Since lower values of w could be',\n",
       " 'is vast , so we turn to semisupervised learning . Here we adapt self-training , a simple technique that leverages a supervised learner ( like the perceptron ) to perform semisupervised learning ( ; TARGETCIT . In our version , a model is trained on the labeled data , then used to label the unlabeled target data . We iterate between training on the hypothetically-labeled target data plus the original labeled',\n",
       " 'the grammar . Refinements have been in common use in syntactic parsing for years now . Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy ( TARGETCIT , several approaches have been introduced to automatically induce latent symbols on existing trees . We use the splitmerge method commonly used in syntactic parsing ( ) . In its original setting ,',\n",
       " 'be obtained from various sources such as dictionaries , gazetteers , rule-based systems ( Str¨otgen and ) , statistically trained classifiers ( ) , or some web resources such as Wikipedia ( TARGETCIT . However , in practice , outputs from existing mention identification and typing systems can be far from ideal . Instead of obtaining the above ideal annotation , one might observe the following noisy an',\n",
       " 's from learner corpora text ( as opposed to native speaker text ) and use knowledge of the observed number for prediction . Use of observed values has been shown to be beneficial for grammar correction ( TARGETCIT ; ) . If the model predicts requires correction , then the observed number is toggled to obtain the corrected noun-number . In order to bias the system towards improved precision , we apply the co',\n",
       " 'can be computed ( referred to as B-precision , B-recall , etc. ) . Actual B Non-B B CM ( 1 , 1 ) CM ( 0 , 1 ) Non-B CM ( 1 , 0 ) CM ( 0 , 0 ) Figure 2 : Example confusion matrix ( Tb = { 1 } ) 3.4 B-Based Inter-coder Agreement TARGETCIT , p. 156–157 ) adapted four inter-coder agreement formulations provided by to use S to award partial credit for near misses , but because S produces cosmetically high agreement values they gros',\n",
       " 'proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants . Finally , they has been a recent interest in automatic evaluation of paraphrases ( ; TARGETCIT ; ) . 3 Experimental setting We used the main aspects of the methodology described by for constructing evaluation corpora and assessing the performance of techniques on the task of sub',\n",
       " 'd to represent aggregate properties of the entities . Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions ( ; TARGETCIT ; ) , and can even provide a strong signal for unsupervised coreference ( ) . A second problem , that has received significantly less attention in the literature , is that the pairwise core',\n",
       " 'ng the results of various runs employing differing seed tuples , constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process ( T. McOTHERCIT ; TARGETCIT ; ) . However , the analysis of has shown that semantic drift is an inherent property of iterative bootstrapping algorithms and therefore poses a fundamental problem . They have shown th',\n",
       " 'average for each sentence ; this is mostly due to shorter sentences that may only receive one or two parses . We perform the same diversity experiment using the DepBank-style grammatical relations ( GRs , TARGETCIT ; ) output of the parser . GRs are generated via a dependency to GR mapping in the parser as well as a post-processing script to clean up common errors ( ) . GRs provide a more formalism-',\n",
       " 'alue of 0.05 for BLEU and TER . 7 Conclusion We propose and present a comprehensive study of several multi-metric optimization ( MMO ) methods in SMT . First , by exploiting the idea of ensemble decoding ( TARGETCIT , we propose an effective way to combine multiple Pareto-optimal model weights from previous MMO methods ( e.g . ) , obviating the need for manually trading off among metrics . We also proposed tw',\n",
       " 'ils All frequency co-occurrence information has been extracted from the ukWaC corpus ( ) . The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger ( ; TARGETCIT . Distributional word space algorithms have been implemented in Python . The maximum entropy classifier was implemented using the Maximum Entropy Modeling Toolkit for Python and C++ ( ) . 3 Resul',\n",
       " 'uline or feminine Lexical features FN * R Rationality : rational , irrational , ambiguous , unknown or N/A LMM Undiacritized lemma Table 1 : Features used in the CADIM submission with the Easy-First Parser ( TARGETCIT . Training Set Test Set LAS UAS LaS 5K ( SPMRL ’ 2013 ) dev < 70 81.7 84.7 92.7 All ( SPMRL ’ 2013 ) dev < 70 84.8 87.4 94.2 test ( old split ) < 70 81.7 84.6 92.8 5K ( SPMRL ’ 2013 ) dev 81.1 84.2 92.7 Al',\n",
       " 'timates . Specifically , this empirical esti˜H ( W ) for a sequence W corresponds to : 1 log |W |− |W |1 : f ( x ) log f ( x ) x : types For this equation to work , one needs to estimate other model parameters . See TARGETCIT for a comprehensive treatment . mate 166 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , pages 166–170 , Sofia , Bulgaria , August 4-9 2013. c�2013 Association for',\n",
       " 'otators strongly prefer different numbers of tweets in a summary for different topics . However , most of the previous work produced summaries of a pre-fixed length and has no evaluation on conciseness . TARGETCIT a ) and also noticed the ideal length of summaries can be very different from topic to topic , and had to use the length of human reference summaries to decide the length of system outputs , inf',\n",
       " 'apers has followed the mold of “ NLP for Twitter , ” including part-of-speech tagging ( ) , named entity recognition ( ) , parsing ( ) , dialogue modeling ( ) and summarization ( TARGETCIT . These papers adapt various parts of the natural language processing pipeline for social media text , and make use of a range of techniques : • preprocessing to normalize expressive lengthening , and el',\n",
       " 'he generic procedure of Sch¨utze ( 1998 ) , we propose algorithms for a number of tensor-based models , where the composition is modelled as the application of linear maps ( tensor contractions ) . Following TARGETCIT and many others , we test our models on two disambiguation tasks similar to that of , and on the phrase similarity task introduced in ( ) . In almost every case , the results show that dis',\n",
       " 'etwork deeper by adding hidden layers followed by hyperbolic tangent nonlinearities . 3 Experimental Set-up 3.1 Baseline ASR system While investigating DNN LMs , we worked on the WSJ task used also in ( TARGETCIT for model M language model . This set-up is suitable for our initial experiments since having a moderate size vocabulary minimizes the effect of using a shortlist at the output layer . It also allows us',\n",
       " 'approaches . 1 Introduction In recent years , there has been a growing interest among researchers in methods for incremental natural language understanding ( NLU ) for spoken dialogue systems ; see e.g . ( TARGETCIT ; ; DeOTHERCIT ) . This work has generally been motivated by a desire to make dialogue systems more efficient and more natural , by enabling them to provide lower latency responses ( ) , hu',\n",
       " 'g from c oarser parses , as our model makes use of some syntactic features . 7 machine translation . The handful of papers that we have found on animacy annotation—centrally , Øvrelid ( 2005 ) , and TARGETCIT — classify only the basic ANIMATE/INANIMATE contrast , but show some promise in doing so . Their work shows success in automatically classifying individual words , and related work has shown that animacy',\n",
       " 'tion during last decades . From a bilingual corpus , all translation knowledge can be acquired automatically in SMT framework . Phrasebased model ( ) and hierarchical phrase-based model ( ; TARGETCIT show state-of-the-art performance in various language pairs . This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus . However , the errors',\n",
       " 'stem Chinese Arabic LM+TM adaptation 33.2 47.7 +RMA+dev-smoothing+DF 33.5 48.4 * * Table 4 : RM adaptation improves over a baseline containing adapted LMs and TMs . tem was tuned with batch lattice MIRA ( TARGETCIT . 4.3 Results For our main baseline , we simply concatenate all training data . We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs . Our metric is cas',\n",
       " 'gain in the response . More complicated matching pattern could also be learned . For example , the response to a post asking “ how to ” is statistically longer than average responses . As a particular case , TARGETCIT applied translation model ( ) on similar parallel data extracted from Twitter in order to extract the word-to-word correlation . Please note that with more sophisticated natural language process',\n",
       " 's of pages users click . This parallel data is used to train a projection matrix for creating the mapping between words in queries and documents based on user feedback , using a Siamese neural network ( TARGETCIT . Each row vector of this matrix is the dense vector representation of the corresponding word in the vocabulary . Perhaps due to its unique information source , we found this particular word embedding s',\n",
       " 'c similarity between two tweets , according to syntactic and lexical information . In particular , in line with the idea of using convolution tree kernels to model complex semantic tasks , e.g . ( ; TARGETCIT , we adopted the Smoothed Partial Tree Kernel ( ) ( SPTK ) . It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures , which are partially sim',\n",
       " 'rformance of the graph-based systems that were compared , where McDonald06 refers to the second-order parser of McOTHERCIT , Koo08-standard refers to the second-order parser with the features defined in TARGETCIT , Koo10-model1 refers to the third-order parser with model1 of , Koo08-dep2c refers to the second-order parser with cluster-based features of ( ) , Suzuki09 refers to the parser of OTHER',\n",
       " 'l likelihood for the parse ( see ) . 4 Training We have two feature sets phrase and deps , for which we fix weights using parallel stochastic optimization of a structured SVM objective ( ; TARGETCIT . To the single feature in the set gen ( i.e . the generative model score ) , we give the weight 1 . The combined models , phrase+deps , phrase+gen , and phrase+deps+gen , are then model combinations of the fi',\n",
       " 'ic , and string similarity , are informative ( ; Daumé and ) . This thesis builds upon this work and uses a diverse set of signals for translating full sentences , not just words . Recently , TARGETCIT , , and have worked toward learning a phrase-based translation model from monolingual corpora , relying on decipherment techniques . In contrast to that research thread , we make the rea',\n",
       " 'ransformation to improve transformation accuracy . In recent years many works have been devoted to the word segmentation task . For example , the introduction of global training or complicated features ( TARGETCIT ; ) ; the investigation of word structures ( ) ; the strategies of hybrid , joint or stacked modeling ( ) , and the semisupervised and unsupervised technologies utilizing raw text ( O',\n",
       " 'quot ; value= & quot ; 1998-XX-XXTNI & quot ; > noite < /TIMEX3 > emqueovoo TWA800 < EVENT eid= & quot ; e2 & quot ; class= & quot ; OCCURRENCE & quot ; stem= & quot ; cair & quot ; aspect= & quot ; NONE & quot ; tense= & quot ; PPI & quot ; polarit TARGETCIT to generate the specific verb forms that are used in the queries . They are mostly third person singular forms of several different tenses . The indicators that we used are ratios of Google Hits . They c',\n",
       " 'rformance over AESOP 2011 submitted metrics . 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage ( ; TARGETCIT . However , users may still find it difficult to read such high-content coverage summaries as they lack fluency . To promote research on automatic evaluation of summary readability , the Text Analysis Co',\n",
       " 'w As a basis , we use MaryTTS ( Schröder and ) , but replace Mary ’ s internal data structures and processing strategies with structures from our incremental SDS architecture , the INPROTK toolkit ( TARGETCIT ; ) , which implements the IU model for incremental dialogue processing ( ) . The model conceptualises – and the toolkit implements – incremental processing as the processing of increment',\n",
       " '7 , Sofia , Bulgaria , August 4-9 2013. c�2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation ( TARGETCIT ; ) . In order to facilitate research on multilingual syntactic analysis , we present a collection of data sets with uniformly analyzed sentences for six languages : German , English , French , Kore',\n",
       " 'Entity-Centric , Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track . The TARGETCIT system has marginally higher B3 and BLANC F1 scores , but does not outperform our model on the other two metrics and the average F1 score . Table 5 shows that our model has higher B3 F1 scores than all',\n",
       " 'ain boundary words at both left and right extremities of the target string for the purpose of LM evaluation , which leads to a high time complexity . The time complexity in theory and with beam search ( TARGETCIT is shown in Table 1 . 2.2 Earley-style Top-down Decoding The Earley-style decoding algorithm performs a topdown depth-first parsing and generates the target translation left to right . It applies Contex',\n",
       " 'utral pronoun its , the improvement in F1 increased at 1.9 % , as shown in Table 3 . 8 Related Work Closest to our clustering approach from Section 2 is the error-driven first-order probabilistic model of TARGETCIT . Among significant differences we mention that our model is non-probabilistic , simpler and easier to understand and implement . Furthermore , the update step does not stop after the first clustering er',\n",
       " 'ational Linguistics the most frequent errors made by learners of English . Not surprisingly , much published research on grammatical error correction focuses on article and preposition errors ( ; TARGETCIT ; ) , with relatively less work on correcting word choice errors ( ) . Article and preposition errors were also the only error types featured in the HOO 2012 shared task . Likewise , althou',\n",
       " '25 Transactions of the Association for Computational Linguistics , 1 ( 2013 ) 25–36 . Action Editor : Hal Daum´e III . Submitted 10/2012 ; Published 3/2013 . c�2013 Association for Computational Linguistics . TARGETCIT and contains pairs of naturallanguage action descriptions plus their associated video segments . Each of the pairs is annotated with a similarity score based on several manual annotations . • We report',\n",
       " 'ic knowledge in the generation process . Several authors have proposed using conditional models to predict the probability of phrase translation in context ( ) . Of particular note is the work of TARGETCIT , who use a conditional model to predict morphological features conditioned on rich linguistic features ; however , this latter work also conditions on target context , which substantially complicates de',\n",
       " 'implementation of the decomposed inference algorithm . 6 Discussion Lagrangian Relaxation in the literature In the literature , in applications of the Lagrangian relaxation technique ( such as ( ; TARGETCIT and others ) , the relaxed problems are solved using specialized algorithms . However , in both the relaxations considered in this paper , even the relaxed problems can not be solved without an ILP solver ,',\n",
       " 'propagation alone [ PRED-ARG ( CP ) ] improves the performance substantially over the comparable graph construction with different graph analysis algorithms , in particular , HITS and PageRank approaches of TARGETCIT . The two completely connected variants of the graph propagation on the Pred-Arg graph , [ ® PRED-ARG ( PMI ) ] and [ ® PRED-ARG ( CP ) ] , do not necessarily improve the performance over the simpler and comput',\n",
       " 'ubstitute selection : SDSM performs at par with window-based distributional vectors . 2 . Experiments with phrasal units on two-word composition : state-of-the-art results are produced on the dataset from TARGETCIT in terms of correlation with human judgment . 3 . Experiments with larger structures on the task of judging event coreferentiality : SDSM shows superior performance over state-ofthe-art window-based word',\n",
       " 'ic and human evaluation . Second , though a tuning metric should correlate strongly with human judgment , MERT ( and similar algorithms ) invoke the chosen metric so often that it must be computed quickly . TARGETCIT claimed that TESLA tuning performed better than BLEU tuning according to human judgment . However , in the WMT 2011 “ tunable metrics ” shared pilot task , this did not hold ( ) . In ( ) , huma',\n",
       " 'P applications must make decisions when the relationship between T and W is more ambiguous than in RTE corpora . That is why we decided to apply the scrambling technique on RTE corpora for evaluation ( TARGETCIT . For each pair in an entailment relationship we replaced the name of the entities in W with entities from T. For example the sentence Bob Iger is Disney CEO which originally was in entailmentship wit',\n",
       " 'quirement . There are a number of ideas on how to define composition in such vector spaces . A general framework for semantic vector composition was proposed in , with and more recently TARGETCIT providing good overviews of this topic . Notable approaches to this issue include , who compose nouns and adjectives by representing them as vectors and matrices , respectively , with the composi',\n",
       " 'ly the dual decomposition method for joint inference problems , all of the past work deals with cases where the various model components have the same inference output space ( e.g. , dependency parsing ( TARGETCIT , POS tagging ( ) , etc. ) . In our case the output space is the much more complex joint alignment and NER tagging space . We propose a novel dual decomposition variant for performing inference ove',\n",
       " 'WordNet , we can measure the semantic similarity or relatedness between a pair of concepts ( or word senses ) , and by extension , between a pair of sentences . We use the similarity measure described in ( TARGETCIT which finds the path length to the root node from the least common subsumer ( LCS ) of the two word senses which is the most specific word sense they share as an ancestor . 3.2 Feature Combination The fe',\n",
       " 'd are induced iteratively using the distributed representations of nearby words , not the nearby words themselves . Using distributed representations helps to alleviate data sparsity problems . Recently , TARGETCIT b ) induced distributed representations for the crosslingual setting . There , the induced embedding is learned jointly over multiple languages so that the representations of semantically similar words e',\n",
       " 'entrated on taking full advantage of the monolingual corpora in both source and target languages , and proposed methods for bilingual lexicon induction from non-parallel data ( , 1999 ; ; TARGETCIT ; Daumé III and ) and proposed unsupervised statistical machine translation ( bilingual lexicon is a byproduct ) with only monolingual corpora ( ) . In the bilingual lexicon induction ( OTH',\n",
       " 'to stacked learning ( SL ) , a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models , i.e. , graph-based and transition-based models ( TARGETCIT ; ) . However , the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies . 3 Dependency Parsing Given an input se',\n",
       " 'lts without using an additional word dictionary—just by using a high order n-gram language model . ( ) report experiments on large vocabulary substitution ciphers based on the Transtac corpus . ( TARGETCIT improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus . We run our method on the same corpus and report improvement',\n",
       " 'ation of our resource via automatic alignment we lose more than two thirds of the original synsets ( see Section 3.1 ) , our results are promising . They are also not that distant from results reported by TARGETCIT , whose best system , a combination of unigrams and the best set of features , achieves an accuracy of 60.50 % on a three-way classification like ours , evaluated against a manually annotated set of Engli',\n",
       " 'ndicates that stacking has a good effect on training the models . To further investigate the effects of guide features , we tried to define unlabeled versions of the secondorder guide features used in ( TARGETCIT ; McOTHERCIT ) . However , these features did not produce good results , and investigation to find the cause is an important future work . We also examined parsing errors in more detail . Table 11 shows roo',\n",
       " 'arallel training data , with * pro * and * PRO * inserted between the original Chinese words . Then we run GIZA++ ( ) to generate the word alignment for each direction and apply grow-diagonal-final ( TARGETCIT , same as in the baseline . We want to measure the impact on the word alignment , which is an important step for the system building . We append a 300-sentence set , which we have human hand alignment ava',\n",
       " 'parsing has shown that it is possible to use exact dynamic programming under certain conditions , but this leads either to very inefficient parsing or to very restricted feature representations . Thus , TARGETCIT present a dynamic programming scheme for a feature-rich arc-standard parser , but the resulting parsing complexity is O ( n7 ) and they therefore have to resort to beam search in practical parsing experim',\n",
       " 'tes ( Onto ) , Senseval-2 ( SE-2 ) , and combined ( Onto+SE-2 ) . Results are reported for all three of our signature comparison measures and also for two previous works ( last two rows ) . word senses ( ; TARGETCIT . proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English ( ODE ) . The approach leverages semantic similarities in',\n",
       " 'ry . One of the most well-known extractive approaches is maximal marginal relevance ( MMR ) , which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria ( TARGETCIT . Greedy MMR-style algorithms are widely used ; however , they can not take into account the whole quality of the summary due to their greediness , although a summary should convey all the information in',\n",
       " 'ion ( EM ) to this end can in some cases bring substantial accuracy gains . For discriminative models , self-training has been shown to be quite effective for adapting monolingual parsers to new domains ( TARGETCIT , as well as for relexicalizing delexicalized parsers using unlabeled target language data ( ) . Similarly Täckström ( 2012 ) used self-training to adapt a multi-source direct transfer named-entit',\n",
       " 'requirements of the dataset ; as for validation , we found that three judgments were sufficient for a final categorisation . An alternative to our rather artificial way to collect data is presented in ( TARGETCIT , employing web forum structure . We have presented an experiment with two basic TE algorithms which establishes that the difficulty of the dataset is roughly comparable with the RTE-5 Search task test',\n",
       " 'her semantic phenomena like coreference , negation and modality . We also observe that no single model or composition operator performs best for all tasks and datasets . The latent sense mixture model of TARGETCIT performs well in recognizing semantic relations in general web text . Because of the difficulty of adapting it to a specialized domain , however , it does less well in biomedical question answering , wher',\n",
       " 'he reference hypergraph , we collect new synchronous translation rules and record them in the grammar G. We also calculate the average feature values of hypergraphs using the inside-outside algorithm ( TARGETCIT , so as to compute the gradients . 4 Features One advantage of the discriminative method is that it enables us to incorporate arbitrary features . As shown in Section 2 , our model incorporates both loca',\n",
       " 'ng the sign test with p < 0.05 . Evaluation and Baselines To measure parsing performance , we use unlabeled attachment score ( UAS ) given by the CONLL-X dependency parsing shared task evaluation script ( TARGETCIT . We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of McOTHERCIT and to a self-training baseline ( ) . The parsing baseline is equiva',\n",
       " 'onll2007 and functional references but poorly against the lexical and oldLTH references.18 Considering the latter two references , a different system would be selected as the highest performing , namely TARGETCIT ( BH ) over ( BC ) which wins in the other cases . 18The common difference here is that the latter two references do not treat prepositions as heads of PPs . 74 This evaluation method rewards many',\n",
       " 'set creation , database curation , summarization , and information retrieval . Other previous studies have used citing sentences in various applications such as : scientific paper summarization ( ; TARGETCIT ; ) , automatic survey generation ( ) , and citation function classification ( ) . Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries , pages 1–12 ,',\n",
       " 'ue to improved parser performance . We then analyze the impact of zero pronouns on Chinese syntactic parsing . As a preliminary exploration , we integrate Chinese zero pronouns into the Berkeley parser ( TARGETCIT , experimenting with gold-standard or automatically determined zero pronouns kept or stripped off ( using gold-standard word segmentation provided in the CoNLL-2012 data ) . The results indicate that giv',\n",
       " 'ing ” words on the basis of the message content , and including them in the vector representation . Guo and Diab showed WTMF to outperform LDA on the SemEval-2012 semantic textual similarity task ( STS ) ( TARGETCIT . The semantic space required for this model as applied here is built from the background tweets corresponding to the target word . We experimented with the missing weight parameter w , , , , of WTMF in th',\n",
       " 'ews monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified KneserNey smoothing ( ) . This was used to create a KenLM ( TARGETCIT . As the tuning set for both language pairs , we used the 2051 sentences in news-test2008 ( NT08 ) , and report results on the 2525 sentences of news-test2009 ( NT09 ) and 2489 of news-test2010 ( NT10 ) . Corp',\n",
       " 'data attributes . Experimentally , they outperform the multi-domain learning baseline , even when it selects the single “ best ” attribute . 1 Introduction Multi-Domain Learning ( ; Daum´e III , 2007 ; TARGETCIT ; ) algorithms learn when training instances are spread across many domains , which impact model parameters . These algorithms use examples from each domain to learn a general model that is also',\n",
       " 'c class information ( ) . Other approaches incorporate information from other sources ( ) or compute heuristic scores for realvalued features based on a large corpus or the web ( ; TARGETCIT . We use four different clusterings in our experiments , each with twenty clusters : dependency-parse-derived NAIVEBAYES clusters , semantic-role-derived CONDITIONAL clusters , SRL-derived NAIVEBAYES clus',\n",
       " 'require annotated texts for training . Statistical significance testing shows that the Event Phrase lookup approach works significantly better than the unigram classifier ( p < 0.05 , paired bootstrap ( TARGETCIT ) . For the sake of completeness , we also evaluated the performance of dictionary look-up using our bootstrapped Agent ( AG ) and Purpose ( PU ) dictionaries , individually . The agents terms produced 42 % pr',\n",
       " 'in their sentence ordering model for discourse generation . combine the entity grid with readability-related features to discriminate documents between easy- and difficult-to-read categories . TARGETCIT use discourse relations to transform the entity grid representation into a discourse role matrix that is used to generate feature vectors for machine learning algorithms similarly to . Several',\n",
       " 'om natural language questions . Based on a review of related work , we suspect that the methodology could be applied with very little adaptation to the systems described in ( ) , ( ) , and ( TARGETCIT , which all have architectures that lend themselves to the type of gray-box evaluation that we have done here . Future work will extend in a variety of directions . Following the test-driven development',\n",
       " 'words are indexed for relative position or not ; and ( 4 ) whether we use all context tokens , or only IV words . Because high-accuracy linguistic processing tools for Twitter are still under exploration ( TARGETCIT b ; ) , we do not consider richer representations of context , for example , incorporating information about part-of-speech tags or syntax . We also experiment with a number of simple but widely-us',\n",
       " ', thus reflecting a short-long pattern , as illustrated in the Temperley p.c . example in Table 2 . Apart from these , shows that arguments are generally located closer to the verb than adjuncts . TARGETCIT also suggest that adverb placement might involve cases which go against dependency length minimization . An examination of 295 legitimate long-short post-verbal constituent orders ( counter to dependenc',\n",
       " 'ich are spelling variations of words that would be related to the context . Liu et al . identified and distinguished nonstandard tokens found in social media texts as intentionally and unintentionally ( TARGETCIT . A normalization system was proposed by integrating different techniques including the enhanced letter transformation , visual priming , and string/phonetic similarity . The proposed system was evaluate',\n",
       " 'ces in general . 3.1 Most common triggers The trigger detection algorithm of the TEES software is based upon SVM classifiers ( Section 2.1 ) , and has been shown to outperform dictionarybased approaches ( TARGETCIT ; ) . To investigate its performance in a largescale application , we first analyse the most frequent trigger words of each event type in EVEX ( Table 1 ) . We notice the presence of different infl',\n",
       " 'tails can be got from . 4 Rule Extraction In what follows , we introduce how to get the rule set . We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit ( TARGETCIT and then is refined using a “ final-and ” strategy . We generate the rule set in two steps : first , we extract two sets of phrases , basic phrases and chunk-based phrases . Basic phrases are defined using t',\n",
       " 'present work . We start by modelling cross-lingual pronoun prediction as an independent machine learning task after doing anaphora resolution in the source language ( English ) using the BART software ( TARGETCIT . We show that it is difficult to achieve satisfactory performance with standard maximum380 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 380–391 , Seatt',\n",
       " 'such as cosine similarity . 2 Related Work Probabilistic content models were proposed by , and related models have since become popular for summarization ( ) , and information ordering ( TARGETCIT ; ) . Other related generative models include topic models and structured versions thereof ( ) . In terms of domain learning in the form of template induction , heuristic methods involving',\n",
       " '. We experiment with 3 to 6 ranks ( the case where C = 2 reduces to the standard entity-based model ) . 5.1.2 ’ s best results were achieved by employing an automatic coreference resolution tool ( TARGETCIT for extracting entities from a source document , and the permutations were generated only afterwards — entity extraction from a permuted document depends on knowing the correct sentence order and the o',\n",
       " 'evel and topic-level representations . Our scheme can be applied on top of any context-insensitive “ base ” similarity measure for rule learning , which operates at the word level , such as Cosine or Lin ( TARGETCIT . Rather than computing a single context-insensitive rule score , we compute a distinct word-level similarity score for each topic in an LDA model . Then , when applying a rule in a given context , these',\n",
       " 'ontinuation of a morpheme • O : Outside morpheme ( whitespace ) • B : Beginning of a morpheme • F : Non-native character ( s ) 2Segmentation also improves translation of compounding languages such as German ( TARGETCIT and Finnish ( ) . 147 Translation Model e Target sequence of I words f Source sequence of J words a Sequence of K phrase alignments for ( e , f ) ri Permutation of the alignments for target word or',\n",
       " 'CIT ) to disambiguate connectives , with small improvements . Lexical consistency has been addressed by the use of post-processing ( ) , multi-pass decoding ( ) , and cache models ( ) . TARGETCIT addressed the issue of tense selection for translation from Chinese , by the use of inter-sentential tense n-grams , exploiting information from previously translated sentences . Another way to use a lar',\n",
       " 'ent results varying the amount of Urdu-English comparable corpora used to induce translations and estimate additional features . Table 4 also shows the Hiero ( ) and SAMT ( ) results that TARGETCIT report for the same 10Because the automatic word alignments are noisy , this oracle is conservative . 265 Experiment Tamil Diff . Telugu Diff . Bengali Diff . Malayalam Diff . Hindi Diff . Urdu Diff . BLEU BL',\n",
       " 'lel tracks . This was seen as a conceptual analogue to the structures used in Barselou ’ s Frame Theory ( ) . Recall first the classic event structure distinctions of Generative Lexicon Theory ( cf . TARGETCIT , shown below : ( 8 ) a . EVENT —+ STATE |PROCESS |TRANSITION b . STATE : —+ e c. PROCESS : —+ e1 ... en d. TRANSITIONach : —+ STATE STATE e. TRANSITIONacc : —+ PROCESS STATE Let us assume a GL feature structu',\n",
       " '/˜nivre/research/ Penn2Malt.html 2OTHERCIT http : //nlp.cs.lth . se/software/treebank_converter/ Regime ( 1 ) is very commonly done in papers which report dependency parsing experiments ( e.g. , ( McOTHERCIT ; TARGETCIT ; ) ) . Penn2Malt uses the head finding table from . Regime ( 2 ) is based on the recommendations of the two converter tools ; as of the date of this writing , the Penn2Malt website says : “ P',\n",
       " 'ent structure to syntactic arguments ( ) . The last decades have seen a surge in activity on the computational front , spurred in part by efforts to annotate large corpora for lexical semantics ( TARGETCIT ; ) . Initially , we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora ( ) . More rece',\n",
       " 'ase-based translation , as implemented in the open-source Moses translation system ( ) to ensure that they are reproducible . Our methods are easily extended to hierarchical phrase-based models ( TARGETCIT . It is not clear whether the same conclusions would hold : on 437 the one hand , complex phrasal rules might overfit even more badly than phrases ; on the other hand , hierarchical models might have more',\n",
       " 'hem with a new discriminative classifier that greatly improves previous work . The temporal reasoning community has long depended on document timestamps to ground relative time expressions and events ( TARGETCIT ; Llid´o et al. , 2001 ) . For instance , consider the following passage from the TimeBank corpus ( ) : And while there was no profit this year from discontinued operations , last year they contribut',\n",
       " 'analyses . Given the high level of lexical ambiguity in its lexicon , parsing with the ERG should therefore also benefit from supertagging , but while various attempts have shown possibilities ( ; TARGETCIT ; ) , supertagging is still not a standard element in the ERG parsing pipeline . There are two main reasons for this . The first is that the ERG lexicon does not assign simple atomic categories t',\n",
       " 'n is derived from ‘ China ( 中 国 ) ’ , and ‘ a bank ( 银 行 ) ’ . However , many names often originate from abbreviation ( such as ‘ WTO ’ ) ; hence we can not always leverage meanings . Corpus-based approaches ( ; TARGETCIT exploit high-quality bilingual evidence such as parenthetical translation , e.g. , “ 成龙 ( Jackie Chan ) ” , ( ) , semi-structural patterns ( ) , and parallel corpus ( ) . However , the cover',\n",
       " 'ferent from MT evaluation metrics , quality prediction ( QP ) systems do not rely on reference translations and are generally built using machine learning techniques to estimate quality scores ( ; TARGETCIT ; ) . Some interesting uses of sentence-level MT quality prediction are the following : decide whether a given translation is good enough for publishing asis ( ) , or inform monolingual ( t',\n",
       " 'ied this to POS tagging and shallow parsing . It was later applied to parsing ( ) and named entity recognition ( ) . The second idea , first proposed by and applied more broadly by TARGETCIT , is to train a model on a resource-rich language and apply it to a resourcepoor language directly . The disparity between the languages is mitigated by the choice of features . In addition to cross-lin',\n",
       " 'corpus . 4.2 Entity Typing It has become standard in clustering approaches to distributional semantics to assign types to predicates before clustering , and only cluster predicates with the same type ( TARGETCIT ; ) . This is useful for resolving ambiguity—for example the phrase born in may express a place-of-birth or date-of-birth relation depending on whether its second argument has a LOC or DAT type',\n",
       " 'ed here have treated the learning tasks in isolation . Examples include models of word segmentation from phonemic input ( ) or phonetic input ( ) , models of phonetic clustering ( ; TARGETCIT ; ) and phonological rule learning ( ) . present a model that is similar to ours , using a noisy channel model implemented with a finite-state transducer to learn about phonetic',\n",
       " 'ain Bolt ” Week ( b ) Atemporal entity : “ Hillary Clinton ” Figure 1 : Illustration on temporality al. , 2006 ; ) . T is computed using frequency vectors for entities and combined with PH ( ) . ( TARGETCIT extend Tao ’ s approach by iteratively updating overall similarities using R. ( ) holistically combine all the features : PH , CX , T , and R. However , T used in previous approaches is a good feature',\n",
       " 'r simple strategy clearly outperforms ( ) in terms of both precision and recall . ( ) seems to solve more accurately but a more limited number of cases . We also include the results from ( TARGETCIT obtained when using for training a larger corpus extended heuristically ( best ) and the results obtained with no additional training data ( no extra train ) . Our approach 3http : //nlp.stanford.edu/softwar',\n",
       " 'es with length above 30 from the datasets for all experiments.9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method ( TARGETCIT a ) . This algorithm represents the translation of a sentence as a large SCFG rule , which it then factorises into lower rank SCFG rules , a process akin to rule binarisation commonly used in SCFG decodin',\n",
       " 'quations obtained , 3. selecting o ( t ) among the solutions aggregated into Eo ( t ) . In this description , we define an analogical equation as an analogy with one form missing , and 1We refer the reader to ( TARGETCIT for a more technical exposition . we note [ x : y : : z : ? ] the set of its solutions ( i.e . undoable ∈ [ reader : doer : : unreadable : ? ] ) .2 L = { ( Schell , 谢尔 ) , ( Zemens , 泽门斯 ) , ( Zell , 泽尔 ) , ( Schemansky , 谢曼斯基 )',\n",
       " 'hiring ” event from ‘ Bob worked as an analyst for Dell ’ . The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules ( ; TARGETCIT , and generate knowledge resources for inference systems . However , despite their potential , utilization of inference rule resources is currently somewhat limited . This is largely due to the fact that',\n",
       " 'we found it a promising direction for further exploration . adapted the standard entity-based coherence model to the same German corpus , but replaced the original linguistic dimension used by TARGETCIT — grammatical role — with topological field information , and showed that for German text , such a modification improves accuracy . For English text , two extensions have been proposed recently .',\n",
       " 'The grammar learned can then be used to infer the most probable native language that a given text written in a second language is associated with . The latter approach is actually closer to the work of TARGETCIT using adaptor grammars for perspective modeling , which inspired our general approach . This alternative approach is also similar in nature to the work of B¨orschinger et al . ( 2011 ) in which grounded le',\n",
       " 'n proposed to integrate morphology into n-gram language models , including factored language models ( ) , discriminative language modeling ( Arı- soy et al. , 2008 ) , and more heuristic approaches ( TARGETCIT . Despite the fundamentally open nature of the lexicon ( ) , there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of mode',\n",
       " 'sition . Note that our three operations ( intersection , join , and bridging ) are quite permissive , and we rely on features , which encode soft , overlapping rules . In contrast , CCG-based methods ( ; TARGETCIT encode the combination preferences structurally in non-overlapping rules ; these could be emulated with features with weights clamped to −oc . Denotation features While it is clear that learning from de',\n",
       " 'al Linguistics ( CL ) and Artificial Intelligence ( AI ) includes : semantic analysis of narratives ( Elson and McOTHERCIT ) ; summarizing fiction ( ) and performing information-extraction on fiction ( TARGETCIT ; modeling affect and reader-response in narrative ( ; McOTHERCIT ) ; properties of narrative such as novelty ( ) and irony ( ) ; models of discourse in narrative ( ) ; computa',\n",
       " 'eve that our good precision scores are a result of using this feature . In our experiments , we tried adding additional training data from other text corpora : the NUS Corpus of Learner English ( NUCLE ) ( TARGETCIT and the Gigaword corpus . Unfortunately , we did not see any consistent improvements over simply using the FCE corpus . The general rule of thumb that “ more data is better data ” did not seem to hold true',\n",
       " 'the Artificial Intelligence ( AI ) and Finance domains . which , given a ( term , hypernym ) pair , extract from the Web sentences matching manually-defined patterns like “ < hypernym > such as < term > , and * ” ( TARGETCIT b ) . This term extraction process is further extended by harvesting new hypernyms using the corresponding inverse patterns ( called DAP−1 ) like “ * such as < term1 > , and < term2 > ” . Similarly to ProToDoG , t',\n",
       " 'ed using the same linear function of the corresponding vectors and matrix . Tensor decomposition generalizes matrix factorization and has been applied to several NLP applications recently . For example , TARGETCIT proposed an approximation algorithm for PCFG parsing that relies on Kruskal decomposition . Van de modeled the composition of subject-verb-object triples using Tucker decomposition , which resu',\n",
       " 'syntactic constraints into hierarchical translation models . Despite some differences in the granularity of syntax units ( e.g. , tree fragments ( ) , treebank tags ( ) , and extended tags ( TARGETCIT ) , most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations . These approaches inevitably exacerbate the data sp',\n",
       " 'ternal morphological structure of word . This assumption can be traced back to the original IBM word-based models ( ) and several significantly improved models , including phrase-based ( ; TARGETCIT , hierarchical ( ) and syntactic ( ) models . These improved models worked well for translating languages like English with large scale parallel corpora available . Different from language',\n",
       " 'the open domain ( ) . For efficiency , they only use shallow features . Reverb ( ) is a state-of-the-art open domain extractor that targets verb-centric relations , which have been shown in TARGETCIT to cover over 70 % of open domain relations . Taking their output as input , algorithms have been proposed to resolve objects and relation synonyms ( Resolver ) , extract semantic networks 1028 ( SNE ) , and m',\n",
       " 'base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy . This broad characterization includes lexicalized parsers ( ) , unlexicalized parsers ( TARGETCIT , and latent variable parsers ( ) . Figures 1 ( a ) , 1 ( b ) , and 1 ( c ) show small examples of contextfree trees that have been annotated in these ways . When multi-part annotations are used in the same',\n",
       " 'ct such phenomena at the character level ; in this case , qu : cu corresponds to an individual misspelling feature . 4.7 Meta-features We included a number of document-specific metafeatures as suggested by TARGETCIT : the conzentrated 0.3 konzentrierte concentrated 0.4 Figure 1 : A cognate word influencing the spelling . average number of words per sentence , the average word length , as well as the total number of c',\n",
       " '> S1 , % prefix = > Cost , % max cost inside = > ICost % inside cost } . back ( Item , Action , Parent1 , Parent2 , C ) . tail ( Item , Ancestor ) . Listing 1 : Item structure Instead of the items proposed in ( TARGETCIT , we switched to items closer to those proposed in ( ) , corresponding 1Because we use Dynamic Programming techniques , keeping the b-best items at step m actually corresponds to keep more than t',\n",
       " 'ting work is restricted to n-grams only ( ) . Although all existing research use distributed architectures that follow the client–server paradigm , the real implementations are in fact different . TARGETCIT and store training corpora in suffix arrays such that one sub-corpus per server serves raw counts , and test sentences are loaded in a client . This implies that when computing the language mod',\n",
       " 'ions affect the similarity between sentences . 2.1 Two Basic Composition Operations As we want to keep this analysis simple , we focus on two basic operations : the simple additive model , ( presented in ( TARGETCIT and cited as a comparative method in many research papers ) , and the full additive model ( estimated in ( ) ) . We analyze these basic operations when resulting composed vectors are used to compute',\n",
       " 'der a more expressive model by using features defined over mentioncluster or cluster-cluster ( ) . For these models , the inference and learning algorithms are usually complicated . Very recently , TARGETCIT propose a probabilistic model which enforces structural agreement constraints between specified properties of mention cluster when using a mention-pair model . This approach is very related to the prob',\n",
       " 'ecent MT shared tasks – whether the evaluation was structured as ranking translations from best-to-worst , or by direct estimation of fluency and adequacy – of agreement between annotators decreasing ( TARGETCIT ; CallisonOTHERCIT ) . Inconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments , and is the target of this paper : by revising the annotation',\n",
       " 'uch a system with a feature set consisting of function words , POS bi-grams , and character n-grams . These features provide a strong baseline but can not capture many linguistic phenomena . More recently , TARGETCIT a ) considered syntactic features for this task , using logistic regression with features extracted from parse trees produced by a state of the art statistical parser . They investigated two classes of f',\n",
       " 'istic methods , we would like to pursue a principled learning-based generative framework , in analogy to the approaches for syntactic-based inference . An attractive work in this spirit is presented in ( TARGETCIT a ) , that propose a model which is both lexical and probabilistic . Later , improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-a',\n",
       " 'g feature introduced by . Language Train Dev OOV Dev OOV Words Word Types Word Tokens Tamil 452k 44 % 25 % Bengali 272k 37 % 18 % Hindi 708k 34 % 11 % Table 3 : Information about datasets released by TARGETCIT . Training data gives the number of words in the source language training set . OOV rates give the percent of development set word types and work tokens that do not appear in the training data . 4 Preli',\n",
       " 'proof-ofconcept study , our goal has not been to demonstrate the superiority of automatic optimisation over handcrafted behaviour . Previous studies have shown the advantages of optimisation ( ; TARGETCIT ; ) . Rather , our main goal has been to demonstrate that incremental NLG can be phrased as an optimisation problem and that reasonable action policies can be learnt so that an application withi',\n",
       " ') , question answering ( ) , wordsense discrimination ( Schütze , 1998 ) and disambiguation ( McOTHERCIT ) , semantic similarity computation ( ; McOTHERCIT ) and selectional preference modeling ( TARGETCIT . A shortcoming of DSMs is that they ignore the syntax within the context , thereby reducing the distribution to a bag of words . Composing the ∗ * Equally contributing authors distributions for “ Lincoln ”',\n",
       " 'ingle sentiment can be assigned . Previous approaches neither detect citation sentiment and context simultaneously nor use as large a corpus as we do . 2 Corpus Construction We chose the dataset used by TARGETCIT comprising 310 papers taken from the ACL Anthology ( ) . The citation summary data from the ACL Anthology Network1 ( ) was used . This dataset is rather large ( 8736 citations ) and since ma',\n",
       " 'f 60 % . 4.2 Automatic evaluation The use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics . Well known examples are the ROUGE ( TARGETCIT and BLEU ( ) evaluation metrics used in the summarization and MT communities . These metrics assess the quality of a system output by computing its similarity to one or more human-generated refe',\n",
       " 'sed in Chowdhury 353 and to recover some of such dependencies . We use the same techniques for unknown characters ( if any ) as described in . Our system uses the SVM-Light-TK toolkit3 ( TARGETCIT ; ) for computation of the hybrid kernels . The ratio of negative and positive examples has been used as the value of the costratio-factor parameter . The SL kernel is computed using the jSRE to',\n",
       " 'ERCIT scaled-up their work by using a crowdsourced emotion lexicon to track emotion dynamics over the course of many novels and plays , including Shakespeare ’ s . In the most recent work we are aware of , TARGETCIT analyzed emotional trajectories at the character level , showing how Miss Elizabeth Bennet ’ s emotions change over the course of Pride and Prejudice . 3 Character-to-Character Sentiment Analysis Characte',\n",
       " 'evaluation has received a lot of research interest . The Workshop on Statistical Machine Translation ( WMT ) hosts regular campaigns comparing different machine translation evaluation metrics ( ; TARGETCIT . In the WMT shared tasks , many new generation metrics , such as METEOR ( ) , TER ( ) , and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human jud',\n",
       " 'ir accuracy on the complete Hansards data . To evaluate , we used manual alignments of a small fraction of sentences , developed by , which we obtained from the shared task resources organized by TARGETCIT . The first 37 sentences of the corpus were development data , with manual alignments provided in a separate file . Test data consisted of an additional 447 sentences , for which we did not provide align',\n",
       " 'me , we measure agreement on each adaption proposed in Section 3 , as well as agreement on argument span determination . Whenever applicable , we also present ( roughly ) comparable statistics of the PDTB ( TARGETCIT . The results are summarized in Table 1 . Chinese PDTB ( % ) tkn no . F ( p/r ) ( % ) rel-ident 3951 * N/A 95.4 ( 96.0/94.7 ) rel-type 3951 95.1 N/A imp-sns-type 2967 87.4 72 arg-order 3059 99.8 N/A argument span',\n",
       " 'cy parsers require . A sample dependency tree from the French data set is shown in Figure 1 . We take two approaches to generating data . The first is traditional manual annotation , as previously used by TARGETCIT for multilingual syntactic treebank construction . The second , used only for English and Swedish , is to automatically convert existing treebanks , as in . 2.1 Automatic Conversion Since the Stan',\n",
       " '� ( w·φ ( i , j ) ) Pr [ j +— i ; d , w ] = 1 Zi ( w , γ ) . ( 7 ) Here Zi ( w , γ ) = P0 < k < i e f ( w·φ ( i , k ) ) is a normalizing constant and γ E ( 0 , 1 ] is a constant temperature parameter that is tuned on a development set ( TARGETCIT . We assume that the event that mention i links to a mention j is independent of the event that mention i′ links to j′ for i =� i′ . Inference with PL3M : Given the probability of a link as in Eq . ( 7 ) ,',\n",
       " 'a , August 8-9 , 2013. c�2013 Association for Computational Linguistics treebank2 , Chinese in the Classical Chinese Literature treebank ( ) or Persian in the Uppsala Persian Dependency Treebank ( TARGETCIT . In this paper , we describe the conversion of an existing Italian resource into the SD annotation scheme , with the final aim of developing a standard–compliant treebank , the Italian Stanford Dependen',\n",
       " 'ger corpora with semantics—such as Ontonotes ( ) or the Groningen Meaning Bank ( ) . These typically map words onto senses in ontologies such as WordNet , VerbNet ( ) and FrameNet ( TARGETCIT . However , limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y . Given the difficulty of annotating large amounts of text with semantics , v',\n",
       " 'rkshop on using Mechanical Turk for data annotation . They describe tasks for which MTurk can be used , and summarize a set of best practices . They also include references to the workshop contributions . TARGETCIT a ) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites . They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013 , pages 612',\n",
       " 'SL writer will typically make mistakes in only a small proportion of relevant structures . For example , determiner mistakes usually occur in 5 % to 10 % of noun phrases in various annotated ESL corpora ( TARGETCIT a ) . Third , an ESL writer may make multiple mistakes in a single sentence , which may give misleading local cues for individual classifiers . In the example shown in Figure 1 , the agreement error on the',\n",
       " 'parate topics and C-expressions . In sentiment analysis , researchers have jointly modeled topics and sentiment words ( ; Titov and McOTHERCIT ) . Our model is more related to the ME-LDA model in ( TARGETCIT , which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words . We also use such a variable . However , unlike sentiments and topics in reviews , which are emitted in t',\n",
       " 's better in the prediction of second order dependencies . 4.3.2 Related experiments on sequence models Bagging has been applied to enhance discriminative sequence models for Chinese word segmentation ( TARGETCIT b ) and POS tagging ( ) . For word segmentation , experiments on discriminative Markov and semi-Markov tagging models are reported . Their experiments showed that Bagging can consistently enhance a',\n",
       " 'nsition-based dependency parser . They show that using some morphological features ( root , case , and suffix ) outperforms a baseline using POS as the only feature , with both gold and predicted settings . TARGETCIT make use of MaltParser ’ s feature configuration file to take advantage of morphological features in parsing with gold data . Their experiments show that case and subordination type considerably increase',\n",
       " 'of the difficulty of recognizing deceptive opinions , there has been a widespread and growing interest in developing automatic , usually learningbased methods to help users identify deceptive reviews ( TARGETCIT ; ) . The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by who trained a supervised classifier to distinguish du',\n",
       " 'l ’ s method and the Downhill Simplex algorithms are approaches based on heuristics to select lines λM + γdM . It is difficult to find theoretically sound reasons why one approach is superior . Therefore TARGETCIT instead choose the direction vectors dM at random . They report that this method can find parameters that are as good as the parameters produced by more complex algorithms . 2.3 describe a proc',\n",
       " '( exact , stemming , synonyms , and paraphrasing ) , a lexical metric accounting for F-Measure . Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees ( TARGETCIT : DP-HWCMi , -4 for grammatical categories chains , DP-HWCMir-4 over grammatical relations , and DP-Or ( ⋆ ) over words ruled by non-terminal nodes . Also , one measure that estimates the similarity over const',\n",
       " 'tables ) , but we continued to report systems as winners whenever there was not statistically significantly outperformed by any other system . 4.2 Ratio of Wins and Ties In the following years ( ; TARGETCIT , we abandoned the idea of using fluency and adequacy judgments , since they showed to be less reliable than simple ranking of system translations . We also started to interpret the 5-system comparison',\n",
       " 'ary . 4http : //www.metaoptimize.com/projects/ wordreprs/ 5http : //mpqa.cs.pitt.edu/ 6http : //www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN ( TARGETCIT b ) - 79.0 RAE ( rand ) ( ) 85.7 76.8 TCRF ( ) 86.1 77.3 RAE ( init ) ( ) 86.4 77.7 NB ( ) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3 :',\n",
       " 'as demonstrated that promoting topic coherence in natural-language discourse helps discourse modeling ( ) . We extend PROFINDER to leverage this intuition by incorporating a “ stickiness ” prior ( TARGETCIT to encourage neighboring clauses to stay in the same frame . Specifically , along with introducing the background frame , the frame transition component now becomes YX PA−DEP ( depi , j|Si , j ) PF−TRAN ( Fi+1|Fi',\n",
       " 'document requires extensive temporal knowledge , most notably , the document ’ s creation date to ground its relative expressions ( e.g. , this year = 2012 ) . Not only did the latest TempEval competitions ( TARGETCIT ; ) include tasks to link events to the ( known ) document creation time , but state-of-the-art event-event ordering algorithms also rely on these timestamps ( ) . This knowledge is assumed',\n",
       " 'research areas in the ACL ARC . 4 Collocation Segmentation The ACL ARC contains many different segmentation types : discourse segmentation ( ) , topic segmentation ( ) , text segmentation ( TARGETCIT , Chinese text segmentation ( ) , word segmentation ( ) . Segmentation is performed by detecting boundaries , which may also be of several different types : syllable boundaries ( M¨uller , 200',\n",
       " 'transfer experiments for dependency parsing . The baseline results are comparable to those in McOTHERCIT and thus also significantly outperform the results of recent unsupervised approaches ( ; TARGETCIT . Importantly , crosslingual cluster features are helpful across the board and give a relative error reduction ranging from 3 % for DA to 13 % for PT , with an average reduction of 6 % , in terms of unlabel',\n",
       " 'to discover template-like representations ( ) or sentence-level paraphrases which could be used for extraction or generation . Various approaches to concept learning use clustering techniques . ( TARGETCIT apply various clustering procedures to learn a small number of slots in three typical information extraction domains , using manually annotated data and fixing the number of concepts to be learnt . ( OTH',\n",
       " 'tional word clusters have a positive impact on syntactic accuracy , but little or no impact on morphological accuracy . This is consistent with previous findings in the literature , mainly from English ( TARGETCIT ; ) , and it is interesting to see that it holds also for richly inflected languages and when added on top of features derived from external lexical resources . One issue worth discussing is the',\n",
       " 'hey only use as features morphosyntactic clues ( passives , gerunds and to-infinitives ) that can be found in certain function words and partof-speech tags . Even better results , however , were obtained by TARGETCIT in a postprocessing step that makes use of rules inspired by work in theoretical linguistics . Gabbard et al ( 2006 ) reported further improvement largely by recasting the Campbell rules as features to s',\n",
       " 'by µ and σ , with µ represents the average time point that the sub-event emerges and σ determines the duration of the sub-event . These distributions bear similarities with the previous work ( ; TARGETCIT . In addition , there are often background or “ noise ” topics that are being constantly discussed over the entire lex sim ( ci , cj ) = { 1 ci ( cj ) is part of cj ( ci ) 1 EditDist ( ci , cj ) < θ 0 Otherwise 1155 ev',\n",
       " 'those from other systems . 1 Introduction This paper describes the systems submitted by the National Research Council Canada ( NRC ) for the Cross-Lingual Word Sense Disambiguation task at SemEval 2013 ( TARGETCIT . As in the previous edition ( ) , this word sense disambiguation task asks systems to disambiguate English words by providing translations in other languages . It is therefore closely related to',\n",
       " 'possessives interpretation as well . Concurrent with the lack of NLP research on the subject is the absence of available annotated datasets for training , evaluation , and analysis . The NomBank project ( TARGETCIT provides coarse annotations for some of the possessive constructions in the Penn Treebank , but only those that meet their criteria . 7 Conclusion In this paper , we present a semantic relation inventory',\n",
       " 'Building a high-performing parser for a language with no existing treebank is still an open problem . Methods that use no supervision at all ( ) or small amounts of manual supervision ( ; TARGETCIT have been extensively studied , but still do not perform well enough to be deployed in practice . Projection of dependency links across aligned bitexts ( ) gives better performance , but crucially',\n",
       " 'omprising sentence level , worker level and crowd ranking based features . However , automatic evaluation of translation quality is difficult , such automatic methods being either inaccurate or expensive . TARGETCIT have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations . The quality of the validations is ensured using goldstandard sentence translations . O',\n",
       " 'ted by ˇStˇep´anek ( 2006 ) who claims that it can represent shared modifiers using a single additional binary attribute , while MS would require a more complex co-indexing attribute . An argumentation of TARGETCIT follows a similar direction : We would like to change our [ MS ] handling of coordinating conjunctions to treat the coordinating conjunction as the head [ PS ] because this has fewer ambiguities than [ MS ] .',\n",
       " 'e training point of view this approach can be seen as similar to cotraining ( ) , other applications of which to NLP are too numerous to list here . Most closely related is the joint inference in TARGETCIT , the main difference being that it relies on a manually annotated parallel corpus , aligned on the argument level , and evaluates only the inference procedure and only on in-domain data . Other related',\n",
       " 'esents factual information ( ) . Subjective text may express positive , negative , or neutral opinion . Previous work addressed the problem of identifying the polarity of subjective text ( ; TARGETCIT . Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons ( i.e . lists of positive and negative words ) . Several approaches have been devised for bu',\n",
       " 'aining data . 1 Introduction The adoption of discriminative learning methods for SMT that scale easily to handle sparse and lexicalized features has been increasing in the last several years ( ; TARGETCIT . However , relatively few systems take full advantage of the opportunity . With some exceptions ( ) , most still rely on tuning a handful of common dense features , along with at most a few thousa',\n",
       " 'rch away from Minimum Error Rate Training ( MERT ) ( ) , and toward other discriminative methods that can optimize more features . Examples include minimum risk ( ) , pairwise ranking ( PRO ) ( TARGETCIT , RAMPION ( ) , and variations of the margin-infused relaxation algorithm ( MIRA ) ( ) . While the objective function and optimization method vary for each optimizer , they can all be broadly',\n",
       " 'ing for a particular viewpoint in a debate , as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate . Work on debate summarisation and subgroup detection ( TARGETCIT ; AbuOTHERCIT ) has often used data from online debate forums , particularly those forums where users are asked to select whether they support or oppose a given proposition before they can participate .',\n",
       " 'ld one from each corpus , then linearly interpolate them , choosing weights that minimise the perplexity on a suitable heldout set of in-domain data . This method has been applied by many authors ( e.g . ( TARGETCIT ) , and is implemented in popular language modelling tools like IRSTLM ( ) and SRILM ( ) . Similar interpolation techniques have been developed for translation model interpolation ( OTHERCI',\n",
       " 'T ) a machine learning approach . ( ) create a Dutch sentiment lexicon based on reviews at an online bookseller . The helpfulness of online reviews has been investigated by e.g . ( ) while ( TARGETCIT have modeled review comments . From an information retrieval perspective , the INEX social book search competition has explored the use of online reviews from Amazon and LibraryThing to create book reco',\n",
       " 'the news in the news collections with a standard NLP pipeline : tokenization and sentence boundary detection ( ) , part-of-speech tagging , dependency parsing ( ) , coreference resolution ( TARGETCIT and entity linking based on Wikipedia and Freebase . Using the Freebase dataset , each entity is annotated with all its Freebase types ( class labels ) . In the end , for each entity mentioned in the docume',\n",
       " 'ot been much theoretical work on semantic identity in the NLP community . But there has been a considerable amount of work on the problem of coreference . Focusing on entity coreference are ( McOTHERCIT ; TARGETCIT ; ) . Focusing on event coreference are ( ; 2010 ) . Anaphora and bridging reference are discussed in ( ; 2007 ) . Relevant to events is the TIME-ML corpus ( ) , which provides',\n",
       " 'endency distance into six parts which are 1 if > 1 , 2 if > 2 , ... , 5 if > 5 , 10 if > 10 . 3.3 Third-Order Features The order of parsing is defined according to the number of dependencies it contains ( TARGETCIT . Collins classifies the third-order as two models , Model 1 is all grand-siblings , and Model 2 is grand-siblings and tri-siblings . A grand-sibling is a 4-tuple of indices ( g , h , m , s ) where g is grand',\n",
       " 'dicate the metonymy . We will call all such grammatically related words and the grammatical relations the local context of the PMW . Such types of local context have been used by most prior approaches ( TARGETCIT ; , among others ) . However , Example 2 shows that the local context can be ambiguous or often weak , such as the verb to be . In these examples , the wider context ( database , keyword ) is a better',\n",
       " 'ssible meanings of a given term . In order to automatically obtain large domain glossaries , in recent years computational approaches have been developed which extract textual definitions from corpora ( TARGETCIT ; ) or the Web ( ) . The methods involving corpora start from a given set of terms ( possibly automatically extracted from a domain corpus ) and then harvest textual definitions for these',\n",
       " 'to recognize aspect terms and opinion expressions ( ) , recent work seems to favor generative statistical models more ( ; Titov and McOTHERCIT ; Titov and McOTHERCIT ; Blei and McOTHERCIT ; TARGETCIT ; ) . One typical problem with these models is that many discovered aspects are not meaningful to end-users . Some of these studies focus on distinguishing aspects in terms of sentiment variatio',\n",
       " 'o built a corpus including about 800 Kb text . Sun et al . ( 2009 , 2010 ) used a corpus including 435 sentences ( 4067 words ) as the test set in their research . These corpora are all for word segmentation . TARGETCIT described the initial effort in segmenting the Dzongkha ( Tibetan ) scripts . Their experiments are made on 8 corpora in different domains , which include only 714 words in total . described the D',\n",
       " 'e adapted , whereas in BSS we still have full control over the estimation of such model and need not to aim at a specific domain , although it might often be so . BSS is related with instance weighting ( TARGETCIT ; ) . Adaptation and BSS can be considered to be orthogonal ( yet complementary ) problems under the instance weighting paradigm . In such case , instance weighting can be considered to span a comp',\n",
       " 'eturns the optimal solution . 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased ( ) and syntax-based translation models ( ; TARGETCIT . There is a line of work proposing exact algorithms for machine translation decoding . Exact decoders are often slow in practice , but help quantify the errors made by other methods . Exact algorithms p',\n",
       " 'ficant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system . 1 Introduction Even though phrase-based machine translation ( PBMT ) ( ) and tree-based MT ( ; TARGETCIT ; ) systems have achieved great success , many problems remain for distinct language pairs , including long-distant word reordering . To improve such word reordering , one promising way is to sepa',\n",
       " 'sues with markable identification and anaphoricity across the different domains . 4.2 Coreference Resolution Models We conducted experiments using two coreference resolution architectures . Reconcile4 ( TARGETCIT is a freely available pairwise mention classifier . For classification , we chose Weka ’ s ( ) Decision Tree learner inside Reconcile . Reconcile contains roughly 60 features ( none lexical ) , largely',\n",
       " 'trative annotations are shown in Table 2 . It is worth highlighting that these examples signal that our models are considering pattern-free descriptions , that is to say , unlike other systems ( ; TARGETCIT which consider definitions aligning an array of well-known patterns ( e.g. , “ is a ” and “ also known as ” ) , our models disregard any class of syntactic constraint . As to a baseline system , we accounted fo',\n",
       " 'iguity on IR . He concluded that because the effectiveness of WSD can be negated by inaccurate WSD performance , high accuracy of WSD is an essential requirement to achieve improvement . In another work , TARGETCIT used a manually sense annotated corpus , SemCor , to study the effects of incorrect disambiguation . They obtained significant improvements by representing documents and queries with accurate senses as w',\n",
       " 'result . Note that , in CMSAE , an opinion target should be proposed along with its polarity . The correctness of the polarity is also necessary . Soft Evaluation : The soft evaluation metric presented in ( TARGETCIT 2010 ) is adopted by CMSAE . The span coverage c between each pair of the proposed target span s and the gold standard span s ’ is calculated as follows , s S s S ( 12 ) In Equation 12 , the operator |· |cou',\n",
       " 'sks including syntactic parsing ( ) , semantic parsing ( ) , partof-speech tagging ( ) , semantic role labeling ( ) , named entity recognition ( ) . machine translation ( TARGETCIT ; ) and surface realization in generation ( ) . However , to our knowledge , there has been no previous attempt to apply discriminative reranking to grounded language acquisition , where go',\n",
       " 'ion was proposed by as reluctant sentence paraphrasing under length constraints . Jing and McOTHERCIT analyzed human-generated summaries and identified a heavy reliance on sentence reduction ( TARGETCIT . The extraction by of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task ( ; McOTHERCIT ; Galley and McOTHERCIT',\n",
       " 'ge model and decoding method as the baseline system . With large training data , moving to a 5-gram language model , increasing the cube pruning pop limit to 1000 , and using Minimum Bayes-Risk decoding ( TARGETCIT over 500-best lists collectively show a slight improvement . Monolingual post-processing yields further improvement . This decoding/processing scheme corresponds to our final translation system . 4.1 Imp',\n",
       " 'work , ( ) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences . have also discussed the problem of generating surveys of multiple papers . TARGETCIT presented experiments on generating surveys of scientific topics starting from papers to be summarized . More recently , have presented initial results on automatically generating related work',\n",
       " 'of the Web and , consequently , the increase of nonEnglish speaking surfers , we have witnessed an upsurge of interest in multilinguality . SemEval-2010 tasks on cross-lingual Word Sense Disambiguation ( TARGETCIT and cross-lingual lexical substitution ( ) were organized . While these tasks addressed the multilingual aspect of sense-level text understanding , they departed from the traditional WSD paradigm',\n",
       " 'of inversion and concatenation . Many alignment-inspired features can be used in MRM . This paper only uses those commonly-used ones that have already been proved useful in many previous work ( ; TARGETCIT ; ) . Following the common practice in SMT research , the MERT algorithm ( ) is used to tune feature weights in MRM . Due to the fact that all FDTs of each sentence pair share identical tr',\n",
       " 'inguistic Annotation Workshop , pages 184–192 , Jeju , Republic of Korea , 12-13 July 2012. c�2012 Association for Computational Linguistics not follow the social norm of taking face into account , such as TARGETCIT suggest . The dialogue only progresses due to the social intentions of the other discourse participants in reaction to the adversarial individual , e.g . defending one ’ s honor . We adopt the Grosz and Sid',\n",
       " 's a stronger form of the distributional hypothesis of . In fact , this idea can be related to the notion of distributional generality , introduced by Weeds , Weir , and McOTHERCIT and developed by TARGETCIT . A term x is distributionally more general than another term y if x occurs in a subset of the contexts that y occurs in . The idea is that distributional generality may be connected to semantic genera',\n",
       " 'n captured in the biomedical literature into structured information that can be used to index and analyse bio-molecular relationships . This year ’ s task builds on previous instantiations of this task ( TARGETCIT ; ) , with only minor changes in the task definition introduced for 2011 . The task organisers provided full text publications annotated with mentions of biological entities including proteins a',\n",
       " 'se . We refer to the UEM algorithm with parameter y as UEM.y . 3.1 Relationship between UEM and Other EM Algorithms The relation between unconstrained versions of EM has been mentioned before ( ; TARGETCIT . We show that the relationship takes novel aspects in the presence of constraints . In order to better understand different UEM variations , we write the UEM E-step ( 6 ) explicitly as an optimization pr',\n",
       " 'is still less frequent than many other IS classes and recognition of minority classes is well known to be more difficult . We therefore use a cascaded classification algorithm to address this problem ( TARGETCIT . 814 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 814–820 , Seattle , Washington , USA , 18-21 October 2013. c�2013 Association for Computational Linguist',\n",
       " 'expressions from natural language . Language Grounding There is a large body of research mapping natural language to some form of meaning representation ( ; Wong and Mooney , 827 2006 ; ; TARGETCIT ; ) . In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated . The most directly related work in these domains , is that by Kw',\n",
       " 'WSD resources remains quite fragmented . The few publically available implementations of individual disambiguation algorithms , such as SenseLearner ( ) , SenseRelate : :TargetWord ( ) , UKB ( TARGETCIT , and IMS ( ) , are all tied to a particular corpus and/or sense inventory , or define their own custom formats into which existing resources must be converted . Furthermore , where the algorithm d',\n",
       " 'pe Systems < 40 Full D dep2 90.86 88.38 MaltParser 87.1 85.8 86.6 - C MSTm.lt† 90.55 88.82 OTHERCIT† 90.63 88.84 OTHERCIT† 89.40 86.63 H 88.9 86.1 Ours 91.48 89.62 S - 87.26 TARGETCIT 92.34 89.91 - 91.59 Table 2 : Comparison of different approach on CTB4 test set using UAS metric . MaltParser = ; MSTMalt=Nivre and McOTHERCIT . Type D = discriminative dependency parser',\n",
       " 'tial , implicit , and affixal negation . Further research could focus on these aspects of negation . Apart from scope resolvers for negation , several full scope resolvers have been developed for modality . TARGETCIT a ) test whether the scope resolver for negation ( ) is portable to resolve the scope of hedge cues , showing that the same scope resolution approach can be applied to both negation and hedging .',\n",
       " 'However , perceptron training with inexact search is less studied for bottom-up parsing and , more generally , inference over hypergraphs . In this paper , we generalize the violation-fixing perceptron of TARGETCIT to hypergraphs and apply it to the cube-pruning parser of Zhang and McOTHERCIT . This results in the highest reported scores on WSJ evaluation set ( UAS 93.50 % and LAS 92.41 % respectively ) without the a',\n",
       " 'number of mentions . A number of possible avenues for future study are apparent . First , our alignment to a knowledgebase can benefit from more document-aware linking to entities , such as the Wikifier ( TARGETCIT . Second , we would like to augment mention features with additional information available from the knowledge base , such as Wikipedia categorization and gender attributes . We also want to investigate a',\n",
       " 'ers , or to extensions of those like the ones by or , because these already have it . The idea of creating dependency arcs of length 1 in advance to help the classifier has been used by TARGETCIT . However , their system creates such arcs in a separate preprocessing step rather than dynamically by adding a transition to the parser , and our approach obtains better LAS and UAS results on all the',\n",
       " 'efs and Cuay´ahuitl ( 2011 ) in the near future . The NLG system is thus implemented on top of McKeown ’ s ( 1985 ) Document Structuring Schemata ( using the recent implementation OpenSchema6 ) and SimpleNLG ( TARGETCIT . I use two schemata , in one the n-balls are presented in order while in the other the attributes are presented in order . One of the schemata I am using is shown in Fig . 2 . Document structuring schema',\n",
       " 'eju , Republic of Korea , 8-14 July 2012. c�2012 Association for Computational Linguistics experiment results along with a discussion in Section 5 . Finally , Section 6 draws a conclusion . 2 Related Works TARGETCIT presented a phrasebased transliteration system that groups characters into substrings mapping onto target names , to demonstrate how a substring representation can be incorporated into CRF models with',\n",
       " 'a sentence involves summing over all possible derivations , we instead find the MR associated with the most probable derivation . 8 Experimental setup and evaluation We evaluate the system on GeoQuery ( TARGETCIT , a parallel corpus of 880 English questions and database queries about United States geography , 250 of which were translated into Spanish , Japanese , and Turkish . We present here additional translatio',\n",
       " 'In particular , our pFSM model and the log-linear parameterization were inspired by . Another body of literature that is closely related to this work is FSM models for word alignment ( ; TARGETCIT . The stochastic Inversion Transduction Grammar in for instance , is a pFSM with special constraints . More recently , further explored the connection between Linear Transduction Gramma',\n",
       " 'possibility , but still requires the computation of such reduction cf . ( ) . Other approaches use randomised indexing for storing counts or hashing functions to approximate counts and measures ( TARGETCIT ; ) . Another possibility is the usage of distributed processing like MapReduce . In ( ) a DT is computed using MapReduce on 200 quad core nodes ( for 5.2 billion sentences ) respectively 2',\n",
       " 'n. Our model also outperforms two more baselines : KGSC predicts that the IF will press the button with the minimal overall distance , which is the distance metric used by the “ movement-based system ” of TARGETCIT ; random visible selects a random button from the ones that are currently visible to the IF . The fact that this last baseline does not approach 1 at action time suggests that multiple buttons tend to',\n",
       " 'hods are not susceptible to problems with local maxima , and give consistent parameter estimates . In this paper we derive a spectral algorithm for learning of latent-variable PCFGs ( L-PCFGs ) ( ; TARGETCIT . Our method involves a significant extension of the techniques from . L-PCFGs have been shown to be a very effective model for natural language parsing . Under a separation ( singular value ) co',\n",
       " 'se methods all employ generative models with strong independence assumptions and weak feature representations , which upper bounds their accuracy far below that of feature-rich discriminative parsers ( TARGETCIT ; ) . In this paper , we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parser',\n",
       " 'intervals , are used to evaluate the performance of the summaries . In order to obtain a more comprehensive measure of summary quality , we also conduct manual evaluation on TAC data with reference to ( TARGETCIT ; ) . 5.2 Comparison with other Bayesian models In this subsection , we compare our model with the following Bayesian baselines : KL-sum : It is developed by Haghighi and Vanderwende ( ) by',\n",
       " 's are due to high corpus frequency but provide no disambiguating information , and the weighting also allows the T parameter to be more consistently set across corpora of different sizes . Second , while TARGETCIT used only nouns as vertices in the graph , we include both verbs and adjectives due to needing to identify senses for both . Third , for graded senses , we disambiguate a context by reporting all overlapp',\n",
       " 'science . Much of this work has focused on up-weighting subsets of the training or language modeling data that are most similar to the new domain ( ) . Other work has focused on literary texts ( TARGETCIT ; ) . Most relevant is , which presents a model for translating Italian poetry into English . That work focuses on preserving meaning as well as rhythm and is an interesting first attemp',\n",
       " 'm bilingual interpretation for English and Spanish . Both of the above systems rely primarily on hand-built rules . 3 Temporal Representation We define a compositional representation of time , similar to TARGETCIT , but with a greater focus on efficiency and simplicity . The representation makes use of a notion of temporal types and their associated semantic values ; a grammar is constructed over these types , and',\n",
       " 'ency trees , using respectively the schemes defined at the Penn Chinese Treebank ( ) and Stanford ( ) . During the practicums , adaptations to these schemes for Classical Chinese ( ; TARGETCIT were presented . In the fifth week , the web interface for searching and visualizing treebanks , which would later be used for a research assignment ( see section 5 ) , was demonstrated . Also , students were',\n",
       " 'soft 79.14 71.55 75.15 82.58 77.96 80.20 Table 1 : NER results on bilingual parallel test set . Best numbers on each measure that are statistically significantly better than the monolingual baseline and TARGETCIT b ) are highlighted in bold . training the reranker , and the reranker model selection was performed on the development dataset . 5 Bilingual NER Results The main results on bilingual NER over the test po',\n",
       " ', which strongly favors the EM2010 system . 8 Extracting Family Relationships In this section , we describe an application of the speaker identification system to the extraction of family relationships . TARGETCIT extract unlabeled networks where the nodes represent characters and edges indicate their proximity , as indicated by their interactions . Our goal is to construct networks in which edges are labeled by',\n",
       " 'tatistical architectures . While early NLG systems were mainly based on manually created rules ( ) , later approaches started applying statistical methods to the subtasks involved in generation ( TARGETCIT , focusing on scalability and easy portability and often relying on overgeneration and subsequent ranking of generation possibilities . Personalization has been a concern in both strands of research . P',\n",
       " 'based training methods presented in ( ; Nuhn 368 Method BLEU Time ( hours ) Baseline system ( identity translations ) 6.9 1a . EM with 2-gram LM ( ) 15.3 ∼850h 1b . EM with whole-segment LM ( TARGETCIT b ) 19.3 1c . Bayesian IBM Model 3 with 2-gram LM ( ) 15.1 2a . EM+Context with 2-gram LM ( ) 15.2 50h 2b . EM+Context with 3-gram LM ( ) 20.9 200h 3 . Bayesian ( standard ) Gibbs sampli',\n",
       " 'nd Daume III , 2010 ) and multilingual LDA ( ) . They require a bilingual dictionary or document-level parallel corpora . Unsupervised object matching methods have been proposed recently ( ; TARGETCIT ; ) . These methods are promising in terms of language portability because they do not require external language resources . ( ) proposed kernelized sorting ( KS ) ; it finds one-toone corre',\n",
       " 'this work attempts to infer new tuples by gathering path evidence over the whole corpus ( macroreading ) . In addition , their work involves a few thousand examples , while we aim for Web-scale extraction . TARGETCIT use a KB ( YAGO ) to aid the generation of features from free text . However their method is designed specifically for extracting hierarchical taxonomic structures , while our algorithm can be used to dis',\n",
       " '+5-GRAM/PLSA1 28.78 0.31 2-gram/4-SLM+5-gram/PLSA1 language model gives 1.19 percentage point Bleu score improvement over the baseline and 0.68 percentage point Bleu score improvement over the 5-gram . TARGETCIT studied the performance of machine translation on Hiero , the Bleu score is 33.31 % when n-gram is used to re-rank the N-best list ; the Bleu score becomes significantly higher ( 37.09 % ) when the n-gram i',\n",
       " 'oc 3 What we did : build a Hindi GF grammar , compare Hindi/Urdu We first developed a new grammar for Hindi in the Grammatical Framework ( GF ) ( ) using an already existing Urdu resource grammar ( TARGETCIT . This new Hindi resource grammar is thus the first thing we report , though it is not in itself the focus of this paper . 4 Figure 1 : Hindi/Urdu Functor . We used a functor style implementation to devel',\n",
       " 'best reduces error rate in dependency Fscore by 1 % on average , while some methods produce substantial decreases in performance . 1 Introduction Most start-of-the-art natural language parsers ( ; TARGETCIT ; ) use lexicalised features for parse ranking . These are important to achieve optimal parsing accuracy , and yet these are also the features which by their nature suffer from data-sparseness p',\n",
       " 'he default settings is used to calculate the BLEU scores . To test whether a performance difference is statistically significant , we conduct significance tests following the paired bootstrap approach ( TARGETCIT paper , ‘ * * ’ and ‘ * ’ denote p-values less than 0.01 and in-between [ 0.01 , 0.05 ) , respectively . Table 2 lists the rule table sizes . The full rule table size ( including HD-HRs and NRRs ) of our HDHPB mode',\n",
       " 't proposes the exact same corrected sentence as the gold standard edits . This problem has also been recognized by the HOO shared task organizers ( see ( ) , Section 5 ) . Our MaxMatch ( M2 ) scorer ( TARGETCIT overcomes this problem through an efficient algorithm that computes the set of system edits which has the maximum overlap with the goldstandard edits . We use the M2 scorer as the main evaluation metri',\n",
       " 's by extending the translation model , either by adding extra models , such as lexicalized ( ) or discriminative ( ) reordering models or by directly modelling reordering in hierarchical ( TARGETCIT or syntactical translation models ( ) . Preordering is another common strategy for handling reordering . Here the source side of the corpus is transformed in a preprocessing step to become more s',\n",
       " 'rized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share . This perspective was first adopted by ( ) and ( TARGETCIT and then , explored in details in ( ) , ( ) or ( ) . The problem of improving the results of the “ classical ” implementation of the distributional approach as it can be found in ( OTHE',\n",
       " ', or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice ( ) . We will show how existing Cross-Lingual Word Sense Disambiguation tasks ( ; TARGETCIT can be directly seen as machine translation lexical choice ( Section 2 ) : their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WS',\n",
       " 'kit ( ) . For English language modeling , we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit ( ) . All experiments are conducted using the Moses phrase-based SMT system ( TARGETCIT . We use MERT ( ) for decoding weight 4LDC Catalog IDs : LDC2005E83 , LDC2006E24 , LDC2006E34 , LDC2006E85 , LDC2006E92 , LDC2006G05 , LDC2007E06 , LDC2007E101 , LDC2007E103 , LDC2007E46 , LDC2007E86 , LDC',\n",
       " 'rated on individual words rather than sentential representations . Several approaches used nMSRPC uniOverlapi1 , i2 = E ( 18 ) k=1 553 Model Acc . F1 Baseline 66.5 79.9 70.3 81.3 70.6 80.5 TARGETCIT 72.0 81.6 72.6 81.3 ( ( D ) 73.0 82.3 ( + ) 73.5 82.2 74.1 82.4 75.6 83.0 76.1 82.7 76.8 83.6 Table 6 : Overview of results on the MSRCP ( test',\n",
       " 'n-NLP clustering and path creation techniques . Sophisticated approaches such as linear programming and evolutionary algorithms have also been proposed for generating summaries and stories ( McOTHERCIT ; TARGETCIT . In contrast , use a recommender system approach to generate museum tours on the basis of ratings stored within a dynamic user model , and propose the additional use of data mining te',\n",
       " 've speakers of English : article , subject-verb agreement , noun number , and verb form . A significant proportion of research has focused on correcting mistakes in article and preposition usage ( ; TARGETCIT b ) . Several studies also consider verb-related and noun-related errors ( ) . The predictions made by individual models are then applied independently ( ) or pipelined ( ) . The stan',\n",
       " 'of words . Furthermore , we propose a more straightforward and a more compact way of encoding the lemmatisation rules . The majority of other methods are concentrated on lemmatising out-of-lexicon words . TARGETCIT propose a joint model for assigning the set of possible lemmas and POS tags to out-of-lexicon words which is language independent . The lemmatizer component is a discriminative character transducer tha',\n",
       " 'ch on multilingual approaches , from morphosyntactic ( ) and syntacticosemantic ( Peirsman and Pad´o , 2010 ) phenomena to high-end tasks like textual entailment ( ) and sentiment analysis ( TARGETCIT . These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language : however , this objective is still far fr',\n",
       " 'dialog systems includes detecting and handling out of domain utterances for generating feedback ( ) ; using domain-restricted lexical semantics ( ) ; and work on manual data expansion ( TARGETCIT . Our work follows up on this research but provides a systematic investigation of how data expansion , lemmatisation and synonym handling impacts the performance of a supervised QA engine . 3 Experiment',\n",
       " 'pt representation which is common for all Indian languages ( Ganapathiraju 2005 ) . Punjabi machine transliteration for Punjabi language from Shahmukhi to Gurmukhi used the set of transliteration rules ( TARGETCIT . Sproat presented a formal computational analysis of Brahmi scripts ( Sproat 2002-2004 ) . Kopytonenko focused on computational models that perform grapheme-to-phoneme conversion ( Kopytonenko 2006 ) . Gan',\n",
       " 'long history of methods using suface-level lexical patterns for extracting relational facts from text corpora ( ) . Syntactic information in the form of dependency paths have been explored in ( TARGETCIT ; ) . A method of latent embedding of relation instances for sentence-level relation extraction was shown in ( ) . However , none of this prior work makes explicit use of the background KB',\n",
       " 'roposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents , using compositional operations in the vector space ( ; TARGETCIT . All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the',\n",
       " 's and relations , are expressed at the word-level ( ) . Free word order can be handled by non-projective parsing algorithms via either post-processing the output of a strictly projective parser ( TARGETCIT , combining adjacent ( ) or non-adjacent sub-structures ( McOTHERCIT ) . Nevertheless , there is no general solution for resolving rich morphology issue and hence many researcher focus on features',\n",
       " 'xplicitly evaluate the quality of the learned templates , which would require a significant amount of manual evaluation . Instead , they are evaluated extrinsically . We encode the templates as features ( TARGETCIT that could be selected or ignored in the succeeding abstract ranking model . 5.2 Template Filling An Overgenerate-and-Rank Approach . Since filling the relation instances into templates of distinct stru',\n",
       " 'the final mention should be clustered with either of them in subsequent comparisons . 4 Experiments 4.1 Setup We evaluate our system on the ACE 2004 annotated dataset ( ) . Following the setup in TARGETCIT , we split the corpus into training , development , and test sets , resulting in 268 documents in the train set , 107 documents in the test set , and 68 documents in the development set . The data is proces',\n",
       " 'c techniques such as latent semantic analysis ( ) . Topic models can be improved by better modelling the semantic aspects of text , for instance integrating collocations into the model ( ; TARGETCIT or encouraging topics to be more semantically coherent ( ) based on lexical coherence models ( ) , modelling the structural aspects of documents , for instance modelling a document as a se',\n",
       " 'on contains words like block , stop , constrain . Previously LIWC was successfully used to analyze the emotional state of bloggers and tweeters ( ) and to identify deception and sarcasm in texts ( TARGETCIT ; Gonz´alez-Ib´a˜nez et al. , 2011 ) . When LIWC analyzes texts it generates statistics like number of words found in category Ci divided by the total number of words in the text . For our metaphor polari',\n",
       " 'ation errors . 6 This is consistent with previous results obtained in non-adapted settings using other measurement techniques : search errors account for less than 5 % of the error in modern MT systems ( TARGETCIT , or 0.13 % for small beam settings with a “ gap constraint ” ( ) . We use a beam value of 200 for all other experiments in this work . 7.1 Quantitative Results Results are summarized in Tables 3 an',\n",
       " 'ng the highest scoring derivation y∗ that will generate the correct query z using the learned lexicon L. 6 Data For our database D , we use the publicly available set of 15 million REVERB extractions ( TARGETCIT .3 The database consists of a set of triples r ( e1 , e2 ) over a vocabulary of approximately 600K relations and 2M entities , extracted from the ClueWeb09 corpus.4 The REVERB database contains a large cro',\n",
       " 'on from human edited sample generalization ( e.g. , , McOTHERCIT ) Galley and McOTHERCIT ) , and the inclusion of edits beyond deletion , e.g. , substitutions , as has been explored by e.g. , , TARGETCIT , . 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text . Intrinsic and extrinsic evaluations confirm t',\n",
       " 'Extraction Our baseline extracts the most central sentence from the co-occurring text and uses descriptive words from that sentence as the image annotation . Unlike sentence extraction techniques from TARGETCIT a ) , we determine which sentence to extract using the term frequency distribution directly . We extract the sentence with the minimum KLdivergence to the entire document.8 5 BBC Dataset Experiments 5.1',\n",
       " 'malisms the parsing complexity depends exponentially on the rank of the grammar . It is also critically important for tractable statistical machine translation ( SMT ) . Syntaxbased SMT systems ( ; TARGETCIT typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees , such as synchronous context-free grammars ( SCFGs ) ( ) , synchronous tree-sub',\n",
       " 'ement over the individual baseline systems . System combination on the phrasal level can be realised using so-called Confusion Networks . Previous work on this approach are described in more detail in ( TARGETCIT ; ) . The algorithm chooses one of the given MT systems as backbone or skeleton of the hybrid translation , while all other translations are connected using word alignment techniques . Together ,',\n",
       " 'nderstanding ; ( RR ) a restaurant recommendation system , which implements a three-stage frame-based dialogue system that uses rule-base natural language understanding , and ( CH ) our IRIS chatting agent ( TARGETCIT . Regarding input/output modalities , speech and text can be used as input channels for user utterances . Direct connections via microphone and keyboard are supported , as well as remote connections via',\n",
       " 'assifiers that assign a role to each constituent independently of others , and only modeled limited correlations among roles in a sequence ( ) . The correlations have been modeled via role sets ( TARGETCIT , role repetition constraints ( ) , language model over roles ( ) , and global role sequence ( ) . Unsupervised SRL systems have explored even fewer correlations . Lang and Lapata ( 20',\n",
       " 'sed in our experiments Word n-grams Unigrams and bigrams of words present in the essays . POS n-grams One up to four grams of POS tags present in the essays ; tagging is done using default NLTK tagger ( TARGETCIT . Character n-grams One up to six grams of characters in each essay . Spelling errors All words that are not found in the dictionary of Peter Norvig ’ s spelling corrector . 4.2.1 Term Frequency ( TF ) Term',\n",
       " 'd child node represents a dependency of the child on the parent , where the base distribution of the child node is its parent . This smooths each context ’ s distribution like the Bayesian n-gram model of TARGETCIT , which is a Bayesian version of interpolated Kneser-Ney smoothing ( ) . One ramification of this setup is that if a word occurs in a context u , the sharing makes it more likely in other context',\n",
       " 'ut sub-problems , and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems . CP has applications in tree and phrase based machine translation ( TARGETCIT ; ) , parsing ( ) , sentence alignment ( ) , and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definit',\n",
       " 'o a set of authors that heavily skews towards younger ages and minorities , with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets ( ; TARGETCIT . For example , we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriat',\n",
       " 'antly boost state-of-the-art parsing accuracy . Moreover , an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing ( TARGETCIT b ) . Notably , converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules . proach also outperforms the',\n",
       " 'one that learns from stereotypical attributes in addition to plot events . This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles ( ; TARGETCIT and unsupervised relation discovery ( ) . This character-centric perspective leads to two natural questions . First , can we learn what those standard personas are by how individual characters ( wh',\n",
       " 'contain information present in a constituency parse , such as the attachment order of object and subject to a verb . Unsupervised constituency parsing is also an active research area . Several studies ( TARGETCIT ; ) have considered the problem of inducing parses over raw lexical items rather than part-of-speech ( POS ) tags . Additional advances have come from more complex models , such as combining CCM a',\n",
       " 'g human judges on personality , and the second one to evaluate the performance of the system on Twitter data . In the first one , we compared the results of our system on a dataset , called Personage ( see TARGETCIT ) , annotated with personality ratings from human judges . Raters expressed their judgements on a scale from 1 ( low ) to 7 ( high ) for each of the Big Five personality traits on English sentences . In orde',\n",
       " 'ion , POS tagging , and parsing . Previous joint models mainly focus on word segmentation and POS tagging task , such as the virtual nodes method ( ) , cascaded linear model ( ) , perceptron ( TARGETCIT , sub-word based stacked learning ( ) , reranking ( ) . These joint models showed about 0.2 − 1 % F-score improvement over the pipeline method . Recently , joint tagging and dependency parsin',\n",
       " 'which do not occur in the parallel training data – this is not possible in a standard SMT setup . The idea of separating the translation into two steps to deal with complex morphology was introduced by TARGETCIT . applied this method to the language pair English-German with an additional special focus on word formation issues such as the splitting and merging of portmanteau prepositions and compounds',\n",
       " 'arsing accuracies on the retokenized treebanks . UAS is unlabeled attachment score , LAS is labeled attachment score , and POS is part-of-speech tagging accuracy . The treebank sources are ( 1 ) : + TARGETCIT + Petrov and McOTHERCIT , ( 2 ) : , ( 3 ) : , ( 4 ) : McOTHERCIT , ( 5 ) : Abeill´e et al . ( 2003 ) . from the forest-to-string system when they are better than the phrase-based results . We use * to de',\n",
       " 'l evaluation with a standardized relation-extraction benchmark : TAC-KBP 2010 . TAC-KBP asks for extractions of 46 relations on a given set of 100 entities . Interestingly , the Freebase held-out metric ( TARGETCIT ; ) turns out to be heavily biased toward distantly labeled data ( e.g. , increasing human feedback hurts precision ; see Section 4.6 ) . 4.2 Experimental Setup Our first group of experiments use t',\n",
       " '2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers ( ) . Many MDL algorithms , among them TARGETCIT , Daum´e III ( 2009 ) , and , all include some notion of learning domain-specific classifiers on the training data , and combining them in the best way possible . To be clear , we do not cl',\n",
       " 'ase the phrasal constraints are indirectly imposed by entity spans . We also differ in the implementation details , where in their case belief propagation is used in both training and Viterbi inference . TARGETCIT a ) presented a supervised learning method for performing joint parsing and word alignment using log-linear models over parse trees and an ITG model over alignment . The model demonstrates performance i',\n",
       " 'bscure references , noise , etc. ) . Meso-topics can be distinguished from the local topics because the participants often make polarized statements about them . We use the Stanford part-of-speech tagger ( TARGETCIT to automatically detect nouns and noun phrases in dialogue and select those with subsequent men42 tions as local topics using a fairly simple pronoun resolution method based primarily on presence of s',\n",
       " 'r operations are defined entry-wise . The unity element of the algebra is the identity matrix . 45 Computational Linguistics Volume 38 , Number 1 This means that our proposal is more general than that of TARGETCIT , who suggest using matrix multiplication as a framework for distributional semantic composition . The main differences in our proposal are as follows . • We allow dimensionality to be infinite , instead',\n",
       " 'dom Indexing ( RI ) . Thus we consider three different representations possible within Random Indexing ( ) . Finally , because compositional distributional semantics is an important research topic ( TARGETCIT ; Erk and Pad´o , 2008 ) , we sought to evaluate a principled composition strategy : structured vectorial semantics ( ) . The remainder of this paper proceeds as follows . Section 2 overviews our sim',\n",
       " '79.2 Adaptors grammar , colloc 78.4 75.7 77.1 Particle filter , unigram B¨orschinger and – – 77.1 Regularized compression + MDL , G1 ( b ) — 73.4 80.2 76.8 Bootstrap voting experts + MDL TARGETCIT 79.3 73.4 76.2 Nested Pitman-Yor process , bigram 74.8 76.7 75.7 Branching entropy + MDL 76.3 74.5 75.4 Particle filter , bigram B¨orschinger and – – 74.5 Hierarchical Dirichl',\n",
       " \"tioncrossing arcs ) are being initialized to uniform multinomials . HL·DBM D45 HL·DBM D45 split C ( 18 ) # 3 ( IFJ ) # 2 ( GT ) # 1 HL·DBM D45 CS ( 19 ) * C1 C2 HL•DBM Cl F D ' 1+PlitCl+1 l+1 1989 System DDA ( @ 10 ) ( TARGETCIT 53.1 ( 64.3 ) ( ) 53.3 ( 64.3 ) ( ) 53.3 ( 71.5 ) ( ) 55.7 ( 67.7 ) ( ) 57.0 ( 71.4 ) ( ) 58.4 ( 71.4 ) ( ) 59.1 ( 71.4 ) # 3 ( ) 61.2 ( 71.4 ) # 2 w/Full TrainingIFJ 62\",\n",
       " 'roduce high quality summaries of collections of related documents . Most previous work in extractive MDS has studied the problems of sentence selection ( e.g. , ( ) ) and sentence ordering ( e.g. , ( TARGETCIT ; ) ) separately , but we believe that a joint model is necessary to produce coherent summaries . The intuition is simple : if the sentences in a summary are first selected—without regard to coher',\n",
       " 'erate complementary translations . There is also similarity between our approach and minimum Bayes risk decoding ( ) , variational decoding ( ) , and other “ consensus ” decoding algorithms ( TARGETCIT . These all seek a single translation that is most similar on average to the model ’ s preferred translations . In this way , they try to capture the model ’ s range of beliefs in a single translation . We i',\n",
       " 'manually annotated corpus , such as Penn Discourse Treebank 2.0 ( PDTB ) ( ) , recent studies performed implicit discourse relation recognition on natural ( i.e. , genuine ) implicit discourse data ( TARGETCIT ( ) ( ) with the use of linguistically informed features and machine learning algorithms . ( ) conducted a study of the pattern-based approach presented by ( ) and showed th',\n",
       " 'ization ( ) , and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages ( TARGETCIT ; T¨ackstr¨om et al. , 2012 ) . In this paper , we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters . We propose',\n",
       " 'tinction for Eventuality is between Habituals and State , indicating that the distinction between those tags is still vague and not clearly defined . 4 Related Work In a series of TempEval evaluations ( TARGETCIT ; ) that are aimed at detecting time expressions , events and the relations among them , ( abstract ) tense determination is formulated as a task of determining the relation between an event and t',\n",
       " 'of dialogue states are associated with system actions and rewards . The goal of reinforcement learning is to learn optimal policies that maximize aggregate expected rewards , such as user satisfaction ( TARGETCIT . Learned policies that result from RL exploration do not , by design , necessarily reflect the patterns in the bootstrap dialogue corpus . Additionally , to cover all possible state spaces , reinforcement',\n",
       " 'on of whether the triangle inequality holds for Usim ratings is also interesting for modeling reasons . Several recent approaches model word meaning in context through points in vector space ( ; TARGETCIT ; ; Thater , F¨urstenau , and Pinkal 2010 ; Washtell 2010 ; Van de ) . They work on the tacit assumption that similarity of word usages is metric—an assumption that we can directly test her',\n",
       " 'added to the training corpus on which a new system is trained . 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work ( TARGETCIT ; ) . Among these approaches are sentence-based , phrase-based and word-based output combination methods . In a similar approach , use a feature of the factored translation model framewor',\n",
       " 'teps are : 1 . Spelling errors 2 . Article errors 3 . Preposition errors 4 . Punctuation errors 5 . Noun number errors We use the following tools for syntactic processing : OpenNLP4 for POS tagging , YamCha ( TARGETCIT for constituent chunking , and the MALT parser ( ) for dependency parsing . For language modeling , we use RandLM ( ) . For spelling correction , we use GNU Aspell5 . Words that contain upper-',\n",
       " 'both source and target corpora . The BiLDA model we use is a natural extension of the standard LDA model and , along with the definition of per-topic word distributions , has been presented in ( ; TARGETCIT . BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution 0 . This variable is language-independent , because it is shared by each of the paired b',\n",
       " ', such as that of Galley et al . modelsis that they assign probabilities roughly ac ( 2006 ) , combined with an efficient pruning strategy cording to empirical frequencies for observed nlike cube pruning ( TARGETCIT , shold be able to grams , but fall back to distributions conditioned on integrte our model without much difficulty . smaller contexts for unobserved n-grams , as shown That said , for evaluation purpoes ,',\n",
       " 'upward on Arabic and Hungarian ( p < 0.005 ) , and was unchanged on Basque , Catalan and Turkish ( p > 0.4 ) . a single , left-to-right pass over the sentence . Non-directional Parsing The EasyFirst parser of TARGETCIT tackles similar forms of ambiguities by dropping the Shift action altogether , and processing the sentence in an easyto-hard bottom-up order instead of left-to-right , resulting in a greedy but non-dire',\n",
       " 'formly distributed in this range according to their raw dissimilarity scores . We experiment with 3 to 6 ranks ( the case where C = 2 reduces to the standard entity-based model ) . 5.1.2 Entity Extraction TARGETCIT ’ s best results were achieved by employing an automatic coreference resolution tool ( ) for extracting entities from a source document , and the permutations were generated only afterwards — ent',\n",
       " 'orization frames ( SCFs ) are vital for many NLP applications involving parsing and word sense disambiguation . In parsing , SCFs have been successfully used to improve the output of statistical parsers ( TARGETCIT , ) which is particularly significant in high-precision domain-independent parsing . In word sense disambiguation , SCFs have been identified as important features for verb sense disambiguation',\n",
       " 'Q ) + 742 • For each document d – Draw the region label y ( R ) d – Draw the time quartile label y ( dQ ) – For each word n , draw w ( d ) n — βyd 4.1 Parameter Estimation We follow the MAP estimation method that TARGETCIT a ) used to train all sparse latent variables η , and perform Bayesian inference on other latent variables . The estimation of all variance variables τ remains as plugging the compound distribution of No',\n",
       " 'entities , whereas singletons may be generic or temporal NPs which might be thought of as coreferent in a loose sense , but are not 6This method of analysis is similar to that undertaken in and TARGETCIT b ) , though we split our mentions along different axes , and can simply evaluate on accuracy because our decisions do not directly imply multiple links , as they do in binary classification-based systems',\n",
       " 'MTnews testing corpora respectively . D Para Vid Europ OnWn News Tr 0.4688 0.4175 0.5349 Te 0.4617 0.4489 0.4719 0.6353 0.4353 Table 3 . Run 2 Official Person-Correlation measure In the Task-6 results ( TARGETCIT , Run2 was ranked 72th out of 85 participants with 0.4169 Pearson-Correlation ALL competition rank . As anticipated , Run2 released fair results . Its performance is penalized or awarded proportionally t',\n",
       " 'ion 3 . Automatic work on bridging distinguishes between recognition ( ) and antecedent selection . Work on antecedent selection suffers from focusing on subproblems , e.g . only part-of bridging ( TARGETCIT ; ) or definite NP anaphora ( ) . Most relevant for us is who restrict anaphora to definite descriptions but have no other restrictions on relations or antecedent NPs ( in a Fren',\n",
       " 't what s/he has just said , or abandon what s/he just said and restart with revised content . This is a phenomenon called repair in speech literature . There is extensive literature on speech repairs [ 8 ] TARGETCIT [ 13 ] . Typically , a speech repair instance can be characterized as a template that consists of a reparandum and an alteration [ 13 ] . The reparandum is the speech sequence that is erroneous or inappropri',\n",
       " '‘ whisper — * talk ’ , ‘ win — * play ’ and ‘ buy — * own ’ . The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures ( ; TARGETCIT . Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods , assuming that verbs are semantically similar if they occur in similar contexts ( OTHER',\n",
       " 'irs of mentions , and makes training and tuning very slow . 13For the ACE05 and ACE05-ALL datasets , we revert to the ‘ AllPairs ’ ( AP ) setting of Reconcile because this gives us baselines competitive with TARGETCIT . Since we did not need to retune on these datasets , training and tuning speed were not a bottleneck . Moreover , the improvements from our Web features are similar even when tried over the SIG baseline',\n",
       " 'er outputs via MaltBlender ( ) . IGM : ALPAGE also uses MATE and MaltParser , once in a pipeline architecture and once in a joint model . The models are combined via a re-parsing strategy based on ( TARGETCIT . This system mainly focuses on MWEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle MWEs , which then serve as input for the two parsers . The IMS : SZEGED :',\n",
       " 'n approaches to summarization are based on topic models . These generative models represent documents as mixtures of latent topics , where a topic is a probability distribution over words . In TOPICSUM ( TARGETCIT , each word is generated by a single topic which can be a corpus-wide background distribution over common words , a distribution of document-specific words or a distribution of the core content of a gi',\n",
       " 'ues from a wide variety of perspectives from text-to-record alignment using structured classification ( ) , iterative retraining ( ) , and generative models of segmentation and alignment ( TARGETCIT to text-to-interaction mapping using reinforcement learning ( ) , graphical model semantics representation ( ) , and Combinatory Categorial Grammar ( ) . A number of systems have als',\n",
       " 'f relations ( ) . Early unsupervised approaches to the SRL problem include the work by , where the VerbNet verb lexicon was used to guide unsupervised learning , and a generative model of TARGETCIT which exploits linguistic priors on syntactic-semantic interface . More recently , the role induction problem has been studied in where it has been reformulated as a problem of detecting altera',\n",
       " 't is based on the Markov ’ s chain assumption . It assigns a higher probability to fluent/grammatical sentences . λφ , λLM and λd are used to give a different weight to each element . For more details see ( TARGETCIT . Three different SMT systems were used to translate the human annotated sentences : two existing online services such as Google Translate and Bing Translator5 and an instance of the open source phrase',\n",
       " 'pressions for each sentence . We therefore propose to apply a sentence pre-selection step before the compression . This kind of selection step has been used in previous ILP-based summarization systems ( TARGETCIT ; ) . In this work , we propose to use a simple supervised support vector regression ( SVR ) model ( ) to predict a salience score for each sentence and select the top ranked sentences for',\n",
       " 'c features based on intuitions ; contextual features remain the same for all problems . The classification framework is implemented in the GATE system , using the machine learning libraries it provides ( TARGETCIT . In particular , we have used the Support Vector Machines learning libraries ( ) which have given acceptable classification results in other NLP tasks . The framework allows us to run crossvalid',\n",
       " '( ) and ACE 2004 ( NIST , 2004 ) . 2 Related Work The idea of Latent Left-linking Model ( L3M ) is inspired by a popular inference approach to coreference which we call the Best-Left-Link approach ( TARGETCIT ; ) . In the best-left-link strategy , each mention i is connected to the best antecedent mention j with j < i ( i.e . a mention occurring to the left of i , assuming a leftto-right reading order ) ,',\n",
       " '— grammatical role — with topological field information , and showed that for German text , such a modification improves accuracy . For English text , two extensions have been proposed recently . TARGETCIT augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features , and their extension significantly outperformed the standard model on',\n",
       " 'thout positing disjoint senses . We now describe some alternatives to word sense inventory approaches to word meaning , most of which do not rely on disjoint senses . 2.3.1 Substitution-Based Approaches . TARGETCIT explore the use of synonym or near-synonym lexical substitutions to characterize the meaning of word occurrences . In contrast to dictionary senses , substitutes are not taken to partition a word ’ s mean',\n",
       " 'of simply aggregrating local decisions about pairs of mentions . Like these systems , our model adopts the entity-mention model ( Morton 909 Computational Linguistics Volume 39 , Number 4 2000 ; ; TARGETCIT 8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity . Previous systems do this by encoding constraints using rich probabilistic',\n",
       " 'to the same entity in reality ( ; ) . It is a core component in natural language processing and information extraction . Both rule-based approach ( ) and statistic-based approach ( ; TARGETCIT are proposed in coreference resolution study . Besides the frequently used syntactic and semantic features , the more linguistic features are exploited in recent works ( ) . CoNLL-2012 proposes a',\n",
       " 'nological MWUs , in order to recognize their nested structures , has been more recently recognized ( ) . While some work has been done in automatic processing of terminology for Slavic languages ( TARGETCIT ; ) , which are morphologically complex , relatively Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon ( CogALex-III ) , pages 195–214 , COLING 2012 , Mumbai , December 2012 . 195 few',\n",
       " 'omputational Linguistics 2 Related work While most SMT systems operate at the sentence level , there is increased interest in modeling document context and consistency in translation . In earlier work ( TARGETCIT , we investigate whether the “ one sense per discourse ” heuristic commonly used in word sense disambiguation ( ) can be useful in translation . We show that “ one translation per discourse ” largel',\n",
       " 'T NOUN P Figure 1 : A Greek sentence which is correctly parsed by a delexicalized English parser , provided that part-of-speech tags are available in both the source and target language . 2009 ; ; TARGETCIT . Annotations in multiple languages can be combined in delexicalized transfer as well , as long as the parser features are available across the involved languages . This idea was explored by McOTHERCIT ,',\n",
       " 'or automatic clustering . gives an overview of these approaches to Interactive Question Answering . combine a manually built tree for main topics with an n-gram model for topic shifts . TARGETCIT use machine learning to explore follow-up questions . Instead of attempting a deep processing approach involving information extraction , question answering or summarization techniques , we prefer a shal',\n",
       " 'is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality ( ; Giménez and Màrquez , 2007 ; ; TARGETCIT but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time . did tune on QUEEN , a simplified version of ULC that discards the semanti',\n",
       " 'g to bad neighbors . 3.2 Building of the initial thesaurus Before introducing our method for improving distributional thesauri , we first present the way we build such a thesaurus . As in ( ) or ( TARGETCIT a ) , this building is based on the definition of a semantic similarity measure from a corpus . The corpus used for defining this measure was the AQUAINT-2 corpus , a middlesize corpus made of around 380',\n",
       " 'ar to the one between f0 and f , norms . 2http : //cvxr.com/tfocs/ The CE Corpus The first corpus of human judgments we have considered has been collected for the WMT12 shared task on quality estimation ( TARGETCIT .3 The data set is made of 2,254 English sentences and their automatic translations in Spanish predicted by a standard Moses system . Each sentence pair is accompanied by three estimates in the range 1',\n",
       " 'ther by assuming an unrealistic degree of phonetic regularity for word segmentation ( ) or assuming pre-segmented input for phonetic and lexical acquisition ( ; Feldman et al. , in press ; TARGETCIT . This paper presents , to our knowledge , the first broadcoverage model that learns to segment phonetically variable input into words , while simultaneously learning an explicit model of phonetic variat',\n",
       " 'ary sentences from reviews . An aspect is a component or attribute of a product such as “ battery ” , “ lens cap ” , “ battery life ” , and “ picture quality ” for cameras . Aspectoriented summarization ( ; TARGETCIT collects sentiment assessments for a given set of aspects and returns a list of pros and cons about every aspect for a review or , in some cases , on a per-sentence basis . Aspect-oriented summarization',\n",
       " 'in this article is different . We assume that we have a fixed grammar , and our goal is to estimate its parameters . This approach has shown great empirical success , both in the supervised ( Collins 2003 ; TARGETCIT and the unsupervised ( ) settings . There has also been some discussion of sample complexity bounds for statistical parsing models , in a distribution-free setting ( Collins 2004 ) . The distributio',\n",
       " '. We compared regularised logistic regression against two baselines : naive Bayes and support vector machines ( SVMs ) , which have been used for ( dis ) agreement classification in previous works ( ; TARGETCIT . For SVMs , we used the toolbox LIBSVM in to implement the classification and probability estimation . We tuned the parameter C in regularised logistic regression and SVM , using cross-validati',\n",
       " 'wever , experiments on data with variation have called these conclusions into question . In particular , GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier ( TARGETCIT , and our results replicate this finding ( oversegmentation for the “ segment only ” model ) . In addition , the GGJ bigram model , which achieves much higher segmentation accuracy than the unigram model on',\n",
       " 'earch problems . In light of our emphasis on Wikipedia , a connection can be drawn between identifying prerequisites and measuring the semantic relatedness of concepts using Wikipedia ’ s link structure ( TARGETCIT . We consider here a related but narrower question , namely whether an inter-page link will improve comprehension for a specific reader . In the area of intelligent tutoring and educational data mining ,',\n",
       " 'guage-model n-gram statistics on a word lattice of candidate realisations to guide a ranker . Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models ( TARGETCIT ; ) . Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they < empty > victim the victim man a young he < em',\n",
       " 'they do not indicate sentiment polarities . 3http : //books.google.com/ngrams . ct+1 p 1767 GN corpus was created . Another Chinese GN corpus with 3493 words was generated in the similar way from HowNet ( TARGETCIT . Generation of Labeled Examples . Let T = { Y+1 , Y−1 } denotes the initial labeled set , where N most highly confident target candidates but not in our GN corpora are regarded as the positive example set',\n",
       " 'a bilingual language model which incorporates words in the source and target languages to predict the next unit , which they use as a feature in a translation system . This line of work was extended by TARGETCIT who develop a novel estimation algorithm based around discriminative projection into continuous spaces . Also relevant is , who present a sequence model of translation including reordering . Our',\n",
       " 'curacy ( the Brown model gets .68 many-to-one accuracy with a 1M word corpus ) . Given that close to 95 % of the word occurrences in human labeled data are tagged with their most frequent part of speech ( TARGETCIT , this is probably not surprising ; one-tag-per-word is a fairly good first approximation for induction . 2.2 Word-feature models One problem with the algorithms in the previous section is the poverty o',\n",
       " 'n found to play an significant role in many applications ( ) ( ) ( ) complementing evidence from Twitter messages and network structure . In recent work on stock market prediction , ( TARGETCIT used Twitter messages ( Tweets ) from StockTwits to identify expert investors for predicting stock price rises . They used a support vector machine ( SVM ) to classify each stock related message to two pol',\n",
       " 'ted Syntax attachment , pps , np , fragments , pp , noun , vp , nos , pattern 2.5 Trends within the subdisciplines in Biomedical NLP Literature Our analysis of temporal trends builds on the idea proposed by ( TARGETCIT in their analysis of the changing trends in the field of computational linguistics over time . In their approach , they attempted , among other things , to analyze which topics were up and coming in the f',\n",
       " 'e same evaluation procedure is applied to output from two previous extraction methods . The first method starts by internally generating a small set of seed instances for a class label given as input ( TARGETCIT . A set expansion module then expands the seed set into a longer , ranked list of instances . The instances are extracted from unstructured and semi-structured text within Web documents . The documents a',\n",
       " 's that are based on the available knowledge resources ( e.g. , WordNet , or available taxonomies ) ( ; Hughes and Ramage , ) , and methods that use contextual/property distribution around the words ( TARGETCIT ; ) . ( ) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the pri',\n",
       " 'one while Le Zhao was at Carnegie Mellon University . learning models . Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it ( ; TARGETCIT . However , their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base ,',\n",
       " 'translation probability would be overestimated . We used Good-Turing technique described in ( ) to decrease it to some more realistic value . 3.5 Decoding Minimum Bayes-Risk ( MBR ) MBR decoding ( TARGETCIT aims to minimize the expected loss of translation errors . As it is not possible to explore the space of all possible translations , we approximated it with the 1,000 most probable translations . A minus',\n",
       " 'rence rules , such as DIRT ( ) , are based on word-level similarity measures , almost all prior models addressing contextsensitive predicate inference rules are based on topic models ( except for ( TARGETCIT , which was outperformed by later models ) . We therefore focused on comparing the performance of our two-level scheme with state-of-the-art prior topic-level and word-level models of distributional sim',\n",
       " 'of H can be inferred from the meaning of T , as would typically be interpreted by people. ” Although this relation does not necessarily require the presence of corresponding predicates , previous work by TARGETCIT shows that word alignments can serve as a good indicator of entailment . 172 As a matter of fact , the same holds true for the task of detecting paraphrases . In contrast to RTE , this latter task require',\n",
       " 'our Parsing Models Table 1 summarizes the features used in our parsing models , which are extracted from two adjacent units Ut_1 and Ut . Since most of these features are adopted from previous studies ( TARGETCIT ; ) , we briefly describe them . Organizational features include the length of the units as the number of EDUs and tokens . It also includes the distances of the units from the beginning and end',\n",
       " 'n be compiled with the help of web resources , such as Wikipedia , or study guides , such as CliffsNotesTM . This preprocessing step could also be performed automatically using a canonicalization method ( TARGETCIT ; however , since our focus is on speaker identification , we decided to avoid introducing annotation errors at this stage . Other preprocessing steps that are required for processing a new novel include',\n",
       " 'gle communication channel . The maximum entropy classifier achieved the best overall performance , reaching accuracy of .789 . This is an encouraging result compared to previous work in a similar domain . TARGETCIT a ) reported an accuracy of .628 for dialogue act classification in a similar domain . However , a direct comparison is not applicable since different data were used in their work . 7 Conclusions and Futu',\n",
       " 'tman-Yor Process ( PYP ) ( ) , namely a sort of nonparametric Bayesian model . The PYP produces power-law distributions , which have been shown to be well-suited for such uses as language modeling ( TARGETCIT b ) , and TSG induction ( ) . One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and',\n",
       " 'statistical models can learn structures of words as well as phrases from the augmented treebank ( Section 4 ) . Although previous authors have noticed the importance of word-structure parsing ( ; TARGETCIT , no detailed description about annotation of word structures has been provided in the literature . Secondly , we designed a unified dependency parser whose input is unsegmented sentences and its output',\n",
       " 'the optimized parameter updates discussed in Section 4.1 , and the “ -L ” suffix 3The other major difference is the use of a simpler learning rate , ak , which was very slow in our preliminary studies . 4 TARGETCIT minimized logistic loss sampled from the merged n-bests , and sentence-BLEU was used for determining ranks . 5We used liblinear ( ) at http : //www . csie.ntu.edu.tw/˜cjlin/liblinear with the solver',\n",
       " 'ults are reported in Table 5 . We could not report on Meteor as Meteor does not explicitly output alignment scores . We did not expect the aligners to beat any of the system A % P % R % 75.6 77 90 TARGETCIT 73.9 74.9 91.3 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 ( b ) Paraphrase Identification system MAP MRR 0.4271 0.5259 0.6029 0.6852 OT',\n",
       " 'tions in Twitter . ( ) described an approach to segmentation and classification of a wider range of names in tweets based on CRFs ( using POS and shallow parsing features ) and Labeled LDA resp . ( TARGETCIT proposed NER ( segmentation and classification ) approach for tweets , which combines KNN and CRFs paradigms . The reported precision/recall figures are significantly lower than the state-of-the-art resul',\n",
       " 'use a handful of features or use small training sets of a few thousand sentences ( e.g. , Och 2003 ; ) . Although there is growing interest in large-scale discriminative training ( e.g. , ; TARGETCIT ; ) , only recently does some improvement start to be observed ( e.g. , ) . It still remains uncertain if the improvement is attributed to new features , new training algorithms , objective',\n",
       " 'ut ignored the important role of the context . Besides , there exists some research focusing on word sense subjectivity disambiguation , which aims to classify a word sense into subjective or objective ( TARGETCIT ; ) . Obviously , this task is different from ours . 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of web sources to collect more useful pseudo contexts',\n",
       " 'ng . Unfortunately , this results in fewer alignments , and improvements are only shown on mentions that are easier to align and corefer ( such as the non-transcript documents in ) . Alternatively , TARGETCIT link each mention to multiple entities in the knowledge base , improving recall at the cost of lower precision ; the attributes of all the linked entities are aggregated as features . Although this appro',\n",
       " 'different contexts but not generally interchangeable , are mixed together in the same distribution . In SMT the target Language Model ( LM ) helps selecting the adequate translation candidate in context . TARGETCIT extends ( ) by adding syntactic constraints to the model . Paraphrase extraction is done by pivoting using word-alignment information , as before , but sentences are syntactically annotated and pa',\n",
       " 'PDTB , relation types are organized hierarchically : there are 4 classes , which can be further divided into 16 types and 23 subtypes . 3 Related work Discourse parsing was first brought to prominence by TARGETCIT . Since then , many different algorithms and systems ( ; LeOTHERCIT ) have been proposed , which extracted different textual information and adopted various approaches for discourse tree building .',\n",
       " 't. 2.2 Features Several works have studied the problem of confidence estimation ( ) or related problems such as predicting readability ( ) or developing automated essay scoring systems ( TARGETCIT y all use the same basic features : IBM 1 score measures the quality of the “ association ” of the source and the target sentence using bag-of-word translation models ; Language model score accounts for t',\n",
       " 'by decoding the label annotations ) . 5 Experiments In this section , we evaluate the performance of the undirected planar , 2-planar and Covington parsers on eight datasets from the CoNLL-X shared task ( TARGETCIT . Tables 1 , 2 and 3 compare the accuracy of the undirected versions with naive and label-based reconstruction to that of the directed versions of the planar , 2-planar and Covington parsers , respective',\n",
       " \"ERCIT ) . Finally , rules are assessed using some similarity measure between corresponding argument vectors . The state-of-the-art DIRT algorithm ( ) uses the highly cited Lin similarity measures ( TARGETCIT to score rules between binary templates as follows : Lin ( v , v ' ) = EwEvnvl [ v ( w ) + v ' ( w ) ] ( 1 ) EwEvUvl [ v ( w ) + v ' ( w ) ] DIRT ( l → r ) �= Lin ( vl : x , vr : x ) · Lin ( vl : y , vr : y ) where v and v ' are two argument vec\",\n",
       " 'ments in each pair , rather than the segments themselves . Several phrase-level generative models have been proposed , almost all relying on multinomial distributions for the phrase alignments ( ; TARGETCIT ; ) . This is a consequence of treating alignments as functions rather than partitions . Word alignment and phrase extraction via Inversion Transduction Grammars ( ) , is a linguistically',\n",
       " 'formance of word pair features when used with no additional features . Results are shown in Table 1 . Our word pair features outperform the previous formulation ( represented by the results reported by ( TARGETCIT , but used by virtually all previous work on this task ) . For most relation classes , tf is significantly better than pmi . 3 3Significance was verified for our own results in all experiments shown in th',\n",
       " 'dgements on five other system outputs to compute an overall ranking . The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community ( ; TARGETCIT . There has been some effort to overcome these problems . HTER ( ) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same mean',\n",
       " 'with modified Kneser-Ney smoothing ( ) . The minimum count cut-off for unigrams , bigrams , and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3 . Language model inference used KenLM ( TARGETCIT . Uncased IBM BLEU was used for evaluation ( ) . MERT was used to train the feature weights for the baseline systems on TUNE1 . We used the learned parameters to generate M-best and diverse lists',\n",
       " 'experiments , we use two baseline systems : our in-house tree-to-string decoder implemented according to ( denoted as traditional ) and the Earley-style top-down decoder implemented according to TARGETCIT ( denoted as topdown ) , respectively . We compare our bottom-up left-to-right decoder ( denoted as bottom-up ) with the baseline in terms of performance , translation quality and decoding speed with differe',\n",
       " 'tely 200 readability formulas have been reported in the literature ( DuOTHERCIT ) ; statistical language processing techniques have recently entered into the fray for readability assessment . and TARGETCIT have demonstrated the use of language models is more robust for web documents and passages . studied the impact of grammar-based features combined with language modeling approach for readabili',\n",
       " 'rk intersects with two strands of work . The first involves learning models of semantics guided by denotations or interactions with the world . Besides semantic parsing for querying databases ( ; TARGETCIT , previous work has looked at interpreting natural language for performing programming tasks ( ) , playing computer games ( ) , following navigational instructions ( ) , and interact',\n",
       " 'st of WordNet opposites , we chose one word randomly to be the target word , and the other as one of its candidate options . Four other candidate options were chosen from Lin ’ s distributional thesaurus ( TARGETCIT .11 An entry in the distributional thesaurus has a focus word and a number of other words that are distributionally similar to the focus word . The words are listed in decreasing order of similarity . N',\n",
       " 'errors – article , noun , and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena . The HOO-2011 shared task collection ( TARGETCIT contains a very small number of noun and agreement errors ( 41 and 11 in test , respectively ) , while the HOO-2012 competition ( ) only addresses article and preposition mistakes . Indeed , in paral',\n",
       " 'across different languages . Conducting a complete and faithful evaluation across languages would require a harmonized universal annotation scheme ( possibly along the lines of ( de ; McOTHERCIT ; TARGETCIT ) or task based evaluation . As an approximation we use unlabeled TedEval . Since it is unlabeled , it is not sensitive to label set size . Since it internally uses function-trees , it is less sensitive to',\n",
       " 'of the sequence models is to reconstruct the discourse graph of a document where the structure is unknown . In the conversation domain , this corresponds to the task of thread reconstruction ( ; TARGETCIT c ) . Given only a flat structure , can we recover the reply structure of messages in the conversation ? Previous work with BHMM found the optimal structure by computing the likelihood of all permutations',\n",
       " 'lysis are presented in section 3 . The last section is the conclusion and further work . 2 Parsing Model 2.1 Stanford Factored Model The Stanford parser , precisely , the highly optimized factored model ( TARGETCIT has been employed to perform our experiment . The factored model is the combination of unlexicalized PCFG model and dependency model . To our knowledge , the unlexicalized model did not encode word infor',\n",
       " 'so we can investigate whether sensetagging enables the learner to make better use of a limited quantity of training data . 3.2 Evaluation Our primary evaluation metric is Elementary Dependency Match ( TARGETCIT . This converts the semantic output of the ERG into a set of dependency-like triples , and scores these triples using precision , recall and F-score as is conventional for other dependency evaluation . F',\n",
       " 'in additional relevant information . Two main approaches have been used to address textual inference ( for either ranking or classification ) . One is based on transformations over syntactic parse trees ( TARGETCIT ; ) . Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage ( ) . In the second approach , lexical model',\n",
       " 'mitted Phenomena The representation described is a simplification of the complexities of time . Notably , a body of work has focused on reasoning about events or states relative to temporal expressions . TARGETCIT describes temporal expressions relating to changes of state ; explores NPI licensing in temporal expressions . Broader context is also not -2 -1 -0.3 1 2 11/13 11/20 t 12/4 12/11 Δ , � � 0 11/27',\n",
       " 'n-English phonemes ( e.g. , avant ) , or silent consonants ( e.g. , limn ) . The alignment is restricted to matching each letter symbol to at most one phoneme , and is derived with the ALINE phonetic aligner ( TARGETCIT , which has been shown to outperform other 1-1 alignment methods ( ) . 6.2 Vowel Counter Syllables that contain multiple vowel groups may be confusing to readers even if they correctly represent',\n",
       " 'tance , compared distributional measures and reported Precision @ 1 of 76 % for the best one . For improving the performance , some attempts were made to combine single measures , such as ( ; TARGETCIT . However , most studies are still not taking into account the whole range of existing measures , combining mostly sporadically different methods . The main contribution of the paper is a systematic anal',\n",
       " 'ining/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers ( policies ) . The PTB test data is tagged by a Stanford part-of-speech ( POS ) tagger ( TARGETCIT trained on sections 02–21 . We use the provided gold POS tags for the CoNLL test data . All results are evaluated by the unlabeled attachment score ( UAS ) . For fair comparison with previous work , punctua',\n",
       " 'negation expressions ( i.e . cues ) in free text has been a subject of research interest for quite some time ( e.g . etc ) , automatic detection of full scope of negation is a relatively new topic ( TARGETCIT ; ) . Detection of negation cues , their scope and corresponding negated events in free text could improve accuracy in other natural language processing ( NLP ) tasks such as extraction of factual',\n",
       " 'wDiff ( WD ) , solves these problems by counting an error whenever there is a difference between the number of segments in the prediction as compared to the reference . Recent work in topic segmentation ( TARGETCIT continues to use both metrics , so we also present both here . During initial testing , we noted a fairly serious shortcoming with both these metrics : all else being equal , they will usually prefer a sys',\n",
       " 'also have a higher likelihood . This is an important background for parametric models such as ( 2 ) where the M-step can not be solved exactly . For IBM-3/4/5 computing exact expectations is intractable ( TARGETCIT and approximations have to be used ( in fact , even computing the likelihood for a given θ is intractable ) . We 5See e.g . the author ’ s course notes ( in German ) , currently http : //user.phil-fak.uni-duessel',\n",
       " 'category . The crude relatedness model does not seem to help in our preliminary experimental study . Instead , we leverage the recently proposed polarity-inducing latent semantic analysis ( PILSA ) model ( TARGETCIT , which specifically estimates the degree of synonymy and antonymy . This method first forms a signed cooccurrence matrix using synonyms and antonyms in a thesaurus and then generalizes it using a low-',\n",
       " 'ide-stepped the issue by restructuring the source language before decoding to resemble the target language using syntactic rules , either automatically extracted ( Xia and McOTHERCIT ) , or hand-crafted ( TARGETCIT ; ) . The flexibility of CCG syntax is also gaining recognition as a useful tool for constraining statistical MT decoders . describes an incremental CCG parsing language model , although',\n",
       " 'GE . 5 ROUGE Evaluation More common than F-measure , ROUGE ( ) is often used to evaluate summarization . claimed to have demonstrated that ROUGE correlates well with human summaries , both TARGETCIT , and have cast doubt upon this . It is important to acknowledge , however , that ROUGE is actually a family of measures , distinguished not only by the manner in which overlap is measured ( 1-gra',\n",
       " 'ted rules , of the form a [ h1 ] b [ h2 ] c [ h3 ] for internal rules1 and a [ h1 ] w for lexical rules . Here a , b , c E N are non-terminals , w E T is a terminal and h1 , h2 , h3 E R are latent annotations . Following TARGETCIT we also define the set of skeletal rules R , in other words , rules without hidden states , of the form a — * b c or a — * w. • p : Rx — * R > 0 defines the probabilities associated with rules conditioned on',\n",
       " 'ational Linguistics the most frequent errors made by learners of English . Not surprisingly , much published research on grammatical error correction focuses on article and preposition errors ( ; TARGETCIT b ) , with relatively less work on correcting word choice errors ( ) . Article and preposition errors were also the only error types featured in the HOO 2012 shared task . Likewise , although all er',\n",
       " 'res that take the number of collocates as a core parameter . Entropy and other probabilistic measures have been used for MWE extraction since the earliest work . For example , the main idea in ( ; TARGETCIT , is that the MWE ’ s idiosyncrasy , ( ) , is reflected in the distributions of the collocates . introduced the Entropy of Permutation and Insertion : M EPI = − E p ( ngrama ) log [ p ( ngrama ) ] ( 1',\n",
       " 'given in Table 4 in Section 2.2 , and up to 92.8 % with the reduced tagset of 15 DepRels . All inter-annotator agreement figures oscillate around the 90 % threshold recommended in the OntoNotes project ( TARGETCIT . 4 Conclusions and future work In this paper , we report on the results of the annotation of a Spanish corpus , in which the different levels of annotation are clearly separated . We show that thanks to',\n",
       " 'n which it appears and the semantic similarity of two words is computed from the contexts they share . This perspective was first adopted by ( ) and ( ) and then , explored in details in ( TARGETCIT b ) , ( ) or ( ) . The problem of improving the results of the “ classical ” implementation of the distributional approach as it can be found in ( ) for instance was already tackled by',\n",
       " 'swering ( ) , semantic similarity computation ( ; McOTHERCIT ) , automated dictionary building ( ) , automated essay grading ( ) , word-sense discrimination and disambiguation ( TARGETCIT ; * Equally contributing authors Schütze , 1998 ) , selectional preference modeling ( ) and identification of translation equivalents ( ) . Systems that use DSMs implicitly make a bag of word',\n",
       " 'due to the large feature space , this strategy did not work as well when switching the learning algorithm to SVMs . 3 Problem Definition & Task Description Following the setting of SemEval-2012 Task 2 ( TARGETCIT , the problem of measuring the degree of relational similarity is to rate word pairs by the degree to which they are prototypical members of a given relation class . For instance , comparing to the prot',\n",
       " 'ergence of the collected paraphrases . 2.3.2 Sub-Sentential Paraphrases Incorporating sub-sentential paraphrases in machine translation metrics also used for paraphrase detection has proven effective ( TARGETCIT . A large corpus consisting of more than 15 million sub-sentential paraphrases was assembled by using a pivot-based paraphrase acquisition method . acquired paraphrases of sentence pa',\n",
       " 'of the complex sentences . For example , in an experiment with the rules available in [ Siddharthan 2004 ] ’ s system on a random subset of sentences deemed complex in the Simple English Wikipedia corpus [ TARGETCIT ] , we found that only 30 % of these sentences are covered by the rules available . 116 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology , pages 116–125 , Fortaleza ,',\n",
       " 'mation in the test setting . The performance of the algorithms is measured using Pk and WindowDiff ( WD ) metrics ( ) . The C99 algorithm is initialized with a 11×11 ranking mask , as recommended in TARGETCIT . TT is configured according to with sequence length w=20 and block size k=6 . 5.1 Data Set For evaluation , we rely on the Choi data set ( ) , which has been used in several other text s',\n",
       " 'mation need to be treated differently . However , most of the systems developed for processing natural language data do not consider negations present in the sentences . Although various works ( ; TARGETCIT ; ) have dealt with the identification of negations and their scope in sentences , this is still a challenging task . The first task in * SEM 2012 Shared Task ( ) is concerned with finding',\n",
       " 'microblog messages to build an in-domain language model , or generating synthetic bilingual corpora from monolingual data . All experiments were conducted using the Moses machine translation system15 ( TARGETCIT with standard settings . Language models were built using the SRILM toolkit16 ( ) . For all experiments , we report lowercased BLEU4 scores ( ) as calculated by Moses ’ multi-bleu script . Fo',\n",
       " 'of-words models and considered to be context independent , despite of their state-of-the-art performance , such as TF-IDF ( ) , centroid similarity ( ) , and cross-lingual similarity ( CLS ) ( TARGETCIT a ) . They all perform at the word level , exact only ter512 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , page',\n",
       " '1.75 Our model 15.94 Table 5 : Comparison to previous works ( LexSub dataset ) . In the left column of Table 5 , we compare the performance of our system to representative Semeval 2007 participants , namely TARGETCIT and . In order to make a fair comparison , we report scores for the official test data of Semeval 2007 , using a 10-fold cross-validation scheme . developed their system based on WordNet',\n",
       " 'he v5.04 release . 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank ( ) , the Chinese Treebank ( ) and the Arabic Treebank ( TARGETCIT . There were two updates made to the parse trees as part of the OntoNotes project : i ) the introduction of NML phrases , in the English portion , to mark nominal sub-constituents of flat NPs that do not',\n",
       " 'y our annotation transformer . As comparison , we build a baseline system ( direct parsing ) using the Berkeley parser only trained on the CTB training data . In this experiment , the self-training method ( TARGETCIT a ; McOTHERCIT ) is also used to build another strong baseline system , which uses unlabelled TCT as additional data . Figure 2 shows that our approach outperforms the two strong baseline systems . It achi',\n",
       " 'architecture enables the integration of several systems . We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of ( ) or ( TARGETCIT which fusion n-best lists form different parsers , or use stacking methods where an additional parser is used as a guide for the main parser ( Nivre and McOTHERCIT ) . Finally it should be noted that this',\n",
       " '( ) . However , thus far the research community has focused on the problem of generating grammatical questions ( as in ) or generating effective distractors for multiple-choice questions ( TARGETCIT . While both of these research threads are of critical importance , there is another key issue that must be addressed – which questions should we be asking in the first place ? We have highlighted this',\n",
       " 'tactic parsers will also influence these numbers . The default values have been set based on experiments on the English-German language pair ( ) . It is worth noting that the German parse trees ( TARGETCIT tend to be broader and shallower than those for English . In Section 3 we present some experiments where we choose different settings of these parameters for the German-English language pair . We use th',\n",
       " 'Assuming access to a small amount ∗Performed while faculty at Johns Hopkins University of parallel text is realistic , especially considering the recent success of crowdsourcing translations ( ; TARGETCIT . We frame the shortcomings of SMT models trained on limited amounts of parallel text1 in terms of accuracy and coverage . In this context , coverage refers to the number of words and phrases that a mod',\n",
       " '16 0.2046 0.3082 0.1433 0.2947 Table 4 : Results of the submitted system for each type of error and results of additional experiments with the SMT-based system . The score is evaluated on the m2scorer ( TARGETCIT . ALL is the official result of formal run , and each of the others shows the result of the corresponding error type . Since our system did not distinguish SVA and Vform , we report the combined result f',\n",
       " 'extracted with 53 respect to individual participants . While individual behavior may index friendship state , we posit that patterns of interaction will be more effective . For example , prior research ( TARGETCIT suggests that the number and length of conversational turns ( ) , presence of mutual smiles and non-mutual smiles ( ) , mutual gaze and nonmutual gaze ( ) , as well as posture shifti',\n",
       " 've speakers of English : article , subject-verb agreement , noun number , and verb form . A significant proportion of research has focused on correcting mistakes in article and preposition usage ( ; TARGETCIT b ) . Several studies also consider verb-related and noun-related errors ( ) . The predictions made by individual models are then applied independently ( ) or pipelined ( ) . The stan',\n",
       " 'nd , this is not straightforward in the 3We believe that this might be the case only if we would introduce new information ( e.g . features ) for the system . presence of real valued features . For example , TARGETCIT introduce real-valued features for supporting German PP-attachment recognition – the mutual information of noun and preposition co-occurrence estimated from a huge unlabeled corpus – and this single f',\n",
       " '3 Entity Annotation In this section we describe our entity annotation system . Recently there has been lot of work addressing the problem of annotating text with links to Wikipedia entities ( ; TARGETCIT ; ) . We adopt a similar approach , wherein we first find the best disambiguation ( BESTDISAMBIGUATION ) for a given mention and then decide to prune it ( PRUNE ) , via the dummy mapping NA ( similar',\n",
       " 'ore the trees have an average size of only about 9 nodes . 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications , e.g . : parse reranking and adaptation ( ; TARGETCIT ; ) , chunking and dependency parsing ( ; Daum´e III and ) , named entity recognition ( ) , text categorization ( ) and relation extraction ( ) . To our knowled',\n",
       " 'ct of SRL errors , we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser ( ) , 1- best parse tree of Bikel parser ( ) and Stanford parser ( TARGETCIT . Therefore , at last , we can get 5 SRL result for each sentence . For the training set , we use these SRL results to do rule extraction respectively . We combine the obtained rules together to get a comb',\n",
       " 'correct preposition . This paper describes the University of Illinois system that participated in the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners ( TARGETCIT . Fourteen teams took part in the the competition . The scoring included three metrics : Detection , Recognition , and Correction , and our team scored first or second in each metric ( see for deta',\n",
       " 'bout one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation . 1 Introduction Most of the modern Statistical Machine Translation ( SMT ) systems , for example ( ; TARGETCIT ; ) , employ a large rule set that may contain tens of millions of translation rules or even more . In these systems , each translation rule has about 20 dense features , which represent key stati',\n",
       " 'rer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed . For statistical significance testing , we use sign-test with bootstrap re-sampling ( TARGETCIT with 1,000 samples . 4.3 SMT Baseline We build a baseline error correction system , using the MOSES SMT system ( ) . Word alignments are created automatically on “ good-bad ” parallel text from HOO-',\n",
       " 'SIFT features , we compare it against the SIFTonly version of our model , with each system using the same computed image and text features . We re-implement the MixLDA system mostly as it is described in TARGETCIT b ) , with a few changes to make it more comparable to our model : Obviously in our version of MixLDA the test instances are only the unseen image as there is no other surrounding text . The number of top',\n",
       " 'uring the generation of a more concise and fluent text ( Shaw 1998 ; Dalianis 1999 ) . Aggregation ( typically syntactic aggregation [ ] ) has received considerable attention from the NLG community ( TARGETCIT ; O ’ ) , and has been applied in various existing generation systems such as the intelligent tutoring application developed by . Our aggregation mechanism works to combine propositions i',\n",
       " 'monly appended after the last three characters . For example , words ending with ung most commonly have an s appended when they are used in compounds . 4.7 Recasing Phrasal includes an LM-based recaser ( TARGETCIT , which we trained on the target side of the bitext for each language . On the newstest2012 development data , the German recaser was 96.8 % accurate and the French recaser was 97.9 % accurate . 5 Translat',\n",
       " 'phrases . ( ) model sentences in a vectorial representation and propose an approach based on semi-supervised recursive autoencoders in order to predict sentence-level sentiment distributions . ( TARGETCIT propose a graph-based method for computing a sentence-level sentiment representation . The vertices of the graph are the opinion targets , opinion expressions and modifiers of opinion and the edges repr',\n",
       " 'rison , our model is an unsupervised method . The same authors proposed an unsupervised method which relies on associations of aspects with topics indicative of stances mined from the Web for the task ( TARGETCIT . In contrast , our model is also an unsupervised one but we do not rely on any external knowledge . Part of our work is related to detecting agreement/disagreement from text . For this task , nor1032 mal',\n",
       " 'target word . 4 Syntactic Simplification We are developing a text simplification system which will integrate different simplification modules , such as syntactic simplification , lexical simplification ( TARGETCIT and content reduction . At the moment the most advanced module of this system is the one for syntactic simplification . In ( ) we describe the functioning of the simplification grammar in more de',\n",
       " 'E plain sentences . We performed sentence splitting using the implementation of in NLTK 2.0.1rc2 . We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in ( TARGETCIT as shown in Table 1 with Maximum Entropy ( ME ) modeling ( ) as a multi-class classifier . We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters . For web n-gr',\n",
       " 's on selecting in-domain data from a general domain corpus . In particular , perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model . and TARGETCIT apply this method to language modeling , and and apply this method to translation modeling . suggest a slightly different approach , using crossentropy difference as a ranking',\n",
       " '07 n/a 303 1.32 % 100 % ( ) Twitter Nov 2009 – Feb 2010 6150 3802 3.87 % 99.34 % ( ) SMS/Twitter Aug 2009 4660 2040 2.41 % 96.84 % ( ) Twitter Aug 2010 – Oct 2010 549 558 2.87 % 99.10 % ( TARGETCIT Table 3 : Statistics of different SMS and Twitter data sets . The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words . For each nonstandard token ,',\n",
       " 'in detection and recognition , and second in correction . Table 11 shows our performance after the revisions were applied . 10 Discussion The HOO 2012 shared task follows the HOO 2011 pilot shared task ( TARGETCIT , where the data was fully corrected and error-tagged and the participants could address any types of mistakes . The current task allows for comparison of individual systems for each error type conside',\n",
       " 'in quotation marks . 4 Multi-Task Learning We next turn to the problem of learning the model from training data . Prior work in compressive summarization has followed one of two strategies : and TARGETCIT learn the extraction and compression models separately , and then post-combine them , circumventing the lack of fully annotated data . gathered a small dataset of manually compressed summaries ,',\n",
       " 'and a good number of such systems have demonstrated the feasibility and practicality in automatic acquisition of parallel corpora from bilingual and/or multilingual web sites , e.g. , STRAND ( ; TARGETCIT ; ) , BITS ( ) , PTMiner ( ) , PTI ( ) , WPDE ( ) , the DOM tree alignment model ( ) , PagePairGetter ( YE et al. , 2008 ) and Bitextor ( Espl ` a-OTHERCIT ) . Most of the',\n",
       " 'iterature in natural language processing ( , Marcu 2000 , , inter alia ) as does automatic coreference resolution , which has significantly increased in accuracy in recent years ( , TARGETCIT , ) . We formulate and test two hypotheses in this position paper : First , we anticipate that given stylistic considerations and their fundamental narrative function , prose literary texts are in',\n",
       " ') the unsupervised relation-based approach of , ( 3 ) two supervised methods , and ( 4 ) an upperbound derived from the gold standard decision abstracts . The LONGEST DA Baseline . As in and TARGETCIT , this baseline simply selects the longest DRDA in each cluster as the summary . Thus , this baseline performs utterance-level decision summarization . Although it ’ s possible that decision content is spr',\n",
       " 'described in and to induce synchronous grammar rules , a process which requires phrase alignments and syntactic parse trees . use generic non-terminal category symbols , as in TARGETCIT , as well as grammatical categories from the Stanford parser ( ) . Their method for rule induction generalizes to any set of non-terminals . We further refine this process by adding semantic nota',\n",
       " 'ingle predicate with two predicates , as in “ grow ” and “ increase in size ” . Handling both cases may require maintaining both composite and decomposed forms of a relation . 4 Some systems , such as WEBRE ( TARGETCIT , do use additional semantic resources and are able to achieve better recall . ing “ take a walk ” to “ walk ” ) , and handling of transparent nouns ( reducing “ I ate a pound of chocolate ” to “ I ate chocolate',\n",
       " 'EDITED tag “ is used to show the repetition and restarting of constituents that are repaired by subsequent speech ” ( ) . the same dependency structure headed by the INNA word , similar to CATiB ( TARGETCIT . We treat the focus particle AmmA like a preposition in our dependency structure , following CATiB . 4.5.3 Dependency Label Scheme Our dependency scheme consists of a total of 35 labels . Many of these',\n",
       " 'IC model is a mention-ranking approach resembling models used by and , though it is trained using a novel parameterized loss function . It is also similar to the MLN-JOINT ( BF ) model of TARGETCIT , but we enforce the singleparent constraint at a deeper structural level , allowing us to treat non-anaphoricity symmetrically with coreference as in and . The model of also',\n",
       " 'en query . However , in their research which warps around the OpenEphyra question answering system , shows that passage ranking is not the most important task in question answering . Ramakrishnan et al . ( TARGETCIT also support this concept showing that high quality answer can only be extracted through the proper understanding of the target required by the end user . But Whittaker et al . ( ) bring out that',\n",
       " 'corpus described in Table 3 below . # Sentences # Words Training 94926 1235163 Tuning 1446 23600 Test 500 9792 Table 3 : Corpus distribution The baseline system was setup by using the phrase-based model ( TARGETCIT ; ) . For the language model , we carried out experiments and found on comparison that 5-gram model with modified Kneser-Ney smoothing ( ) to be the best performing . Target Hindi corpus f',\n",
       " 'CIT ) , as well as text summarization ( ) . In Text Coherence Detection ( ) , sentences are linked together by similar or related words . For Word Sense Disambiguation , researchers ( ; TARGETCIT a ) introduced a sense similarity measure using the sentence similarity of the sense definitions . In this paper we illustrate the different effect of four feature types including direct lexical matchin',\n",
       " 'nough closed set , such as for PoS tags , but this is infeasible when the set is extended to PoS plus function words . The use of adaptor grammars here can be viewed as a form of feature selection , as in TARGETCIT . Baseline models To serve as a baseline , we take the commonly used PoS bigrams as per the previous work of NLI ( ) . A set of 200 PoS bigrams is selected in two ways : the 200 most frequent in t',\n",
       " 'valuating algorithms against humanproduced data in visual domains , aiming to mimic human references to objects . This interest has manifested most prominently in the 2007-2009 REG Challenges ( ; TARGETCIT based on the TUNA Corpus ( van ) . The GBA performed among the best algorithms in all three of these challenges . However , in particular its ability to analyze relational information could not be',\n",
       " 'nging even for humans , many SCFs have no reliable cues in data , and some SCFs ( e.g . those involving control such as type raising ) rely on semantic distinctions . As SCFs follow a Zipfian distribution ( TARGETCIT , many genuine frames are also low in frequency . State-of-theart methods for building data-driven SCF lexicons typically rely on parsed input ( see section 2 ) . However , the treebanks necessary for trai',\n",
       " 'le 1 shows a case where unknown words were learned through translation model adaptation . Note that even the Google translator did not recognize the word GN_u. , ,o which was transliterated as “ Msellat ” . TARGETCIT a ) point out that dialectal variants are often transliterated by Google . Note also , that the unadapted translation erroneously translated the place name “ sitra ” as “ jacket ” , a mistake which was also m',\n",
       " 'se of NLP for Building Educational Applications , pages 300–305 , Atlanta , Georgia , June 13 2013. c�2013 Association for Computational Linguistics far as we are aware , the HOO 2011 system description of TARGETCIT is the only work to specifically reference hyphen errors . They use rules derived from frequencies in the training corpus to determine whether a hyphen was required between two words separated by white',\n",
       " 'n that is responsible for a shift in lexical complexity : ( 4 ) a. lie < say falsely < say untruthfully b. sample < typical sample < representative sample 4 Results The Task 1 overall result can be found in ( TARGETCIT . The mmSystem achieved an average ranking ( score=0.289 ) if compared with the other participating systems and the baselines that corresponds to an absolute inter-annotator agreement between system out',\n",
       " 'tunately , the performance were not much better than a random baseline , which indicates the difficulty of this task . In comparison , a supervised learning approach seems more promising . The UTD system ( TARGETCIT , which mined lexical patterns between co-occurring words in the corpus and then used them as features to train a Naive Bayes classifier , achieved the best results . However , potentially due to the lar',\n",
       " 'omplexity of the loss and the difficulty of the search lead to instabilities during learning . Remedies have been suggested , typically involving additional search directions and experiment replicates ( TARGETCIT ; ) . But despite these improvements , MERT is ineffectual for training weights for large numbers of features ; in addition to anecdotal evidence from the MT community , illustrated with',\n",
       " 'ing guided learning , a framework for bidirectional sequence classification , which integrates token classification and inference order selection into a single learning task and uses a perceptron-like ( TARGETCIT passive-aggressive classifier to make the easiest decisions first . Recently , , proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism',\n",
       " 'obable and inducible parts/roles and argues that only in the first and maybe the second case the antecedent triggers the 3See also the high results for our specific category for comparative anaphora ( TARGETCIT . 4We thank an anonymous reviewer for pointing this out . bridging anaphor in the sense that we already spontaneously think of the anaphor when we read the antecedent . Also , bridging recognition on its',\n",
       " 'to “ GPU-ify ” other machine translation models and other components in the machine translation pipeline . An obvious next step is to extend our work to the hierarchical phrase-based translation model ( TARGETCIT , which would involve extracting “ gappy ” phrases . has tackled this problem on the CPU , but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the',\n",
       " 'IT ) . Additionally , Vuli´c et al . ( 2011 ) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model ( ; Jagarlamudi and Daum´e III , 2010 ; TARGETCIT to identify potential translation candidates in the cross-lingual settings without any background knowledge . In this paper , we show that a transition from their semantic space spanned by cross-lingual',\n",
       " 'onstruct the strings w. However , this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens , as in transformation models ( TARGETCIT ; ) . 7 Experimental Evaluation 7.1 Data preparation Scraping Wikipedia . Wikipedia documents many variant names for entities . As a result , it has frequently been used as a source for mining nam',\n",
       " 'rus construction in MRL , and demonstrate the empirical benefit of our methodology for a Hebrew thesaurus . 1 Introduction Corpus-based thesaurus construction has been an active research area ( ; TARGETCIT ; ) . Typically , two statistical approaches for identifying semantic relationships between words were investigated : first-order , cooccurrence-based methods which assume that words that occur fr',\n",
       " '7,610 69,453,695 dev 1,894 48,384 53,584 mt06 1,664 39,694 47,143 mt08 1,357 33,701 40,893 Table 4 : Chinese-English Corpus . For English dev and test sets , word counts are averaged across 4 references . TARGETCIT , we cluster at two granularities ( 20 clusters and 50 clusters ) , and allow the discriminative tuner to determine how to best employ the various representations . We add the sparse features in Table 2 t',\n",
       " 'and simplification ( ) . Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora . Several approaches rely on bilingual parallel data ( ; TARGETCIT ; ) , while others leverage distributional methods on monolingual text corpora ( ) . So far , however , only preliminary studies have been undertaken to combine the information from these t',\n",
       " 'he present article . 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by , who used a context-free grammar induced using the state-split model of TARGETCIT in combination with a standard probabilistic Earley parser to compute surprisal estimates . 1056 Demberg , Keller , and Koller ParsingwithPsycholinguisticallyMotivatedTree-AdjoiningGrammar Table 4 Linear m',\n",
       " 'uistics , negation has mainly drawn attention in sentiment analysis ( ) and the biomedical domain . Recently , two events ( ) targeted negation mostly on those subfields . Among many others , TARGETCIT and propose scope detectors using the BioScope corpus . Considering scope is indeed a step forward , but focus must also be taken into account to represent negated statements and detect their p',\n",
       " 'particular , we use measures such as translation model entropy , inspired by . Additionally , we apply the method suggested by using perplexity ratio instead of cross-entropy difference . TARGETCIT suggest a method for adaptation of translation models . They pass two phrase tables directly to the decoder using multiple decoding paths . As we show in Section 5 , the application of this method to our',\n",
       " 'dness , a dataset with a high Inter-Annotator agreement is required . We use the manually labelled mappings from the Omega ontology2 ( ) to the WordNet senses , provided by the OntoNotes project ( TARGETCIT . The OntoNotes dataset creation involved a rigorous iterative annotation process producing a coarse sense inventory which guarantees at least 90 % InterTagger agreement on the sense-tagging of the sam',\n",
       " 'as the readability or the overall responsiveness . To evaluate such aspects , a manual evaluation is required . A fairly standard approach for manual evaluation is through pairwise comparison ( ; TARGETCIT . In this approach , raters are presented with pairs of summaries generated by two systems and they are asked to say which one is best with respect to some aspects . We followed a similar approach to co',\n",
       " ',10,15 } , where the best results are achieved with R = 3 . We evaluate two versions of COUNT+CFG : one with R = 3 and the other with R = 15 ( K = 50k for both ) . 5 Results and Fragment Analysis We build on TARGETCIT ’ s results and compare against bigram , CFG and TSG baselines . Each baseline model is built from the same E-2 regularized Method coarse fine COUNT+CFG , R = 3 89.1 67.2 COUNT+CFG , R = 15 88.2 66.6 bigra',\n",
       " 'THERCIT ) , we use the most basic one : we raise the likelihood term in power T , where the parameter T is chosen empirically . 7 Empirical Evaluation 7.1 Data and Evaluation We keep the general setup of ( TARGETCIT a ) , to evaluate our models and compare them to the current state of the art . We run all of our experiments on the standard CoNLL 2008 shared task ( ) version of Penn Treebank WSJ and PropBank .',\n",
       " 'r statistics . The artificial errors method is based on generating artificial errors7 in correct native English training data . The method was implemented within the Averaged Perceptron ( AP ) algorithm ( TARGETCIT c ; ) , a discriminative learning algorithm , and this is the algorithm that we use in this work . The NBpriors method is a special adaptation technique for the Naive Bayes algorithm ( ) . W',\n",
       " 'ech quite effectively ( ) , without requiring linguistic resources . They should provide useful generalization for reordering decisions . Inspired by recent successes in semi-supervised learning ( TARGETCIT ; 1 � 2||w||2+C i ci 25 corpus sentences words ( ar ) words ( en ) train 1,490,514 46,403,734 47,109,486 dev 1,663 45,243 50,550 mt08 1,360 45,002 51,341 mt09 1,313 40,684 46,813 Table 3 : Arabic-English C',\n",
       " 'us as it renders inference simpler and affords flexibility ( e.g. , additional aspects can be incorporated into the model or trained separately on different datasets ) . Our work differs from and TARGETCIT in three important respects . Firstly , we develop a genuinely abstractive model that is not limited to deletion . Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned',\n",
       " 'version that uses topic models ( ) and an extended version that includes entity-specific features ( ) . We further apply the discourse-new model by and the pronoun-based model by TARGETCIT . For all of the aforementioned models , we use their respective implementation provided with the Brown Coherence Toolkit10 . Note that the toolkit only returns one coherence score for each document . To',\n",
       " 'see widespread adoption . Online methods ( ) , are recognized to be effective , but require substantial implementation efforts due to difficulties with parallelization . Pairwise ranking ( ; TARGETCIT recasts tuning as classification , and can be very easy to implement , as it fits nicely into the established MERT infrastructure . The MERT algorithm optimizes linear weights relative to a collection of',\n",
       " '64.40 63.71 * 58.31 60.89 58.86 w/ Lifespan 58.14 73.14 * 64.78 * 63.38 58.83 * 61.02 59.52 * Table 7 : B3 , CEAF-03 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows TARGETCIT . Scores are on automatically predicted mentions . 4 Application to coreference resolution To assess the usefulness of the lifespan model in an NLP application , we incorporate it into the Stanford core',\n",
       " 'a wide variety of NLG choices . 8 Discussion In this paper , we show how users ’ utterances can give a dialogue system consistent and reliable indicators not only of how to solve its NLU problems , as in TARGETCIT , but also how to solve its NLG problems . Thus , we can now design dialogue systems to learn to imitate their human users in certain cases . To do so , the system needs to work in a domain where users ar',\n",
       " '-world class of junior schools ought to have ( in English ) an identifier such as junior school or a label such as “ junior school ” . Ontology developers who follow this best practice ( and , according to ( TARGETCIT , the vast majority do ) produce ontologies in which the entities are easily recognisable and understood by human readers who can parse these identifiers , to infer , for example , that “ junior school ” is',\n",
       " 'ther reinforced the authors appreciation of the importance of this feature introduced in ( ) . Strangely enough this feature seems to be unavailable in the standard Moses ( ) and Joshua ( TARGETCIT grammar extractors , that also implement SAMT grammar extraction 5.1.2 Specific choices and settings Joshua Viterbi experiments Based on experiments reported in ( Mylonakis and Sima ’ an , 2011 ; )',\n",
       " 'om raw data in English , an approach that needs both full parsing and semantic interpretation using WordNet ( ) in order to extract verb arguments and measure the similarity betweern verbs . In ( TARGETCIT an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages . However , the obtained results were not assessed in a real information extr',\n",
       " 'nsition system of or the more restrictive dynamic programming variant of Cohen , G´omez-Rodr´ıguez , and , the set of structures that yield binarizable productions with the algorithm of TARGETCIT , or the set of mildly ill-nested structures ( G´omez-OTHERCIT ; G´omez-OTHERCIT ) . As mentioned earlier , the notion of multiplanarity was originally introduced by Yli-Jyr¨a ( 2003 ) , who also presents add',\n",
       " 'pated in both tracks , with an ensemble system . For the dependency track , the ensemble includes the MATE parser ( ) , a best-first variant of the easy-first parser by , and turbo parser ( TARGETCIT , in combination with a ranker that has the particularity of using features from the constituent parsed trees . CADIM ( ) uses their variant of the easy-first parser combined with a feature-rich',\n",
       " 'thology reports , clinical notes , discharge summaries and medical literature . These annotation tools use UMLS , BioPortal3 or in-house built vocabularies as sources for the building of medical lexicons . TARGETCIT proposed a large scale terminology system for storing terms , which are compiled into finite-state lexicons to perform term lookup in texts . For that goal , they set up a relational database where terms',\n",
       " 'using a deterministic heuristic to convert these to phrase alignments ( ) . There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem ( TARGETCIT ; ; DeOTHERCIT ) . However , most of these have met with limited success compared to the simpler heuristic method . One key problem with typical models of phrase alignment is that they choose a si',\n",
       " 'chers concentrated on taking full advantage of the monolingual corpora in both source and target languages , and proposed methods for bilingual lexicon induction from non-parallel data ( , 1999 ; TARGETCIT ; ; Daumé III and ) and proposed unsupervised statistical machine translation ( bilingual lexicon is a byproduct ) with only monolingual corpora ( ) . In the bilingual lexicon indu',\n",
       " 'nsidered string similarity measures alongside a wide range of other features . For our string similarity features , the texts were lemmatized using the implementation of Lancaster Stemming in NLTK 2.0 ( TARGETCIT , and all punctuation was removed . Limited stopword removal was carried out by eliminating the words a , and , and the . The output of each string similarity measure is normalized to the range of [ 0 , 1 ] ,',\n",
       " 'e leaky boundaries in RST-DT , in other corpora this can be true for a larger portion of the sentences . For example , we observe over 12 % sentences with leaky boundaries in the Instructional corpus of ( TARGETCIT . However , we notice that in most cases where discourse structures violate sentence boundaries , its units are merged with the units of its adjacent sentences , as in Figure 7 ( b ) . For example , this is t',\n",
       " 'coreference relations . The main idea that we borrow from paraphrase acquisition is the use of monolingual ( non-parallel ) comparable corpora , which have been exploited to extract both sentence-level ( TARGETCIT and sub-sentential-level paraphrases ( ; Wang and CallisonOTHERCIT ) . To ensure that the NPs are coreferent , we limit the meaning of comparable corpora to collections of documents that report on',\n",
       " 'grid model by . It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment ( ) , and story generation ( TARGETCIT . Based on the original model , a few extensions have been proposed : for example , and suggested additional features to characterize semantic relatedness between entities and features',\n",
       " 'eving state-of-the-art performance over the NP-chunking task on the CoNLL data . Some similar approaches based on classifiers or sequence labeling models were also used for Chinese chunking ( ) . TARGETCIT conducted an empirical study of Chinese chunking on a corpus , which was extracted from UPENN Chinese Treebank-4 ( CTB4 ) . They compared the performances of the state-of-the-art machine learning models f',\n",
       " 'und the noun clusters , and surface realization to order selected modifiers , realize them as postnominal or prenominal , and select final outputs . The system follows an overgenerate-andselect approach ( TARGETCIT , which allows different final trees to be selected with different settings . 4.1 Knowledge Base Midge uses a knowledge base that stores models for different tasks during generation . These models are p',\n",
       " 'low a set of rules based on the graph edge labels . Note that since entailment is a transitive relation , our entailment graph is transitive i.e. , if entail ( s1 , s2 ) and entail ( s2 , s3 ) then entail ( s1 , s3 ) ( TARGETCIT . Rule 1 ) Among the nodes that are connected with bidirectional entailment ( semantically equivalent nodes ) we keep only the one with more outgoing bidirectional and unidirectional entailment relations',\n",
       " 'roblem . Type-based methods combine the ( type ) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation . In recent work , this has been proposed by TARGETCIT , Erk and Padó ( 2008 ) and Thater et al . ( 2010 ; 2011 ) , which differ in the choice of input vector representation and in the combination operation they propose . A different approach has been taken by Er',\n",
       " 'es are equal , this coefficient can not be calculated and is set to 0 . 4.2 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation ( ) of the PARC 700 Dependency Bank ( TARGETCIT . The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers , including RASP ( ) and 2Slight changes in the performance of the baseline p',\n",
       " 'E. We also plan to exploit a greater variety of distributional inference rules . First , we intend to incorporate logical form translations of existing distributional inference rule collections ( e.g. , ( TARGETCIT ; ) ) . Another issue is obtaining improved rule weights based on distributional phrase vectors . We plan to experiment with more sophisticated approaches to computing phrase vectors such as thos',\n",
       " 'tries per language pair ) . The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language , and the ( human ) reference translation as the target language , see ( L. TARGETCIT . It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus . Moreover , the following measures - limiting unwanted statistical effects',\n",
       " 'mains where data distributions may be different ( ) . In the past few years , domain adaptation techniques have been widely applied to various NLP tasks , such as part-of-speech tagging ( ; TARGETCIT ; Daum´e III , 2007 ) , named-entity recognition and shallow parsing ( Daum´e III , 2007 ; ) . There are also lots of studies for cross-domain sentiment analysis ( ) . However , most of them foc',\n",
       " 'nalyzing their structures , thus greatly facilitating syntactic and semantic analysis of sentences . In fact , previous studies have revealed other good reasons for parsing internal structures of words ( TARGETCIT ; ) . The second argument is that in Chinese many linguistic units can form both words and phrases with exactly the same meaning and part-of-speech , which 1445 Proceedings of the 2012 Joint Con',\n",
       " 'has been shown to perform better than a number of recent syntactic systems including and . Moses-Del : A phrase-based approach also based on Moses which incorporates phrasal deletion ( TARGETCIT b ) . The code was obtained from the authors . For an additional data point to understand the benefit of the grammar augmentation , we also evaluated a deletion-only system previously used for text compre',\n",
       " 'riments , we use all of them as features . In hierarchical clustering , for each sense cluster of a path , we pick the most frequent entity type as a feature . This approach can be seen as a proxy to ISP ( TARGETCIT , since selectional preferences are one way of distinguishing multiple senses of a path . Our Approach+Type This system adds Wikipedia entity type features to our approach . The Wikipedia feature is the',\n",
       " 'me label , and we run Giza++ ( ) to align the resulting semantically augmented corpora . Finally , we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit ( TARGETCIT . For the matching phase , we first annotate T and H in the same way we labeled our parallel corpora . Then , for each n-gram order ( n=1 to 5 , excluding punctuation ) , we use the SPT to calculate a matchi',\n",
       " 'rser2 developed in . 4.4 Experiments Table 3 shows the resulting readability correlations . The last four rows show the correlation scores for our coherence model : LIN is the default model by ( TARGETCIT , LIN+C is LIN with the intra-cell feature class , LIN+E is enhanced with the Explicit/Non-Explicit feature class . We name the LIN model with both new feature sources ( i.e. , LIN+C+E ) DICOMER – a DIscou',\n",
       " 'te , so past algorithms required extra processing stages to filter out these irrelevant documents . and make initial decisions as to whether they should extract or not from a document . TARGETCIT use a genre detector for this problem . Even the generative HMM-based model of Cheung et al . ( ) requires an extra filtering parameter . Our formal model is unique in not requiring additional eff',\n",
       " 'Dependency Treebank . greedy , deterministic parsing ( ; Attardi 2006 ; Nivre 2008 ) , but globally trained models and non-greedy parsing methods such as beam search are increasingly used ( ; TARGETCIT ; ) . In empirical evaluations , the two main approaches to dependency parsing often achieve very similar accuracy , but transition-based parsers tend to be more efficient . In this article , we wi',\n",
       " 'xt of the RTE challenge , and found that often rules that are specific to a certain concept are required . Another example for a semantic inference algorithm that is utilized in real time is provided by TARGETCIT , who recently described a system that , given two terms , determines the taxonomic relation between them on the fly . Last , we have recently suggested an application that uses focused entailment graphs',\n",
       " 'mpare binarization strategies for PCFG-LA parsing , and conclude that the differences between them have a minor effect on parsing accuracy as the num1159 ber of latent annotations increases beyond two . TARGETCIT are forced to use head binarization when combining their lexicalized and unlexicalized parsers . Dual decomposition allows us to combine models with different binarization schemes . 3 Approximation of P',\n",
       " 'an , French , and German ) . The task presents twenty polysemous nouns with fifty instances each to be mapped onto normalised ( lemmatised ) translations in all languages . The task is described in detail by TARGETCIT . Trial data is provided and has been used to optimise system parameters . Due to the unsupervised nature of the task , no training data is provided . However , given that the gold standard of the task is',\n",
       " 'llow concepts to align to topics with varying degrees of granularity , from the very general ( e.g . war ) to the very specific ( e.g . wild west ) . The second is to use constrained LDA approaches ( ; TARGETCIT to attempt to force at least one topic to correspond to each of our seed concept lists . A different approach would leave behind seed lists entirely . In our current approach , only about one third of th',\n",
       " 's allow to use parsers to extract accurate lexico-syntactic information , beyond what can be found in limited annotated corpora . Our work can also be compared with self training approaches to parsing ( TARGETCIT ; ) where a parser is first trained on a treebank and then used to parse a large raw corpus . The parses produced are then added to the initial treebank and a new parser is trained . The main di',\n",
       " 'overed events . In Section 4.3 we show how the discovered event-topic affinity vectors can be useful . For comparison , we consider an existing method called TimeUserLDA introduced in our previous work ( TARGETCIT . TimeUserLDA also models topics and events by separating topic tweets from event tweets . However , it groups event tweets into a fixed number of bursty topics and then uses a twostate machine in a pos',\n",
       " 'Penn Treebank , whether that node is a modifier . This corpus was compiled by combining information from Propbank ( ) with a set of heuristics , as well as the NPbranching structures proposed in ( TARGETCIT . It is important to note that this corpus can only serve as a rough benchmark for evaluation of our model , as the heuristics used in its development did not always follow the correct linguistic analy',\n",
       " 'orem prover . By contrast , our approach uses Markov logic with probabilistic inference . Semantic Textual Similarity ( STS ) is the task of judging the similarity of two sentences on a scale from 0 to 5 ( TARGETCIT . Gold standard scores are averaged over multiple human annotations . The best performer in 2012 ’ s competition was by B¨ar et al . ( 2012 ) , an ensemble system that integrates many techniques including st',\n",
       " 'Figure 1 : T/V label induction for English sentences in a parallel corpus with annotation projection in one language , but remain covert in the other . Examples include morphology ( ) and tense ( TARGETCIT . A technique that is often applied in such cases is annotation projection , the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not ( OTHERCI',\n",
       " 'l approach to machine translation ( MT ) is only formally syntax-based , cube pruning can also be utilized for decoding with syntactically or semantically enhanced models , for instance those by , TARGETCIT , , or . Here , we look into the following key aspects of hierarchical phrase-based translation with cube pruning : • Deep vs. shallow grammar . • k-best generation size . • Hypothesis rec',\n",
       " \"rd aligner itself , i.e. , by bootstrapping on its output . A Maximum Entropy model based approach for English—Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by TARGETCIT . They considered 4 features : translation score , transliteration score , source NE and target NE 's co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence . OT\",\n",
       " 'sed on the algorithms in machine learning , such as averaged perceptron ( ) , maximum entropy ( ) , Margin Infused Relaxed Algorithm ( MIRA ) ( ) , or pairwise rank optimization ( PRO ) ( TARGETCIT . They primarily differ in the mode of training ; online or MERT-like batch , and in their objectives ; max-margin ( ) , conditional loglikelihood ( or softmax loss ) ( ) , risk ( ) , or',\n",
       " 'ns from comparative sentences . Many previous research efforts related to Opinion Target Identification ( ) , focus on the domain of product reviews where they exploit the genre in multiple ways . TARGETCIT used unsupervised methods to identify stances in online debates . They mine the web to find associations indicative of opinions and combine them with discourse information . Their problem essentially de',\n",
       " 'rt the use of grammars containing fully non-lexical unary rules ( such as NP — X1 |NN1 ) . Unless the -- AllowUnary option is given , the rule extractor eliminates these rules using the method described in TARGETCIT . 2.4 Scope Pruning Unlike hierarchical phrase-based rule extraction , GHKM places no restriction on the rank of the resulting rules . In order that the grammar can be parsed efficiently , one of two app',\n",
       " 'ils of our systems . 2 German-English 2.1 Baseline All available data was tokenized , truecased , and the maximum number of tokens were fixed to 70 for the translation model . The Moses open SMT toolkit ( TARGETCIT was used with MGIZA++ ( ) with the standard alignment heuristic grow-diag-final ( ) for word alignments . Good-Turing smoothing was used for phrase extraction . Systems were tuned on newst',\n",
       " 'gate it up and perform inference on ~µ0 unencumbered by the functional mess of Equation ( 1 ) . Thus Poisson-Dirichlet processes allow one to do Bayesian reasoning on hierarchies of probability vectors ( TARGETCIT ; ) . Negatively , however , one needs to sample the auxiliary variables t~ leading to some problems : The range of tk , { 0 , ... , nk } , is broad . Also , contributions from individual data zi have bee',\n",
       " 'inites , they only analyse a subset of the mentions we consider carrying IS . also concentrate on a subproblem of IS only , namely the hearer-old/hearer-new distinctions for person proper names . TARGETCIT and both present algorithms for IS detection on Nissim et al. ’ s ( 2004 ) Switchboard corpus . Both papers treat IS classification as a local classification problem whereas we look at dependencie',\n",
       " 'is proposed here for tutoring . 1 Introduction For the last decades , Intelligent Tutoring Systems ( ITSs ) have become powerful tools in various domains such as mathematics ( ) , physics ( ; TARGETCIT ; ) , computer sciences ( ) , reading ( ) , or foreign languages ( ) . Their appeal relies on the fact that each student does not have to follow an average teaching strategy ,',\n",
       " 'd to the order of the clauses in the target language text . 2 State-of-the-art The task of clause alignment is closely related to that of sentence alignment ( ) and phrase alignment ( DeOTHERCIT ; TARGETCIT . There are two main approaches – statistical and lexical , often employed together to produce hybrid methods . Machine learning techniques are applied to extract models from the data and reduce the nee',\n",
       " 'an introduction to Bayesian decipherment and then describe how to use slice sampling for it . 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing ( ; TARGETCIT b ) . It is very attractive for problems like word substitution ciphers for the following reasons . First , there are no memory bottlenecks as compared to EM , which has an O ( N · V 2 ) space complexity . Sec',\n",
       " 'mples of Used Syntactic Patterns 3.1.2 Unsupervised Word Alignment Model In this subsection , we present our method for capturing opinion relations using unsupervised word alignment model . Similar to ( TARGETCIT , every sentence in reviews is replicated to generate a parallel sentence pair , and the word alignment algorithm is applied to the monolingual scenario to align a noun/noun phase with its modifiers . W',\n",
       " 'ed with a variant of the Chi ( ) . Particle smoothing variants of nese Restaraunt Process ( CRP ) called the Chinese SMC reduce the relative variance of marginals early Restaurant Franchise ( CRF ) ( TARGETCIT ; Goldwater in the sequence , as well improving the diversity et al. , 2006 ) . In the CRP analogy , each latent variof the final sample ( ) . Par- able in a sequence is represented by a customer ent',\n",
       " 'to a sequence of operations . The reordering operations ( gaps and jumps ) are generated by looking at the position of the translator , the last foreign word generated etc . ( Please refer to Algorithm 1 in TARGETCIT ) . The probability of an operation depends on the n−1 previous operations . The model is smoothed with Kneser-Ney smoothing . 3 POS-based OSM Model Part-of-speech information is often relevant for trans',\n",
       " 'unannotated data , ranging from self-training and co-training ( McOTHERCIT ) to more complex methods that collect statistical information from unannotated sentences and use them as additional features ( TARGETCIT ; ) . In this paper , we propose an alternative approach to semi-supervised dependency parsing via feature transformation ( ) . More 1303 Proceedings of the 2013 Conference on Empirical Me',\n",
       " 'This observation has motivated work in grammar transformations that reduce the size of the nonterminal set , often resulting in substantial gains in parsing or translation speed ( ; DeOTHERCIT ; TARGETCIT . More formally , the upper bound on parsing complexity is always at least linear in the size of the grammar constant G , where G is often loosely defined as a grammar constant ; give a nice ana',\n",
       " 'lations that it is indeed correct . With more relations the inference process becomes more complex . Recently there has been some work trying to address the shortcomings of the plain F1 score ( ; TARGETCIT ; ) . However , the community has not yet come to a consensus on the best evaluation approach . Two recent evaluations , TempEval-3 ( UzOTHERCIT ) and the 2012 i2b2 Challenge ( ) , used an imp',\n",
       " '61 0.731 Table 9 : Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 ( ) 0.953 0.946 0.950 CRF + rule-system ( ) 0.947 0.955 0.951 Semi-perceptron ( TARGETCIT ) N/A N/A 0.945 Latent-variable CRF ( ) 0.956 0.948 0.952 ADF-CRF ( ) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10 : Comparison of our approach with the state-of-art systems P',\n",
       " 'fined in terms of a test collection C , consisting of |C |partial scripts , where for each partial script c with missing event e , ranksys ( c ) is |C |cEC 1 ranksys ( c ) This is the evaluation metric used by TARGETCIT . • Recall @ N. The fraction of partial scripts where the missing event is ranked N or less6 in the guess list . 1 | { c : c E C n ranksys ( c ) G N } | In our experiments we use N = 50 , but results are roughly',\n",
       " 'pipeline ( Best ) , the human performance and baseline performance . formed automatic dialog act classification . However , there has been previous work on classifying speech acts in other discourse types . TARGETCIT a ) use Support Vector Machines ( SVM ) and Transformation Based Learning ( TBL ) for the automatic assignment of five speech acts to posts taken from student online forums . They report individual F1-score',\n",
       " 'cost model feature and the six MSD bidirectional lexical distortion model ( ) features ( LINEAR+LEX ) , the outbound and inbound distortion model features discriminating nine distortion classes ( TARGETCIT ( 9-CLASS ) , the proposed pair model feature ( PAIR ) , and the proposed sequence model feature ( SEQUENCE ) . 4.2 Training for the Proposed Models Our distortion model was trained as follows : We used 0.2 mil',\n",
       " 'tes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space . Its 265 S VP NP-1 S NN - com 266 TARGET-order : :v to : :t ROOT VBD T O OP 267 ( TARGETCIT . We selected the subset of frames containing more than 100 sentences annotated with a verbal predicate for a total of 62,813 sentences in 187 frames ( i.e. , very close to the VerbNet datasets ) . For bo',\n",
       " 'rization , and so on . Thus , there have been many studies on automatic keyword extraction . The frequency-based keyword extraction with TFIDF weighting ( ) and the graph-based keyword extraction ( TARGETCIT are two base models for this task . Many studies recently tried to extend them by incorporating specific information such as linguistic knowledge ( ) , web-based resource ( ) , and semantic',\n",
       " 'pe-based methods combine the ( type ) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation . In recent work , this has been proposed by , TARGETCIT and Thater et al . ( 2010 ; 2011 ) , which differ in the choice of input vector representation and in the combination operation they propose . A different approach has been taken by Erk and Padó ( 2010 ) , OTH',\n",
       " 'sortal variables – common formal tools of semantic representation . Also , the representations can be related to each other in a transparent way : Two RMRS structures can then be tested for subsumption ( TARGETCIT a ) , in order to see whether one structure is a less specific variant of the other . If one subsumes the other , the difference between both can be formulated as an RMRS containing all those statements t',\n",
       " 'between the phrases : word overlap , edit distance , ngram-overlap , longest common subsequence and Lesk ( ) . The other scores were computed using lexical resources : WordNet ( ) , VerbOcean ( TARGETCIT , paraphrases ( ) and phrase matching ( ) . We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy , hypernymyhypon',\n",
       " 'erings . addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn lexical reordering triggers . TARGETCIT showed that using larger phrasal units during decoding is superior to MTU-based decoding in an N-gram-based system . However , they do not use phrase-based models in their work , relying only on the OSM',\n",
       " 'source sentences with the Berkeley Parser3 ( ) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain dependency structures . We obtain the word alignments by running GIZA++ ( TARGETCIT on the corpus in both directions , applying “ grow-diag-final-and ” refinement ( ) . We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword',\n",
       " 'nother well known latent model of meaning , which takes a generative approach , is Latent Dirichlet Allocation ( ) . Tensor factorization has been used before for the modeling of natural language . TARGETCIT describes a tensor factorization model for the construction of a distributional model that is sensitive to word order . And Van de uses a tensor factorization model in order to construct a thr',\n",
       " '& McOTHERCIT ) . We use EVALB and eval.pl to calculate scores . For hypothesis testing , we used the paired bootstrap test recently empirically evaluated in the context of NLP by . This 7Following TARGETCIT , the BROWN test set is usually divided into 10 parts . If we start indexing at 0 , then the last ( test ) section has index 9 . We received the BROWN data splits from David McClosky , p.c . involves drawing',\n",
       " 'e F model the gradients correspond to those presented by . The remaining gradients are easy to derive ; we omit details for brevity . 3 Related Work This work extends a previous workshop paper ( TARGETCIT a ) by introducing the neighborhood and entity model , by working with the BPR objective , and by more extensive experiments . Relational Clustering There is a large body of work aiming to discover latent',\n",
       " 'test time . Parameters for LDA and relational LDA were optimized following the same parameter tuning procedure described above . We also compared our model against the unsupervised method introduced in TARGETCIT . Their key idea is to cluster pairs of co-occurring named entities according to the similarity of their surrounding contexts . Following their approach , we measured context similarity using the vector',\n",
       " 'he concludes that the proper treatment of unknown words is of the utmost importance and highlights the problem of producing translations of up to 140 characters , the upper limit on tweet lengths . In ( TARGETCIT , the authors describe their efforts to collect bilingual tweets from a stream of tweets acquired programmatically , and show the impact of such a collection on developing an Arabic-to-English translat',\n",
       " 'include other words ( or phrases ) that appear in the immediate context ( n-gram window ) surrounding w in the monolingual corpus . Similarly , we can add other features based on topic models , orthography ( TARGETCIT , temporal ( ) , etc . to our representation all of which can be extracted from monolingual corpora . Next , given two high dimensional vectors u and v it is possible to calculate the similarity be',\n",
       " 'ant problem is determining the subjectivity of a given sentence . In an early study , the effects of adjective orientation and gradability on sentence subjectivity was studied ( ) . Wiebe et al . ( TARGETCIT presents a broad survey on subjectivity recognition and the key elements that may have an impact on it . In estimating the sentiment polarity , the use of higher-order n-grams is also studied . Pang et .',\n",
       " 'well because the number of variables is O ( |V |2 ) and the number of constraints is O ( |V |3 ) . Thus , even a graph with ∼80 nodes ( predicates ) has more than half a million constraints . Consequently , in ( TARGETCIT , they proposed a method that efficiently decomposes the graph into smaller components and applies an ILP solver on each component separately using a cutting-plane procedure ( ) . Although this',\n",
       " 'are likely useful in disambiguating between the different temporal classes . To verify this , we examine the convolution tree fragments that lie on the support vector of our SVM classifier . The work of TARGETCIT in linearizing kernel functions allows us to take a look at these tree fragments . Applying the linearization process leads to a different classifier from the one we have used . The identified tree frag',\n",
       " 'inistically compute C ( a ) , the final set of coreference chains . While the features of this model factor over coreference links , this approach differs from classical pairwise systems such as or TARGETCIT . Because potential antecedents compete with each other and with the non-anaphoric hypothesis , the choice of ai actually represents a joint decision about i−1 pairwise links , as opposed to systems tha',\n",
       " 'employed a hidden Markov model ( 96.6 % accuracy ) , adopted an averaged perception discriminative sequence model ( 97.1 % accuracy ) . All these models fix the order of inference from left to right . TARGETCIT introduced a cyclic dependency network ( 97.2 % accuracy ) , where the search is bi-directional . have further shown that better results ( 97.3 % accuracy ) can be obtained using guided learning , a f',\n",
       " 'many data-driven tasks of natural language processing , such as statistical machine translation ( ) , cross-language information retrieval ( ) , and bilingual lexical acquisition ( ; TARGETCIT , to name but a few . A general way to develop such corpora from web texts starts from exploring the structure of known bilingual websites , which are usually organized * Performed while a research assoc',\n",
       " 'slation ( SMT ) models ( ) are trained using large , sentence-aligned parallel corpora . Unfortunately , parallel corpora are not always available in large enough quantities to train robust models ( TARGETCIT . In this work , we consider the situation in which we have access to only a small amount of bitext for a given low resource language pair , and we wish to supplement an SMT model with additional transl',\n",
       " 'York Times subset of Gigaword9 , using their setup of 50 iterations with 100 relation types . • Reverb A sophisticated Open Information Extraction system ( ) . Unsupervised Semantic Parsing ( USP ; TARGETCIT ; USP ; ; USP ; ) would be another obvious baseline . However , memory requirements mean it is not possible to run at this scale ( our system is trained on 4 orders of magnitude more data t',\n",
       " 'provement over Baseline 0.6 0.8 Static Linking 1.6 1.4 1.2 1 Non-Transcripts Transcripts 82.50 79.77 83.03 - 83.06 80.25 83.32 81.13 158 Method Pairwise F1 MUC F1 CEAF F1 B3 F1 P / R P / R P / R P / R TARGETCIT - - - - - - 86.7 73.2 79.3 71.6 46.2 56.1 80.4 71.8 75.8 - - 86.3 75.4 80.4 - - - 80.1 - - - 81.8 Wiki-linking 64.15 14.99 24.30 74.41 28.39 41.10 58.54 58.4 58.47 92.89 57.21 70.81',\n",
       " 'l ( synonym resolution ) . As the preprocessing instance-detection step for the problem studied in this paper , open IE extracts relation instances ( in the form of triples ) from the open domain ( ; TARGETCIT ; ) . For efficiency , they only use shallow features . Reverb ( ) is a state-of-the-art open domain extractor that targets verb-centric relations , which have been shown in to cov',\n",
       " 'ations over a parallel corpus of 12 million English words and Chinese words , almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results ( DeOTHERCIT ; TARGETCIT ; DeOTHERCIT ) . Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents a',\n",
       " 'HERCIT ) , which also use flat frames to represent evaluations . Since the cross sentences relations are considered in this work , the discourse-level relation extraction methods are also related to ours . TARGETCIT proposed to use an unsupervised approach to recognizing discourse relations . Lin et al . ( 2009 ) analyzed the impacts of features extracted from contextual information , constituent parse trees , dependenc',\n",
       " 'to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations . TARGETCIT apply this method to generate inflections after translation in two different ways : by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori . follow a',\n",
       " 'nd sentences . However , this has been achieved by either using a subset of relations that can be found in discourse theories ( ) or by using directly ( ) the output of discourse parsers ( TARGETCIT . Discourse cues as predictive features of topic boundaries have also been considered in . This work was extended by , where discourse connectors are used as features for modeling subj',\n",
       " 'ipulate channel probabilities Pθ ( fi|ei ) so that the probability of the observed texts P ( f ) θ is maximized . Theoretically , we can directly apply EM , as proposed in ( ) , or Bayesian decipherment ( TARGETCIT a ) to solve the problem . However , unlike letter substitution ciphers , word substitution ciphers pose much greater challenges to algorithm scalability . To solve a word substitution cipher , the EM algor',\n",
       " 'tag set ing . In early studies , rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus ( ) . For instance , TARGETCIT used a maximum entropy model for POS tagging . In this study , the features for rarely appearing words in a corpus are expanded to improve the overall performance . Following this direction , various stud',\n",
       " 'ihood of a verb being metaphorical based on its selectional preference strength ( ) ; ( 2 ) identifies a set of literal paraphrases for verbs that may be used metaphorically using the algorithm of TARGETCIT ; ( 3 ) performs reverse paraphrasing of each of the identified paraphrases , aiming to retrieve the original expression ; and ( 4 ) if the original expression is retrieved then the verb is tagged as litera',\n",
       " 'ion extraction , where several techniques have been proposed to cluster semantically similar verbalizations of relations ( ) . Early unsupervised approaches to the SRL problem include the work by TARGETCIT , where the VerbNet verb lexicon was used to guide unsupervised learning , and a generative model of which exploits linguistic priors on syntactic-semantic interface . More recently , the role i',\n",
       " 'for the Dundee corpus ( a collection of newspaper texts with eye-tracking data from ten participants ; ) . Phrase structure grammars ( PSGs ) have also been amply used as language models ( ; TARGETCIT ; ) . PSGs can combine statistical exposure effects with explicit syntactic rules , by annotating norms with their respective probabilities , which can be estimated from occurrence counts in text',\n",
       " 'Number , Chinese characters , and Others , are distinguished . Word length is in Unicode . 184 Figure 1 : Example lattice with LM projection 4 Use of Language Model Language Model Augmentation Analogous to TARGETCIT , we can exploit the fact that レッド reddo ( red ) in the example ブラキッシュレッド is such a common word that one can expect it appears frequently in the training corpus . To incorporate this intuition , we used l',\n",
       " 'resolvers not attempt to handle difficult pronouns ? One reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora such as MUC , ACE , and OntoNotes ( ; TARGETCIT . In fact , the Stanford coreference resolver ( ) , which won the CoNLL-2011 shared task on coreference resolution , adopts the once-popular rule-based approach , resolving pronouns simply with rul',\n",
       " '( ) . In addition , new research on the topic has explored the translation of sentences into many languages ( ) , as well as the projection of monolingual knowledge onto another language ( TARGETCIT . In our research we focus on knowledge-based methods and tools for multilingual WSD , since knowledge-rich WSD has been shown to achieve high performance across domains ( ) and to compete with',\n",
       " '3.2 Base Model Similar to the work of , our base model uses Conditional Random Fields1 to learn a sequence labeling model . Each label is either beginning of EDU ( B ) or continuation of EDU ( C ) . TARGETCIT and use boundary labels , which are assigned to words at the end of EDUs . , we use beginning labels , which are assigned to words at the beginning of EDUs . However , we can convert an ou',\n",
       " 'ly create AMRs for Ontonotes sentences , with a smatch score of 0.74 against human consensus AMRs . 5 Related Work Related work on directly measuring the semantic representation includes the method in ( TARGETCIT , which evaluates semantic parser output directly by comparing semantic substructures , though they require an alignment between sentence spans and semantic sub-structures . In contrast , our metric does',\n",
       " 'rds ( ) . In dialogue management , IUs can correspond to dialogue acts ( ) . In speech synthesis , IUs can correspond to speech unit sequences which are mapped to segments and speech plans ( TARGETCIT . IUs are typically linked to other IUs by two types of relations : same-level links connect IUs sequentially and express relationships at the same level ; grounded-in links express hierarchical relatio',\n",
       " 'n for TV news ( ) . They used rule based method for sentence partitioning . In this paper , each compound sentence was divided into some criteria by using compound sentence markers and “ CaboCha ” ( TARGETCIT which is a Japanese dependency structure Analyzer . 5 Criterion words extraction Firstly , we defined criterion words as words that the reviewers notice in the reviews . Criterion words were frequently f',\n",
       " 'e previous row . Symbols in the bottom two lines indicate significant difference between upper bound systems and their corresponding counterparts . Pre Rec Fl OOVR ICTCLAS ( 2003 ) 0.640 0.767 0.698 0.551 TARGETCIT 0.661‡ 0.691‡ 0.675 0.572‡ LCRFiwr > -LCRFcws 0.741‡ 0.775‡ 0.758∗ 0.607∗ FCRF 0.757‡ 0.801‡ 0.778∗ 0.633∗ LCRFiwr > -LCRFcws-UB 0.807‡ 0.815‡ 0.811∗ 0.731‡ FCRF-UB 0.820‡ 0.833‡ 0.826∗ 0.758‡ In general ,',\n",
       " '12 11 8 7 ( a ) IPM ( b ) unbalanced ( c ) balanced ( d ) asynchronous Figure 1 : Comparison of various methods for parallelizing online learning ( number of processors p = 4 ) . ( a ) iterative parameter mixing ( TARGETCIT . ( b ) unbalanced minibatch parallelization ( minibatch size m = 8 ) . ( c ) minibatch parallelization after load-balancing ( within each minibatch ) . ( d ) asynchronous minibatch parallelization ( ) ( no',\n",
       " 'of siblings ( adjacent edges with a common head ) and grandchildren ϕ ( x , y ) = � ϕehm ( x ) ehmEy � + egh , ehmEy � + ehm , ehsEy There are two versions of second order models , used respectively by and TARGETCIT . The difference is that Carreras ’ only considers the outermost grandchildren , while Koo and Collin ’ s allows all grandchild features . Both models permit O ( n4 ) running time . Third-order models score ed',\n",
       " 'ext ( ) , correcting errors with a generative lattice and PCFG reranking ( ) , and identifying a broad range of errors in ESL essays by examining linguistic features of words in sequence ( TARGETCIT . In a 2011 shared ESL correction task ( ) , the best performing system ( ) corrected preposition , article , punctuation and spelling errors by building classifiers for each category . This',\n",
       " 'd senses makes evaluation in traditional word sense disambiguation tasks difficult . However , correla88 tion to human word similarity judgement may provide a way of intrinsic evaluation of the models ( TARGETCIT . The Usim bench mark data look promising for evaluation of word similarity in context ( ) . It is also worth exploring ways to optimise the algorithm , as this has not been the focus of our work',\n",
       " 't guarantee that they can understand or correctly assign the author ’ s intended interpretation or emotional state . In this paper , we investigate a different approach via distant supervision ( see e.g . ( TARGETCIT ) . By using conventional markers of emotional content within the texts themselves as a surrogate for explicit labels , we can quickly retrieve large subsets of ( noisily ) labelled data . This approach ha',\n",
       " 't based on topical similarity in actor speech . A theorist who has grappled with the limitations of network analysis is Franco Moretti . In Network Theory Plot Analysis , takes a similar path as TARGETCIT , where the act of speech signifies interaction . points out that his close reading of the network extracted from Hamlet is limited by several factors . First , edges are unweighted , giving equa',\n",
       " 'hich calculated with all three formulae , while the second used only the simplest one . In this way , the article aims to address both the lack of common TS evaluation metrics as suggested in Section 2 ( TARGETCIT and the scarcity of reading comprehension ( ) evaluation with real users ( ) , by proposing a tailored approach for this type of text simplification evaluation . With this article we aim a',\n",
       " 'atures described in Section 5.2 7Available open-source : http : //phontron.com/lader except opos and ocfg . In addition , we test systems with opos and ocfg added . For English , we use the Stanford parser ( TARGETCIT for both POS tagging and CFG parsing . For Japanese , we use the KyTea tagger ( ) for POS tagging,8 and the EDA word-based dependency parser ( ) with simple manual head-rules to convert a',\n",
       " 'h1 , ch2 ) Ey w · fgra ( x , h , c , ch1 , ch2 ) ( 3b ) E ( h , c , cm1 , cm2 ) Ey w · fgra ( x , h , c , cm1 , cm2 ) ( 3c ) E ( h , c , cmo , tmo ) Ey w · fgra ( x , h , c , cmo , tmo ) Feature Set . The feature set of the transition model is similar to that of TARGETCIT . In addition , we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages . The feature sets of the completion mode',\n",
       " 'ional Linguistics , pages 187–192 , Sofia , Bulgaria , August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far ( ; TARGETCIT a ; ) have only scratched this great potential and are still considered as proof-of-concept studies only . One possible reason of this delay with respect to the image analysis community might be',\n",
       " 'distribution p ( zId ) from a Dirichlet prior . Then for each word to be generated , it picks a topic z for that word , and then a word from the multinomial distribution p ( wIz ) . Following earlier work like TARGETCIT , we ran LDA ( ) on the ACL Anthology , 36 Average Papers Per Year Active Female Male Gender Figure 6 : The average number of papers per active year , where an author is considered active in years',\n",
       " 'e problem when using the SVM Tree kernel as relational classifier is that it allows only for binary classification so that we need to train several binary networks in a one-vs-all paradigm ( see also ( TARGETCIT ) , which will not be able to use the multiclass dependencies of the relational features to optimum effect . 5.4 Results Table 7 shows the comparison of collective classification to local classification',\n",
       " 'ave to rely on using character n-grams as features . Despite recent advances in model combination ( ) , joint learning ( ) and integration of supervised and unsupervised methods ( ; TARGETCIT , etc. , an inherent problem with OOV words is that they are novel character combinations seldom occurring in a training corpus , giving machine learning methods little evidence for prediction . Like oth',\n",
       " 's like chunk head , chunk distance and chunk boundary information . Finally we also experimented with some clause-based features like head/child of a clause , clausal boundary information . Turbo parser ( TARGETCIT uses the concept of supported and unsupported features to mitigate the effect of having large number of features to some extent . 180 4 Experiments and Results 4.1 Data The training and development dat',\n",
       " 'from the constituency treebank as described in . Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by TARGETCIT . 22We also provided a predicted-all scenario , in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology',\n",
       " 'd , we obtain word similarities with all words in the vocabulary through Equation 5 , and output the most similar words by this metric . The second method is based on a random walk approach , similar to ( TARGETCIT , wherein a probabilistic interpretation is imposed on the graphs by row-normalizing all of the matrices involved ( word similarity , feature similarity , and importance weights ) , implying that the trans',\n",
       " 'maxima ( ) . To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed ( e.g . ( DeOTHERCIT ; TARGETCIT ) . While interesting results have been reported , these alternatives have failed to dislodge the IBM approach . In this paper we proposed to retain the original generative stories of the IBM models , whi',\n",
       " 'e need to tune 5 · ( n + 1 ) features . Because such MERT ( ) tuning may be unstable for higher n , several methods were proposed where the n+1 phrase tables are merged into a single one ( ; TARGETCIT . Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables , leading to spurious ambiguity and thus less diversity in n-best lists of a',\n",
       " 'test whether better semantic responses could be learned from data of higher quality , and to measure how it affects the response-based similarity method and the quality of induced lexicons . Following ( TARGETCIT ; ) , we consider only noun word types . We retain only nouns that occur at least 5 times in the corpus . We record the lemmatized form when available , and the original form otherwise . Again foll',\n",
       " 'pages and apply ( among other features ) WordNet similarity metric on pairs of knowledge attributes to determine attribute compatibility . We have integrated the strengths of rule-based systems such as ( TARGETCIT ; ) into a multi-sieve machine learning framework . We show that training sieve-specific models significantly increases the performance on most intermediate sievesieves . We develop a novel appr',\n",
       " 'transition-based model for lexicalized constituent parsing . They use a beam-search decoder so that the transition action sequence can be globally optimized . The averaged perceptron with early-update ( TARGETCIT is used to train the model parameters . Their transition system contains four kinds of actions : ( 1 ) SHIFT , ( 2 ) REDUCE-UNARY , ( 3 ) REDUCE-BINARY and ( 4 ) TERMINATE . The system can provide binarzied CFG tr',\n",
       " 'based on the extra knowledge provided by the paraphrase candidates of the original queries . 2.1 Paraphrase Extraction Paraphrases can be mined from various resources . Given a bilingual corpus , we use TARGETCIT ’ s pivot-based approach to extract paraphrases . Given a monolingual corpus , ’ s method is used to extract paraphrases based on distributional hypothesis . Additionally , human annotated data can',\n",
       " 'pendency treebanks kindly provided by the organizers for the 9 languages of the task , namely Arabic3 , Basque ( ) , French ( Abeillé et al. , 2003 ) , German ( ) , Hebrew ( Sima ’ an et al. , 2001 ; TARGETCIT ; ) , Hungarian ( ) , Korean ( ) , Polish ( ´Swidzi´nski and Woli´nski , 2010 ) , Swedish ( ) . Being very short in time , we essentially used the same set of around 110 templates',\n",
       " 'e on sentence reduction ( ) . The extraction by of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task ( ; TARGETCIT ; McOTHERCIT ; Galley and McOTHERCIT ) . In particular , McOTHERCIT expanded on Knight & Marcu ’ s ( 2002 ) transitionbased model by using dynamic programming to recover optimal transition sequences , and OTHE',\n",
       " 'hese techniques require training data with hand-labeled domain-specific logical expressions . Recently , alternative forms of supervision were introduced , including learning from question-answer pairs ( TARGETCIT ; ) , from conversational logs ( ) , with distant supervision ( ) , and from sentences paired with system behavior ( ) . Our work adds to these efforts by demonstrating a new',\n",
       " 'nments in training data and alignments accepted by a synchronous grammar ( learned from data ) . This is useful for literature on learning from word aligned parallel corpora ( e.g. , ( ; DeOTHERCIT ; TARGETCIT ; Mylonakis and Sima ’ an , 2011 ; ; McOTHERCIT ) ) . A theoretical , formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well . We e',\n",
       " 'ti-objective methods based on roundrobin iteration of single objective optimizations . Research in SMT parameter tuning has seen a surge of interest recently , including online/batch learning ( ; TARGETCIT , large-scale training ( ) , and new discriminative objectives ( ) . However , few works have investigated the multi-metric tuning problem in depth . Linear combination of BLEU and TER is re',\n",
       " 'en combined with dimensionality reduction techniques such as Latent Semantic Analysis ( LSA ) ( ) , and generative machine learning models ( Rudner and 33 ) as well as discriminative ones ( TARGETCIT . As multiple factors influence the linguistic quality of texts , such systems exploit features that correspond to different properties of texts , such as grammar , style , vocabulary usage , topic similar',\n",
       " 'uation . 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks , such as coreference resolution ( ; TARGETCIT , information retrieval ( ) , or question answering ( ) . In particular , the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named en',\n",
       " 't of the tree in both trees is calculated as a number in ( 0 , 1 ) , and the mean of these similarities over all words in a sentence is the score for this sentence.1 While also not 1The very lenient IOB ( TARGETCIT , ) accuracy measure , used sometimes in chunking , can be considered as an extreme case of the LA measure . 118 Proceedings of the 6th Linguistic Annotation Workshop , pages 118–123 , Jeju , Republ',\n",
       " 'eference , we pre-processed each of the datasets using tools7 similar to those used to create the Annotated Gigaword corpus ( ) . Extended Event Coreference Bank Based on the dataset of , TARGETCIT introduced the Extended Event Coreference Bank ( EECB ) to evaluate cross-document event coreference . EECB provides document clusters , within which entities and events may corefer . Our task is different',\n",
       " 'y networks , and showed the best contemporary results . Gim´enez and M ` arquez ( 2004 ) used one-pass , left-to-right and right-to-left combined tagging algorithm and achieved near state-of-the-art results . TARGETCIT presented a tagging approach using guided learning for bidirectional sequence classification and showed current state-of-the-art results.3 Our individual models ( generalized and domainspecific ) are si',\n",
       " 'layer ( NNLM ) with 4 layers 108.0 21.6 DNN LM : h=500 , d=60 109.3 21.5 with 1 layer ( NNLM ) with 3 layers 105.0 21.3 DNN LM : h=500 , d=120 104.0 21.2 with 1 layer ( NNLM ) with 3 layers 102.8 20.8 Model M ( TARGETCIT 99.1 20.8 RNN LM ( h=200 ) 99.8 - RNN LM ( h=500 ) 83.5 - Table 1 shows that DNN LMs yield gains on top of NNLM . However , we need to compare deep networks with shallow networks ( i.e . NNLM ) with the same n',\n",
       " 'elop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words . ^ S = arg max ( | ) arg max ( | ) ( ) P S C = P C S P S S S Recently , TARGETCIT model text message normalization as translation from the texting language into the standard language . model the word-level text generation process for SMS messages , by considering graphemic/p',\n",
       " 'r the hierarchical setups ( HPBT ) described in this paper , the open source Jane toolkit ( ) is employed . Jane has been developed at RWTH and implements the hierarchical approach as introduced by TARGETCIT with some state-of-theart extensions . In hierarchical phrase-based translation , a weighted synchronous context-free grammar is induced from parallel text . In addition to contiguous lexical phrases , hi',\n",
       " 'er we focus on a self-training style bootstrapping algorithm , the Yarowsky algorithm ( ) . Variants of this algorithm have been formalized as optimizing an objective function in previous work by TARGETCIT and , but it is not clear that any perform as well as the Yarowsky algorithm itself . We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds o',\n",
       " 're of words . Even more importantly , many of the models are evaluated on their ability to disambiguate the meaning of specific words , following an idea first introduced by and later adopted by TARGETCIT and others . For example , in this latter work the au115 thors test their multiplicative and additive models as follows : given an ambiguous intransitive verb , say ‘ run ’ ( with the two senses to be those',\n",
       " 'yms , etc . found in the realworld text were converted into standard dictionary words , so that the system can pronounce them correctly . Spell checking plays an important role in this process . ( ; TARGETCIT proposed to use the noisy channel framework to generate a list of corrections for any misspelled word , ranked by the corresponding posterior probabilities . ( ) enhanced this framework by calcul',\n",
       " '— whether these words are adjacent — is , in this case , hardly of any use ( see Table 7b ) . Conditioning on crossing punctuation could be of help then , playing a role similar to that of comma-counting ( TARGETCIT , §2.1 ) — and “ verb intervening ” ( , §5.1 ) — in early head-outward models for supervised parsing . Attached not Attached 337 7,645 2,144 4,040 2,481 11,685 2,478 11,673 3 12 b ) ry ≈ +0.00 Table',\n",
       " 'gnment deadline . 3 The Alignment Challenge The first challenge was word alignment : given a parallel text , students were challenged to produce wordto-word alignments with low alignment error rate ( AER ; TARGETCIT . This is a variant of a classic assignment not just in MT , but in NLP generally . describes a version of it , and we know several other instructors who use it.7 In most of these , the object is',\n",
       " 'acking literature has been given to generative Bayesian network models ( ) . Few trackers have been published using discriminative classifiers , a notable exception being . An analysis by TARGETCIT b ) demonstrates how such generative models can in fact degrade belief tracking performance relative to a simple baseline . The successful use of discriminative models for belief tracking has recently b',\n",
       " 'on top of conventional n-gram back-off language models ( BOLMs ) , have been introduced in ( ) as a potential means to improve discrete language models ( LMs ) . As for our last year participation ( TARGETCIT c ) , we take advantage of the recent proposal of . Using a specific neural network architecture ( the Structured OUtput Layer or SOUL model ) , it becomes possible to estimate n-gram models that u',\n",
       " 'ary rules ( ) . All rules over scope 3 are pruned ( ) . A set of nine standard features was used for the experiments , which includes globally normalized count of rules , lexical weighting ( TARGETCIT , and length penalty . Our in-house decoder was used for experiments with a trigram language model . The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning ( ) . We',\n",
       " 'surmounted for these systems to enjoy wide adoption . All approaches above are based on integer linear programming ( ILP ) , suffering from slow runtimes , when compared to extractive systems . For example , TARGETCIT report 55 seconds on average to produce a summary ; report substantially faster runtimes , but fewer compressions are allowed . Having a compressive summarizer which is both fast and expressive',\n",
       " 'or evaluation , the process of their creation , the participating systems ( 10 teams , 92 runs ) , the approaches adopted and the results achieved . 1 Introduction The cross-lingual textual entailment task ( TARGETCIT addresses textual entailment ( TE ) recognition ( ) under the new dimension of cross-linguality , and within the new challenging application scenario of content synchronization . Cross-linguality r',\n",
       " 'm text has also been considered in a number of earlier studies , these have primarily involved single PTM reactions extracted with special-purpose methods ( ) . The EPI task and associated work ( TARGETCIT were the first to target numerous PTM reactions in a general framework using retrainable extraction methods . The automatic detection of modification statements using keyword matching-based methods has',\n",
       " 'm solution in all but one case.19 5 Related work Since automatic QSD is in general challenging , traditionally quantifier scoping is left underspecified in deep linguistic processing systems ( ; TARGETCIT . Some efforts have been made to move underspecification frameworks towards weighted constraint-based graphs in order to produce the most preferred reading ( ) , but the source of these types of',\n",
       " 'd model of discourse . This work used HMMs to model the progression of sentences in articles , and was shown to be useful for ordering sentences and generating summaries of news articles . More recently , TARGETCIT b ) experimented with similar tasks using a related HMMbased model called the Structural Topic Model . Unsupervised HMMs were applied to conversational data by who experimented with Twitter con',\n",
       " 'over the unsupervised baseline and 3.0 % over the best projection baseline on average . 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing ( ; TARGETCIT and dependency parsing ( McOTHERCIT ; McOTHERCIT ) . For supervised models , the human-annotated corpora on which models are trained , however , are expensive and difficult to build . As alternative strategie',\n",
       " 'on second language acquisition , such as Lexical Density and Type-Token Ratio . Word frequency and its derivations , such as proportion of rare words , are utilized in many models of complexity ( ; TARGETCIT . Inspired by psycholinguistic research , two systems have explicitly set to measure textual cohesion for estimations of readability and complexity : Coh-Metrix ( ) and SourceRater ( ) . On',\n",
       " 'etection is to use a labelled lexicon to score sentences ( Hatzivassiloglou and McOTHERCIT ) . However , such approaches o n p 599 have been found to be highly topic dependent ( Engstr¨om , 2004 ; ) . TARGETCIT worked on a 2,829 sentence citation corpus using a 12-class classification scheme . Although they used context in their annotation , their focus was on determining the author ’ s reason for citing a given',\n",
       " 'differences . For evaluation , we used the mteval-v11b.pl script to compute lowercased BLEU-4 scores ( ) . Statistical significance was measured using an Approximate Randomization test ( ; TARGETCIT . All experiments for training on dev sets were carried out on a single computer . For grammar extraction and training of the full data set we used a 30 node hadoop Map/Reduce cluster that can handle 3',\n",
       " 'and Daelemans 2010 ; Velldal , Øvrelid , and Oepen 2010 ) , a tag sequence grammar ( RASP ) ( ) , as well as constituent analysis in combination with dependency triplets ( Stanford lexicalized parser ) ( TARGETCIT . The majority of systems perform classification at the token level , using some variant of machine learning with a BIO classification scheme and a post-processing step to assemble the full scope ( OTHE',\n",
       " 'ine . 2 Previous Work Prior work suggests that a wide variety of monolingual signals , including distributional , temporal , topic , and string similarity , may inform bilingual lexicon induction ( ; TARGETCIT ; ) . use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations . use an unsupervised rank-combination method for comb',\n",
       " 'as methods to mitigate any ill effects – has not been studied . Motivated by this , we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of TARGETCIT . We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McOTHERCIT . 2 Structured Perceptron for Inexact Hypergraph Search The structured perceptro',\n",
       " 'ed with these phrasal verbs and concentrate on disambiguation of phrasal 2It is written in the Preface of that dictionary . verb semantics , such as the investigation of the most common particle up by ( TARGETCIT . Research on token identification of phrasal verbs is much less compared to the extraction . ( ) describes a regular expression based simple system . Regular expression based method requires hum',\n",
       " 'uct with the impaired user . Such helpers usually know the impaired user very well and can often make reasonable guesses as to the whole utterance intended by the impaired user . Recent work reported in TARGETCIT suggests one way in which the results of a language modeling system and those of a human coconstructor may be integrated into a single system , and such an approach could easily be applied here . 7 Conc',\n",
       " 'AP does a slightly better job parsing sentences . The upper-bound parsing accuracy shows that we do not lose too much information while jointly detecting disfluencies . Our parser is not comparable to ( TARGETCIT and ( ) , since we use dependency relations for evaluation instead of constituencies . Disfluency Detection Evaluation We evaluate our model on detecting edited words in the sentences 7The parser',\n",
       " 'lustering provided by mkcls ( ) . Our use of words at the corners of phrases ( as opposed to the syntactic head , or the last aligned word ) follows , while our use of word clusters follows TARGETCIT . Each feature has the orientation o appended onto it . To help scale and to encourage smoothing , we only allow features that occur in at least 5 phrase pair 5Preliminary experiments indicated that the',\n",
       " 'important differences which relate to orthography in DA , e.g. , the MSA consonant H~ /B/ is pronounced as /t/ in ARZ ( or /s/ in more recent borrowings from MSA ) ; for a fuller discussion , see ( ; TARGETCIT a ) . Examples of morphological differences include changes in the 1Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme ( ) : ( in alphabetical order ) Abt0jHxdðrzsšSDT ˇDS-yfqk',\n",
       " 'gth statistics for the PWKP dataset ( ) . Statistical machine translation ( SMT ) has already been successfully applied to the related task of paraphrasing ( ; Bannard and CallisonOTHERCIT ; TARGETCIT ; ) . SMT typically makes use of large parallel corpora to train a model on . These corpora need to be aligned at the sentence level . Large parallel corpora , such as the multilingual proceedings',\n",
       " 'Tailoring the Translation Model Reverse self-training is a trick that allows to improve the translation model using ( target-side ) monolingual data and can lead to a performance improvement ( ; TARGETCIT . In our scenario , we translated the selected sentences ( in the opposite direction , i.e . from the target into the source language ) . Then we created a new translation model ( in the original direction )',\n",
       " 'how each character works in a literary text . Our paper uses an annotation scheme that is welldefined and has been used in previous computational models that extract social events from news articles ( TARGETCIT . This computational model may be adapted to extract these events from literary texts . However , the focus of this paper is not to adapt the previously proposed computational model to a new domain or g',\n",
       " 'rds . The lexical smoothing scores are computed from IBM-1 probabilities . We run MERT on the development set ( dev ) and evaluate on the test set ( test ) . A second baseline is the technique described in ( TARGETCIT , which we denote as leave-oneout . It is initialized with the heuristically extracted table and run for one iteration , which the authors have shown to be sufficient . Length-incremental training is per',\n",
       " 'al word usage across genders in email . There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade , including , more recently , in microblogs ( ; TARGETCIT ; ) . In spite of the surge of research in both sentiment and social media , only a limited amount of work focusing on gender identification has looked at differences in subjective language acro',\n",
       " 'by mapping the synsets to senses in a more coarse-grained dictionary . A manual , more coarse-grained grouping of WordNet senses has been performed in OntoNotes ( ) . The OntoNotes 90 % solution ( TARGETCIT actually means such a degree of granularity that enables a 90- % -IAA . OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora , caused by the high granularity of senses . The q',\n",
       " 'ally define a normalization task for Twitter , focusing on normalizations between single tokens , and excluding multi-word tokens like tot ( laugh out loud ) . The normalization task has been criticized by TARGETCIT b ) , who argues that it strips away important social meanings . In recent work , normalization has been shown to yield improvements for part-of-speech tagging ( ) , parsing ( ) , and machine',\n",
       " 'raction have been made over the last decade , in particular as the result of standardization efforts such as TimeML ( ) and TimeBank ( ) , as well as dedicated evaluation tasks ( ACE , 2005 ; TARGETCIT ; ) . However , these two lines of research have largely remained isolated from one another . In this paper we bridge this gap and address the task of recognizing stories discussing identical eve',\n",
       " 'e similar if they contain many common substructures , consisting of nodes with identical labels ( ) . proposed a partial tree kernel that adds flexibility in matching tree substructures . TARGETCIT introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels , albeit with a different focus than ours and would not match words with different POS . This',\n",
       " 'annual DUC2 and TAC3 evaluations . In contrast , the Twitter messages ( a.k.a. , tweets ) are very short and noisy , containing nonstandard terms such as abbreviations , acronyms , emoticons , etc . ( ; TARGETCIT . The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser ( ) , limiting the possibility of applying finer-grained event analysis tools . I',\n",
       " 'Hours to Improve . Train/Align Baseline 32.8 +0.0 5 Viterbi ITG 33.5 +0.7 831 BP Relaxed 33.6 +0.8 39 Table 2 : Machine translation results . 7.3 Translation We ran translation experiments using Moses ( TARGETCIT , which we trained on a 22.1 million word parallel corpus from the GALE program . We compared alignments generated by the baseline HMM model , the Viterbi ITG model and the Relaxed BP model.12 The syste',\n",
       " 'orkshops , WSD systems achieve promising performance . In the application of WSD to MT , research has shown that integrating WSD in appropriate ways significantly improves the performance of MT systems ( TARGETCIT ; ) . In the application to IR , WSD can bring two kinds of benefits . First , queries may contain ambiguous words ( terms ) , which have multiple meanings . The ambiguities of these query words can h',\n",
       " \"nce . MST uses Chu-Liu-Edmonds Maximum Spanning Tree algorithm for non-projective parsing and Eisner 's algorithm for projective parsing . It uses online large margin learning as the learning algorithm ( TARGETCIT a ) . 4.3 Our Approach McOTHERCIT compared the accuracy of MSTParser and MaltParser along a number of structural and linguistic dimensions . They observed that , though the two parsers exhibit indistingui\",\n",
       " 'in an ILP . However , the inference models in their work were not a joint model involving multiple local classifiers but only one local classifier was involved in their objective functions . The work of TARGETCIT did formulate a joint inference model with Markov Logic Network ( MLN ) . They , however , used the same setting as the TempEval challenges , thus only pairs of temporal entities in the same or adjacent sen',\n",
       " 'DMV baseline is still state-of-theart for one language ; and the remaining 10 bests are split among five other recent systems ( see Table 8 ) .11 Half of the five came from various lateen EM strategies ( TARGETCIT a ) for escaping and/or avoiding local optima . These heuristics are compatible with how we trained our DBMs and could potentially provide further improvement to accuracies . Overall , the final scores of',\n",
       " '{ DEG , DEC } , which results in the overall improved performance . In contrast , the joint method performs worse on ambiguous POS pairs such as { NN , NR } . This observation is similar to those reported by ( TARGETCIT ; ) . 5 Conclusion In this paper , we proposed a new algorithm for joint Chinese word segmentation , POS tagging , and parsing . Our algorithm is an extension of the CYK 509 error pattern # ↓ error',\n",
       " 'd by equations ( 2 ) and ( 3 ) , although this approach is closely related to that framework . This approach is related to transition-based dependency parsing such as ( ) or dependency tree revision ( TARGETCIT . 3 Training 3.1 Dataset preparation Following ( ; Visweswariah et.al. , 2011 ) , we generate a source-side reference reordering of a parallel training corpus . For each sentence pair , we generate',\n",
       " 'se disentanglement is a necessary pre-requisite to making sense of narrative texts , an interesting direction in NLP that has received an increasing amount of attention ( ; Elson and McOTHERCIT ; TARGETCIT . Recognizing the ( main ) narrative threads comprising a work provides a context for interpreting the text . Disentanglement may thus be viewed as the first step in a literary processing ‘ pipeline ’ . Ide',\n",
       " 'y of studies into normalizing various types of entities for formally written texts . For instance , normalizes gene/protein names using dictionaries automatically extracted from gene databases ; TARGETCIT address cross-document Arabic name normalization using a machine learning approach , a dictionary of person names and frequency information for names in a collection ; demostrates a largescale',\n",
       " 'ilar to that outlined in . We first extract a joint phrase-pair distribution p ( e , f ) from the development set using standard techniques ( HMM word alignment with grow-diag-and symmeterization ( TARGETCIT ) . We then find the set of weights λ� that minimize the cross-entropy of the mixture p ( e |f ) with respect to ,5 ( e , f ) : For efficiency and stability , we use the EM algorithm to find �λ , rather than L-B',\n",
       " 'm width ( s = 8 ) . In particular , we achieve 86.33 % uas on CTB which is 1.54 % uas improvement over the greedy baseline parser . Moreover , the performance is better than the best transition-based parser ( TARGETCIT which adopts a much larger beam width ( s = 64 ) . 6 Conclusion and related work This work directly extends ( ) with beam search and global learning . We show that both the easy-first POS tagger an',\n",
       " 'been gaining interest from academia and industry alike for the past decade , which resulted in a multitude of UIMA-supporting repositories of analytics . Notable examples include METANET4U components ( TARGETCIT featured in U-Compare1 , DKPro ( ) , cTAKES ( ) , BioNLP-UIMA Component Repository ( ) , and JULIE Lab ’ s UIMA Component Repository ( JCoRe ) ( ) . However , despite conforming to t',\n",
       " 'ic model for better results . Finally , recent work has shown the importance of performing coreference resolution jointly for all mentions in a document ( McOTHERCIT ; Daum´e III and Marcu 2005 ; ; TARGETCIT ; ) rather than the classic method of simply aggregrating local decisions about pairs of mentions . Like these systems , our model adopts the entity-mention model ( Morton 909 Computational Lingu',\n",
       " 'ost frequent lexical predicates of extracted by SPred from Wikipedia . 6 Related work The availability of Web-scale corpora has led to the production of large resources of relations ( ; TARGETCIT ; ) . However , these resources often operate purely at the lexical level , providing no information on the semantics of their arguments or relations . Several studies have examined adding semanti',\n",
       " 'in natural language , we use the FUF/SURGE surface realizer ( ) , which offers the richest knowledge of English syntax and widest coverage among the publicly available realizers such as REALPRO ( TARGETCIT . The realization of the sentence-sized units requires referring expressions 559 Computational Linguistics Volume 38 , Number 3 for certain graphical elements , however . Our system handles three differe',\n",
       " 'deling a corpus with n-gram counts n-gram backoff language models have been used for decades in automatic speech recognition and statistical machine translation . We follow the usual FSA construction ( TARGETCIT . The state of a 5- gram FSA model c ( w ) must remember the previous 4-gram . For example , it would include an arc from state defg ( the previous 4-gram ) to state efgh with label h and weight c ( h |defg ) .',\n",
       " '( a closely related task of RE ) , there have been efforts in BioNLP shared tasks of 2009 and 2011 for ( non-mandatory sub-task of ) event negation detection ( 3 participants in 2009 ; 2 in 2011 ) ( ; TARGETCIT . The participants approached the sub-task using either pre-defined patterns or some heuristics . 2This task is popularized by various recently held shared tasks ( ) . negation scopes , it is not',\n",
       " 'sets . English sentences are tagged by the implementations of the POS tagger of , which is trained on WSJ . The source sentences are then parsed by an implementation of 2nd-ordered MST model of TARGETCIT , which is trained on dependency trees extracted from Penn Treebank . As the evaluation metric , we use parsing accuracy which is the percentage of the words which have found their correct parents . We e',\n",
       " 'oth precision and recall . • Careful normalization of the reference and machine translations , including lowercasing and punctuation-stripping . 12This ranking has been disputed over a series of papers ( TARGETCIT ; ) . The paper which initiated the dispute , written by the first author , was directly inspired by the experience of designing this assignment . Spearman ’ s ρ 0.8 0.6 0.4 Figure 3 : Submission his',\n",
       " 'ation of concepts . But how can we quantify the usefulness of these topics with respect to an IR system ? Recently , researchers developed measures which evaluate the semantic coherence of topic models ( TARGETCIT ; ) . We adopt their view of semantic coherence and apply one of these measures to query-oriented topics . Several studies concentrated on improving the quality of document ranking using topic m',\n",
       " 'here has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction ( e.g. , ) , parameter estimation ( e.g. , ) , or both ( e.g. , TARGETCIT ; ) . The focus of this paper is on the parameter estimation phase . We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov rand',\n",
       " 'nerate candidates of opinion expressions and opinion targets first , and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets ( ; TARGETCIT ; ) . In addition to pipeline approaches , bootstrapping-based approaches were proposed ( ) to identify opinion expressions and targets iteratively ; however , they suffer from the problem',\n",
       " 'g algorithm that uses global context vectors as a basis for clustering . described a graph-based clustering methods for word classes . used Bayesian reasoning for word class induction . TARGETCIT described a method for determining bilingual word classes , used to improve the extraction of alignment templates through alignments between classes , not only between words . He also described a monolin',\n",
       " 'guages is becoming increasingly important . This need can be addressed in part by cross-lingual information access tasks such as entity linking ( McOTHERCIT ) , event extraction ( ) , slot filling ( TARGETCIT and question answering ( ; Parton and McOTHERCIT ) . A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation ( MT ) . Traditional MT approache',\n",
       " 'n the line of or who generate contextually appropriate REs for instances of a referent in a text . ’ s GREC data set includes annotations of implicit subjects in coordinations . TARGETCIT deal with implicit subjects in passives , proposing a set of heuristics for adding these agents to the generation input . acquire automatic annotations of implicit roles for the purpose of stud',\n",
       " 'sting hypothesis prune stack if too big 5.1 Data We chose the English-to-German translation systems from the 2009 and 2011 shared task at the annual Workshop for Machine Translation ( CallisonOTHERCIT ; TARGETCIT , providing the first as development data and the second as test data . We chose these sets because BLEU ( ) , our baseline metric , performed particularly poorly on them ; this left room for impro',\n",
       " 'ework improves the quality of process extraction significantly . 3 Joint Model for Process Extraction Given a paragraph x and a trigger set T , we wish to extract all event-event relations E. Similar to TARGETCIT , our model consists of a local pairwise classifier and global constraints . We first introduce a classifier that is based on features from previous work . Next , we describe novel features specific for',\n",
       " 's ( say , gene expression measurements ) . Distant Supervision In Distant Supervision ( DS ) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text ( ; TARGETCIT ; ) , and this alignment is then used to train a relation extractor . A core difference to our approach is the number of target relations : In DS it is the relatively small schema size of the kno',\n",
       " 'riminate ’ between the set of all candidate parses . A widely-used method to achieve this is outlined in . We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit ( TARGETCIT , and learn a maximum entropy model . This method is used by and MacOTHERCIT inter alia . One important implementation detail is that rather than exhaustively ranking all candidates out of poss',\n",
       " 'thm . Let each translation candidate be represented by a feature vector x E IRD where preference pairs for training are prepared by sorting translations according to smoothed sentence-wise BLEU score ( TARGETCIT a ) against the reference . For a preference pair xj = ( x ( 1 ) j , xj 2 ) ) where x ( 1 ) j is preferred over x ( 2 ) j , and ¯xj = x ( 1 ) j − x ( 2 ) j , we consider the following hinge loss-type objective function :',\n",
       " 'ed patterns ( e.g . CONTAINER-FORCONTENT ) . If this fails , the system searches the knowledge base for a relevant analogy in order to discriminate metaphorical relations from anomalous ones . The system of TARGETCIT uses WordNet ( the hyponymy relation ) and word bigram counts to predict verbal , nominal and adjectival metaphors at the sentence level . The authors discriminate between conventional metaphors ( included',\n",
       " 'ny senses will share the same latent semantics profile , as long as they are in the same topic/domain . To solve the sparsity issue we use missing words as negative evidence of latent semantics , as in ( TARGETCIT . We define missing words of a sense definition as the whole vocabulary in a corpus minus the observed words in the sense definition . Since observed words in definitions are too few to reveal the sema',\n",
       " '( ) . In this paper , we also take this stance and present the first computational method that identifies metaphorical expressions in unrestricted text by means of their interpretation . Following TARGETCIT , we define metaphor interpretation as a task of finding a literal paraphrase for a metaphorically used word and introduce the concept of symmetric reverse paraphrasing as a criterion for metaphor ide',\n",
       " 'ference to the output of the joint learning system to account for dependencies not covered by the joint learning model . We report F1 performance scored using the official scorer from the shared task ( TARGETCIT . The task reports two types of evaluation : on the original gold data and on gold data with additional corrections . We refer to the results as Original and Revised . 6.1 Joint Inference Results Table 7',\n",
       " 'ord , +ASIB , +HB 93.28 1,018 PTB-YM §22 , 3rd ord 93.29 709 PTB-YM §22 , 3rd ord , gold tags 94.01 722 This work ( PTB-YM §23 , 3rd ord ) 93.07 735 92.46 112† 92.1– 587† 92.9– 680† TARGETCIT 92.53 66† Zhang and McOTHERCIT 93.06 220 This work ( PTB-S §23 , 3rd ord ) 92.82 604 92.7– 4,460 Table 2 : Results for the projective English dataset . We report unlabeled attachment scores ( UAS )',\n",
       " 'ed meaning can be that John began reading or writing a book . An enumerative approach may postulate another sense for begin . However , an economical proposal in the framework of the Generative Lexicon ( TARGETCIT is the complement coercion operation , which leaves the meaning of the verb intact in different contexts by shifting the semantic type of its complement . In the above example , a book is shifted from an',\n",
       " 'm. At each step ( source to pivot and pivot to target ) , multiple translation outputs will be generated , thus a minimum Bayesrisk system combination method is often used to select the optimal sentence ( TARGETCIT ; ) . A problem with the transfer method is that it needs to decode twice . On one hand , the time cost is doubled ; on the other hand , the translation error of the source-pivot translation system',\n",
       " 'cluster has more than four sub classes . Figure 1 . Russell ’ s circumplex model of 28 affects words To the best of our knowledge , no work has been carried out on Hindi music mood classification . However , TARGETCIT had worked on the preparation of data for Hindustani classical music mood classification . They have performed several sessions for classifying the three Indian Ragas into 13 mood classes . 3 Mood Taxon',\n",
       " 's the base and the update collections at the same time . A second parameter is the size of the n-grams for representing the documents . The original implementations of SUMBASIC ( ) and TOPICSUM ( TARGETCIT were defined over sin219 gle words ( unigrams ) . Still , report some improvements in the ROUGE-2 score when representing words as a bag of bigrams , and mention similar improvements when',\n",
       " '2 summarizes our results . As can be seen , inclusion of lexical features gives our decoder an absolute increase of 6.73 % in BLEU over the 1-BEST system . It also outperforms the discriminative system of TARGETCIT . Our lexical features seem more robust compared to their templates . This is especially the case with infrequent records , where their system struggles to learn any meaningful information . Addition of',\n",
       " 'he task at hand . This has been done for language models using techniques from information retrieval ( ) , or perplexity ( ) . Data selection has also been proposed for translation models ( TARGETCIT . Note that for translation models , data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem , and that the optimal amount of data to select is hard to dete',\n",
       " 'e select the McClosky-Charniak-Johnson Parser2 for syntactic analyses . That parser is improved from the Stanford parser with a selftrained biomedical model . According to the shared task ’ s statistics ( TARGETCIT , it is used by groups achieving high results . In addition , the NLP data of all datasets are prepard and provided for participants . We read and convert the given results into our framework to use in f',\n",
       " 'Kneser-Ney smoothing . Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ ( ) . The A weights for these features are learned using the batch lattice-MIRA algorithm ( TARGETCIT to optimize BLEU-4 ( ) on a tuning set . We use PORTAGE , our internal PBSMT decoder for all experiments . PORTAGE uses a standard phrasal beam-search algorithm with cube pruning . The main differe',\n",
       " 'different feature types ( i.e . the positional word and dependency features ) . 3.3 Comparison with other Topic Modelling Approaches to WSI The idea of applying topic modelling to WSI is not entirely new . TARGETCIT proposed an LDA-based model which assigns different weights to different feature sets ( e.g . unigram tokens vs. dependency relations ) , using a “ layered ” feature representation . They carry out extensive',\n",
       " 'p ( RHSs|RHSt , LHS ) , the noisy-channel translation probability . • p ( LHS , RHSt|RHSs ) , the direct translation probability . • plex ( RHSt|RHSs ) and plex ( RHSs|RHSt ) , the direct and indirect lexical weights ( TARGETCIT . • ppefg ( FRAGt ) , the monolingual PCFG probability of the tree fragment from which the rule was extracted ( GHKM and target-annotated systems only ) . This is defined as Hni=1 p ( ri ) , where r1 ... rn are',\n",
       " 'then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 ( ) to obtain one-to-many alignments in both directions and symmetrized sing the grow-diag-final-and method ( TARGETCIT . 3We approximate corpus BLEU by scoring sentences using a pseudo-document of previous 1-best translations ( ) . We constructed a 5-gram language model using SRILM ( ) from the provided E',\n",
       " 'pproach in a few important algorithmic details , as will be discussed in section 5 . When applied to unsupervised grammar learning , DA has been shown to lead to worse parsing accuracy than standard EM ( TARGETCIT ; in contrast , we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning . The rest of the paper is organized as follows . Sec',\n",
       " 'e seen ILP applied to many structured NLP applications including dependency parsing ( ) , text alignment ( DeOTHERCIT ) and many previous approaches to sentence and document compression ( ; TARGETCIT . 2.2.1 Basic structural constraints We start with constraints that define the behavior of terminal tokens . Let y * jk , yij * and z * j denote indicator variables for the sentence-starting trigram ( START ,',\n",
       " 'ng model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model . TARGETCIT took the source parse tree into account and added soft constraints to hierarchical phrase-based model . used dependency tree to add syntactic cohesion . These methods work with the original SCF',\n",
       " 'ntences are further fed to the guided compression module that produces n-best compressions for each 8http : //www.nist.gov/tac/data/index.html System R-2 R-SU4 CompR TAC ’ 08 Best System 11.03 13.96 n/a ( TARGETCIT 11.70 14.38 n/a ( ) 11.37 14.47 n/a Our System 12.35† 15.27† 43.06 % Our System w/o Pre-selection 12.02 14.98 55.69 % Our System w/ Generic Comp 10.88 13.79 30.90 % Table 3 : Results on the TAC 200',\n",
       " 'performance . , “ We feel that there is a sense of increasing confusion down this line of research ” . To ease comparison for future research we use the same 5-fold split on the document level as TARGETCIT 13 and make our system publicly available ( see Section 5 ) . Table 3 shows that our system ( bottom ) aligns well with the state of the art . Our best system ( composite kernel with polynomial expansion ) re',\n",
       " 'arsing pipeline , we first split sentences using the JULIE Sentence Boundary Detector , or JSBD ( ) . We then parse using a version of clearnlp1 ( Choi and McOTHERCIT ) , a successor to ClearParser ( TARGETCIT , which was shown to have stateof-the-art performance over the CRAFT corpus of full-text biomedical articles ( ) . We use dependency and POS-tagging models trained on the CRAFT corpus ( except wh',\n",
       " 'd derivations from which a wide-coverage CCG lexicon was extracted . CCGbank has been used for the development of wide-coverage CCG parsers ( ) . The same methodology has been applied to German ( TARGETCIT , Italian ( ) , and Turkish ( C¸akıcı , 2005 ) . Their treebanks are annotated with dependencies of words , the conversion of which into phrase structures is not a big concern . A notable contribution',\n",
       " 'dnets ( ) and thus improve the internal connectivity of the individual wordnets , as well as by the fact that the derivational subnets reflect certain cognitive structures in natural languages ( TARGETCIT . n approach to wordnet development based on enrichment with morphosemantic relations has been adopted for English ( ) , as well as for a number of other languages – Turkish ( ) , Czech ( O',\n",
       " 'process . The English-Manipuri parallel corpus developed by ( ) is used in the experiment . Moses4 toolkit ( ) is used for training with GIZA++5 and decoding . Minimum error rate training ( TARGETCIT for tuning are carried out using the development data for two scripts . Table 3 gives the corpus statistics of the English-Manipuri SMT system development . 4.1 Lexical Ambiguity Manipuri is , by large ,',\n",
       " 's for a given stack that are reachable from the hypotheses in previous stacks . However only a few of these hypotheses are retained , while majority of them are pruned away . The cube pruning technique ( TARGETCIT avoids the wasteful generation of poor hypotheses that are likely to be pruned away by efficiently restricting the generation to only high scoring partial translations . We modify the cube pruning for',\n",
       " 'ring similarity , are informative ( ; Daumé and ) . This thesis builds upon this work and uses a diverse set of signals for translating full sentences , not just words . Recently , , TARGETCIT , and have worked toward learning a phrase-based translation model from monolingual corpora , relying on decipherment techniques . In contrast to that research thread , we make the realistic ass',\n",
       " 'ossessive pronouns.4 Most of the determiner errors , however , involve omitting an article ( these make up over 60 % in the training data ) . Similar error patterns have been observed in other ESL corpora ( TARGETCIT a ) . Our system focuses on article errors . The system first extracts from the data all articles , and all spaces at the beginning of a noun phrase where an article is likely to be omitted ( ) . Th',\n",
       " 'lemented with weighted voting of 25 nearest neighbours based on distance and Support Vector Machine is implemented with linear kernel and default parameters . These methods are used with Python , NLTK ( TARGETCIT and Sci-Kit ( ) . Training data contains 872 articles labelled and divided into four categories as follows : 235 articles on politics , 258 articles about social news such as culture , education or',\n",
       " 'with examples of when they should be hyphenated and when they should remain unhyphenated . The second data source for training the model comes from pairs of revisions from Wikipedia articles . Following TARGETCIT , we automatically extract a corpus of error annotations for miss2LDC catalog number LDC93T3A . Tokens Stems Tags Bigrams Dict Prob Distance Verb/Noun 301 Context Hyphenated Unhyphenated NN NN terrific',\n",
       " 'y sensitive to the proverbial “ bum steer ” from unrepresentative short fragments , pace . 8.4 Miscellaneous Systems on Short Sentences Several recent systems ( ; Søgaard , 2011b ; ; TARGETCIT , inter alia ) are absent from Table 8 because they do not report performance for all sentence lengths . To facilitate comparison with this body of important previous work , we also tabulated final accur',\n",
       " 'lish and slight improvement of BLEU in the case of the former . 11We used news-dev2009a as dev and news-dev2009b as devtest and tuned the weights with Z-MERT ( ) . 12We use bootstrap resampling ( TARGETCIT b ) to test our results against the baseline result . 7 System German French Acc . BLEU Acc . BLEU Baseline System cept.stack-size cept.50 25.95 % 19.50 42.10 % 21.44 cept.100 30.04 % 19.79 47.32 % 21.70 cept',\n",
       " 'the cased taggers . However , transferring information from the snippets provides additional benefits , significantly improving even the uncased baseline taggers . This is consistent with the analysis in TARGETCIT . Finally , we see that the direct transfer method from Section 2 significantly outperforms the method described in . Table 3 confirms this trend when focusing on proper nouns , which are partic',\n",
       " 'they defined was for the bootstrapping process while they show how accurately they could predict the goodness of seeds . Early work on bootstrapping includes that of ( ) and that of ( ) . TARGETCIT extended self-training algorithms including that of ( ) , forming a theory different from that of ( ) . We chose to extend the theory of ( ) because it can actually explain recent g',\n",
       " 'y works try to improve rule probability estimation by using context-dependent probabilities in PCFG model , and show that rules with dependent context features perform better than PCFG alone ( ; TARGETCIT ; ) . presented a maximum-entropy-inspired model to estimate probabilities in Markov grammar . The model uses a standard bottom-up best-first probabilistic chart parser to generate poss',\n",
       " 'r the log-linear model . In this paper , we apply our AdNN model to hierarchical phrase based translation , and it can be similarly applied to phrase-based or syntax-based translation . Similar to Hiero ( TARGETCIT , the feature vector h in Eq . ( 5 ) includes 8 default features , which consist of translation probabilities , lexical translation probabilities , word penalty , glue rule penalty , synchronous rule penalty',\n",
       " 'g expressions for the first time in the ASGRE Challenge 2007 ( ) . This entry is described in detail in ( ) and was very successful as well in the following 2008 and 2009 REG Challenges ( TARGETCIT ; ) with a free-na¨ave cost function . This cost function assigns 0 cost to the most common attributes , 2 to the rarest , and 1 to all others . By making the most common attributes free , it becam',\n",
       " 're sophisticated statistical language model trained on a collection of derivations to identify the most probable derivation and thus the presumably most fluent sentence from the set of possibilities ( TARGETCIT ; de Kok , Plank , and van Noord 2011 ; Zarrieß , Cahill , and Kuhn 2011 ) . We have assumed in our construction that terminals are morphologically unanalyzed , full-form words . A more modular arrangement is',\n",
       " 'ave been attempts to compose meanings for sentences and larger passages ( ) , but interest in compositional DSMs has skyrocketed in the last few years , particularly since the influential work of TARGETCIT ; 2009 ; 2010 ) . For the current study , we have reimplemented and adapted to the morphological setting all cDSMs we are aware of , excluding the tensorproduct-based models that have shown to be',\n",
       " '. As described in , continuous reordering rules are extracted . This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules ( TARGETCIT , for German↔English systems . 4.2 Tree-based Reordering Model In addition to the POS-based reordering , we apply a tree-based reordering model for the German↔English translation to better address the d',\n",
       " 'es have been proposed to estimate the error model , P ( t|s ) . For example , in work on spell-checking , improve on a standard edit-distance approach by considering multi-character edit operations ; TARGETCIT build on this by incorporating phonological information . utilise distributional similarity ( ) to correct misspelled search queries . In text message normalisation , model the l',\n",
       " 'del trained on the Penn Treebank . 4We found that using the 1-best worked just as well as the 1000-best on our grammaticality tasks , but significantly overestimated our model ’ s perplexities . 5We follow TARGETCIT in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical . 5.2 Perplexity Perplexity is the standard intrinsic evaluati',\n",
       " 'candidates for switch and mix . The basic idea of our approach is as follows 1 . Train Latin script POS tagger ( LS tagger ) on pure Urdu Latin script data ( Example 2 in table 1 – using Urdu POS tag set , TARGETCIT 2 . Train English POS tagger on English data ( based on English tag sets , ) 3 . Apply LS tagger and English tagger on Urdish data and note the confidence measures of the applied tags on each word',\n",
       " 'urrently , JPRO supports three classifiers : • Perceptron ( ) : the perceptron is self-contained in J-PRO , no external resources required . • MegaM ( Daum´e III and ) : the classifier used by TARGETCIT .2 • Maximum entropy classifier ( ) : the Stanford toolkit for maximum entropy classification.3 The user may specify which classifier he wants to use and the classifier-specific parameters in th',\n",
       " 'on to generate plots for tasks that do not have competitions with publicly available system outputs . The first task is English-French word alignment , where we use three base models : the ITG aligner of TARGETCIT , the joint HMM aligner of , and GIZA++ ( ) . The last two aligners are unsupervised , while the first is supervised . We train the unsupervised word aligners using the 1.1M sentence pair',\n",
       " 'e , tested against the 14 held-out sets from 2006/7 CoNLL shared tasks , and state-of-the-art results ( all sentence lengths ) for systems that : ( i ) are also POS-agnostic and monolingual , including SCAJ ( TARGETCIT a , Tables 5–6 ) and SAJ ( ) ; and ( ii ) rely on gold POS-tag identities to ( a ) discourage noun roots ( Mareˇcek and Zabokrtsk´y , 2011 , MZ ) , ( b ) encourage verbs ( , RF ) , or ( c ) transfer delex',\n",
       " 'ly from connectives . Rather , these approaches take as evidence , lexical and syntactic features of the arguments of the coherence relation , nearby coherence relations , high-level text structure , etc . ( TARGETCIT ; ) . As one might expect , automated approaches use simple features that can be computed reliably . However , performance in recognizing coherence relations in the absense of connectives is still',\n",
       " 'is 345 words . We used 25,000 scenarios from WEATHERGOV for training , 1,000 scenarios for development and 3,528 scenarios for testing . For the second domain ( henceforth WINHELP ) we used the dataset of TARGETCIT , which consists of 128 scenarios . These are articles from Microsoft ’ s Help and Support website4 and contain step-by-step instructions on how to perform tasks on the Windows 2000 operating system . In',\n",
       " 'ot have access to the exact partitions they have created for 5-fold cross-validation . As such , we have implemented a baseline adopting similar surface lexico-syntactic features used in previous work ( TARGETCIT ; ) , including 1 ) part-of-speech tags , 2 ) tenses , 3 ) dependency parses , 4 ) relative position of events in article , A s1 s2 B Class AFTER BEFORE OVERLAP # E-E pairs 3,588 ( 45 % ) 3,589 ( 45 % ) 815',\n",
       " ') is used to measure translation performance , and also the cost function in the max-margin estimation . Statistical significance in BLEU differences was tested by paired bootstrap re-sampling ( TARGETCIT . We used minimum error rate training ( MERT ) ( ) to optimize feature weights for the traditional log-linear model . We used the same decoder as the baseline system in all estimation methods . Wit',\n",
       " 'l and his stand-alone tokeniser in Section 6 . Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separated languages such as Chinese ( ) and Japanese ( TARGETCIT . While at a high level , this work is solving the same problem , the shape of the problems are quite different from a data point of view . Regular joint morphological analysis and segmentation has much',\n",
       " '2.3 Text input affordances Text input affordances — whether standard keyboards or predictive entry on mobile devices — play a role in computer-mediated communication that is perhaps under-appreciated . TARGETCIT b ) investigate orthographic variation on Twitter , and find differences across devices : for example , that messages from iPhones include more contractions than messages from Blackberries , and that tweet',\n",
       " 'Wikinews,7 a service where volunteers submit news articles interspersed with Wikipedia links . We leveraged said links to assemble 40k referring expression tasks . For algorithms , we employed , TARGETCIT and Full Brevity ( FB ) ( ) . Our results show that the first two algorithms produce results in a majority of the referring expression tasks , with the Dale and Reiter algorithm being the most effi',\n",
       " 's ( ... ) , generate their symbols in the reverse order in the target language . In the context of machine translation , ITG has been explored for statistical word alignment in both unsupervised ( ; TARGETCIT and supervised ( ) settings , and for decoding ( ) . Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment ( DeOTHERCIT ) . The work of DeOTHERC',\n",
       " 'ce & Technology ( NAIST ) While many alternatives have been proposed , such a perfect evaluation metric remains elusive . As a result , many MT evaluation campaigns now report multiple evaluation metrics ( TARGETCIT ; ) . Different evaluation metrics focus on different aspects of translation quality . For example , while BLEU ( ) focuses on word-based n-gram precision , METEOR ( ) allows for ste',\n",
       " 'is reported in ( ) ; an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration ( ) . Studies on metric tunability ( ; CallisonOTHERCIT ; TARGETCIT have found that the metric used for evaluation may not be the best metric used for tuning . For instance , ( ) report that tuning on linear combinations of BLEU-TER is more robust than a single m',\n",
       " 'ported that text features such as syllable counts of words , and sentence length were predictors of text difficulty . Newer research in this area has included increasingly more NLPbased investigations ( TARGETCIT ; ) . Some research examines text quality in terms of discourse coherence of well-formed texts ( ; Graesser , McNamara , & ) . Human evaluation of text complexity in curriculum mate',\n",
       " 'evaluation , where the parser ’ s output is used in a downstream task , such as machine translation ( ) , information extraction ( ) , textual entailment ( ) , or semantic dependencies ( TARGETCIT . While some of these approaches give a better sense of the impact of parse errors , they require integration into a larger system , making it less clear where a given error originates . The work we pres',\n",
       " 'ance , we include the SUPER relation that appears in temporal annotations such as the Timebank corpus ( ) and Allen ’ s work , but in practice was not considered by many temporal ordering systems ( TARGETCIT ; ) . Importantly , our relation set also includes the relations CAUSES and ENABLES , which are fundamental to modeling processes and go beyond simple temporal ordering . We also added event coref',\n",
       " 'IT ) or sentiment relations ( ) are inherently useful in data mining , information retrieval and other user-facing technologies . More fundamental structures such as part-of-speech tag sequences ( TARGETCIT or syntactic parse trees ( ; K¨ubler et al. , 2009 ) , on the other hand , comprise the core linguistic analysis for many important downstream tasks such as machine translation ( Chiang , * The major',\n",
       " 'for Arabic-English . MTA ( 4-refs ) and ISI ( 1-ref ) . We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO ( denoted P ) and a linear combination4 ( denoted L ) baseline . Similar to TARGETCIT , we use five different BLEU : RIBES weight settings , viz . ( 0.0,1.0 ) , ( 0.3 , 0.7 ) , ( 0.5 , 0.5 ) , ( 0.7 , 0.3 ) and ( 1.0 , 0.0 ) , marked L1 through L5 or P1 through P5 . The Pareto frontier is then computed from',\n",
       " '• Tag The tag of a given word • Word The tag of and first XP above a word • WProj The tag of and maximal projection of a word Heads is a first-order dependency feature . 3.2 Dependency Parsing Features TARGETCIT showed that chart-based dependency parsing , based on Eisner ’ s ( 1996 ) algorithm , could be successfully approached in a discriminative framework . In this earliest work , each feature function could only',\n",
       " 'ion , a set of automatically induced latent features can effectively work as a proxy for typology . 2 Related Work Traditionally , parallel corpora have been a mainstay of multilingual parsing ( ; TARGETCIT . However , recent work in multilingual parsing has demonstrated the feasibility of transfer in the absence of parallel data . As a main source of guidance , these methods rely on the commonalities in de',\n",
       " 'man translation and model coreference using a word dependency module integrated within the log-linear SMT model as an additional feature function . Annotation projection has been used elsewhere in SMT . TARGETCIT use it to capture long–distance phenomena within a single sentence in the source-language text via the extraction of sentence-level contextual features , which are used to augment SMT translation model',\n",
       " 'and form capture the same meaning ” . Paraphrase recognition is mostly understood as a binary classification process , although recently , some authors proposed a continuous semantic similarity measure ( TARGETCIT . Competing paraphrase recognition approaches are often compared by their performance on the Microsoft Research Paraphrase Corpus ( MSRPC ) . Until 2011 , simple features such as n-gram overlap , dependenc',\n",
       " 'in all word tasks , systems are expected to tag all content words in running text ( ) , while in lexical sample tasks , the evaluation considers a smaller predefined set of target words ( ; TARGETCIT . • language : English is by far the most studied language , but the disambiguation of words in other languages such as Chinese ( ) has been considered . • sense inventory : many tasks use WordNet',\n",
       " 'an NI is also taken into account . While Chen et al. ’ s system has a higher recall than VENSES++ , its performance is still relatively low . The authors argue that data sparseness is the biggest problem . TARGETCIT also used supervised machine learning to model NI resolution for the SemEval data . However , while Tonelli & Delmonte and Chen et al . view NI resolution as an extension of semantic role labelling , Silb',\n",
       " 'rformance , is known as NPhard ( ) . Linear Programming helps to find an accurate approximated solution to this problem and became very popular in summarization field in the last years ( ; TARGETCIT ; ) . However , most mentioned works use exponential number of constraints . Trying to solve a trade-off between summary quality and time complexity , we propose a novel summarization model solvin',\n",
       " 'aligned sentences . The first one is trained on a large-scale corpus gathered by asking users of Amazon ’ s Mechanical Turk Service ( ) to write a one-sentence description of a short video clip ( TARGETCIT . We combined a phrase table and distortion table extracted from this parallel corpus with the same Twitter language model , applying the Moses decoder to generate paraphrases . The additional noise rem',\n",
       " '7 , 2008 ) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality ( ; Giménez and Màrquez , 2007 ; TARGETCIT ; Giménez and Màrquez , 2008 ) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time . did tune on QUEEN , a simplified version of ULC that d',\n",
       " '406 positive and 343 negative . 2.2 Expanded Lexicon We used a pivot-based lexical and phrasal paraphrase generation system ( ) . The paraphraser implements the pivot-based method as described by TARGETCIT with several additional filtering mechanisms to increase the precision of the extracted pairs . The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora : We fi',\n",
       " 'rch from Section 3.3 ) . We set the number of projections k = 3000 for all three methods and for PLEB and FAST-PLEB , we set number of permutations p = 1000 as used in large-scale noun clustering work ( TARGETCIT . Evaluation Metric : We use two kinds of measures , recall and Pearson ’ s correlation to measure the overlap in the approximate and exact similarity lists . Intuitively , recall ( R ) captures the number of',\n",
       " 'f the inserted word , and its probability estimation according to a unigram language model , which yields lower costs for more frequent words . Given a ( T , H ) pair , the system applies a search algorithm ( TARGETCIT to find a proof O = ( o1 , o2 , ... on ) that transforms T into H. For each proof step oi the system calculates a cost c ( oi ) . This cost is defined as follows : the system uses a weightvector w , which is lear',\n",
       " 'iterature in natural language processing ( , Marcu 2000 , , inter alia ) as does automatic coreference resolution , which has significantly increased in accuracy in recent years ( , TARGETCIT , ) . We formulate and test two hypotheses in this position paper : First , we anticipate that given stylistic considerations and their fundamental narrative function , prose literary texts are in',\n",
       " 'ng a bit larger number of tags . Our best token-level accuracy of 97.98 % is comparable and even slightly better than the stateof-the-art results for English : 97.33 % when using Penn Treebank data only ( TARGETCIT , and 97.50 % for Penn Treebank plus some additional unlabeled data ( Søgaard , 2011 ) . Of course , our results are only indirectly comparable to English . Still , our performance is impressive because ( 1 ) o',\n",
       " 'the mentioned drawbacks and whether using information available in phrase-pairs during decoding can help improve search accuracy and translation quality . 5.1 Training We extended the training steps in TARGETCIT to extract a phrase lexicon from the parallel data . We extract all phrase pairs of length 6 and below , that are consistent ( ) with the word alignments . Only continuous phrases as used in a tra',\n",
       " 'esseg ( ) treats words in a segment generated from a segment specific multinomial language model , i.e. , it assumes each segment is generated from one topic , and a later hierarchical extension ( TARGETCIT assumes each segment is generated from one topic or its parents . Other methods using as input the output of topic models include ( ) , ( ) , and ( ) . In this paper we take a generat',\n",
       " 'settings ( ) in classifying sentences in different grammaticality settings . Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality . TARGETCIT demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars ( ) outperformed the Cherry and Quirk models . The intuitions underlying their approach were that larger frag',\n",
       " 'croscope 3.4 Implementational details Our model has been trained on the UKWaC corpus ( ) . The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger ( ; TARGETCIT , and parsed with MaltParser ( ) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1 , so that dependency t',\n",
       " 'be added to the representation in order to filter out unwanted readings . Hole Semantics ( ) , Constraint Language for Lambda Structures ( CLLS ) ( ) , and Minimal Recursion Semantics ( MRS ) ( TARGETCIT are among these frameworks . In an effort to bridge the gap between the above formalisms , a graph theoretic model of scope underspecification was defined by , called Weakly Normal Dominance Gra',\n",
       " 'is gaining recognition ( ) . If a good evaluation metric could not be used for tuning , it would be a pity . The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune ( TARGETCIT . ( ) report similar observations , in addition citing WER being difficult and BLEU-TER being amenable . One unsolved question is whether metric tunability is a problem inherent to the metric onl',\n",
       " 'pairs , approximately 1.5 million words per language . Selected Many research ideas have exploited the Web in unsupervised or weakly supervised algorithms for natural language processing Citing ( e.g . , TARGETCIT ) addressed the issue of language identification for finding Web pages in the languages of interest . , the Web is harvested in search of pages that are available in two languages , wit',\n",
       " 'pe , Center Establishment ( EST ) , for utterances E.g. , in Bruno was the bully of the neighborhood . 1987 propose a default ordering on transitions which correlates with discourse coherence . 1988 TARGETCIT proposed rules for tracking initiative based on utterance types ; for example , statements , proposals , and questions show initiative , while answers and acknowledgements do not . 1989 explored ti',\n",
       " 'hat our approach can effectively utilize the syntactic knowledge from another treebank and significantly improve the stateof-the-art parsing accuracy . 11We thank the authors for sharing their results . TARGETCIT also use the reranker ( RP ) of as a stronger baseline , but the results are missing . They find a less improvement on F score with RP than with GP ( 0.9 % vs. 1.1 % ) . We refer to their Table 5 and',\n",
       " 'ADVP VP VP TO+VB TO VB ADVP VP The first step is to select team members M一 步 是 YKA ( a ) Before ( b ) After TO The first step is to select team members M一 步 是 YKA for learning translation rules ( ; TARGETCIT , or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries ( ) . The most closely related MT system is that of , who train a rule extr',\n",
       " 'e implement parallel rephrasing , changing the meaning of both source and target text simultaneously . There is , on the other hand , little work in phrasebased SMT especially targeting negated sentences . TARGETCIT approach the problem of properly translating negation in their general reordering setting . Transformation rules are applied to syntactic trees , so that the source language word order has a closer rese',\n",
       " 'oms ) .2 ACE mention detection systems ( e.g. , ( ACE , 2007 ; ACE , 2008 ) ) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning . Recently , ( TARGETCIT developed a bootstrapping technique that induces a semantic tagger from unannotated texts . We use their system in our ensemble . There has also been work on extracting semantic class members from the W',\n",
       " 'iloglou and McOTHERCIT ) . However , such approaches have been found to be highly topic dependent ( Engstr¨om , 2004 ; ) , which makes the creation of a general sentiment classifier a difficult task . TARGETCIT worked on a 2,829 sentence citation corpus using a 12-class classification scheme . While the authors did make use of the context in their annotation , their focus was on the task of determining the aut',\n",
       " ': 001 } – [ person ] holds a [ degree ] in [ subject ] from [ institution ] and a [ degree ] from [ institution ] . At this stage , we will have a set of conceptual units with corresponding template collections ( see TARGETCIT for a further explanation of Sections 3.1-3.2 ) . 1A similar approach to the clustering of semantic content is found in Duboue and McOTHERCIT , where text with stopwords removed were used as semantic inp',\n",
       " 'n performed . Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets ( ) , or models it as a subtask of learning fine-grained IS ( ; TARGETCIT ; ) . Results within this latter framework for bridging have been mixed : We reported in low results for bridging in written news text whereas report high results for the four',\n",
       " 'composed of a small amount of words and they are written in informal , sometimes cryptic style . These characteristics make hard the identification of entities and the semantics of their relationships ( TARGETCIT . Further , the scarcity of text in the messages makes it even harder to properly characterize a common context for the entities . Second , as we need to monitor messages that keep coming at a fast pace ,',\n",
       " 't partitionings of the DRDAs for input to the summarization system ; in the System Clusterings setting , we employ a hierarchical agglomerative clustering algorithm used for this task in previous work ( TARGETCIT . clustering method groups DRDAs according to their LDA topic distribution similarity . As better approaches for DRDA clustering become available , they could be employed instead . Evaluation Me',\n",
       " 'largely on the search coverage . Early work in this line focused more on existing segmentation algorithm , such as branching entropy ( ) and bootstrap voting experts ( ) . A recent study ( TARGETCIT on a compression-based algorithm , regularized compression , has achieved comparable performance result to hierarchical Bayes methods . Along this line , in this paper we present a novel extension to the',\n",
       " 'n of the LiveJournal6 users . What they found is that there is a remarkable correlation between the age and the location of the user and those of her friends , although there are interesting exceptions . TARGETCIT train a gender classifier on tweets with word and character-based ngram features achieving accuracy of 75.5 % . Adding the full name feature alone gives a boost to 89.1 % , further features like self-writ',\n",
       " 'a . Several approaches have recently been proposed in this context for the semantic role labeling task . ( ) were the first to introduce an unsupervised semantic parser , followed by ( ) , ( TARGETCIT , ( ) and ( ) . Finally , ( ) , introduced two new Bayesian models that achieve the best current state-of-the-art results . However , all these works use some kind of supervision ( name',\n",
       " 'ich use only raw texts ( ) , and semi-supervised methods ( ) which use both raw texts and annotated corpus . And there are a lot of efforts have also been devoted to bilingual projection ( TARGETCIT , which resorts to bilingual text with one language parsed , and projects the syntactic information from the parsed language to the unparsed one ( ) . In dependency grammar induction , unsupervise',\n",
       " 'back and covers the skipped German words through the following sequence of operations : Jump Back ( 1 ) -+ Generate ( Millionen , millions ) -+ Generate ( von , of ) -+ Generate ( Dollar , dollars ) 2Please refer to TARGETCIT for a list of operations and the conversion algorithm . The generative story of the OSM model also supports discontinuous source-side cepts and source-word deletion . However , it doesn ’ t provide a mecha',\n",
       " 'n generator to match annotated human–human dialogue . In text generation , researchers have been able to exploit automatic analysis of existing resources on such tasks as ordering words more naturally ( TARGETCIT and identifying named entities in line with attested mentions ( ) . However , previous work on training dialogue generation has involved the acquisition or annotation of relevant data ad hoc , for',\n",
       " 'pute Fl as the harmonic mean between precision and recall . For comparison with previous work , we further apply a couple of previously proposed local coherence models : the original entity grid model by TARGETCIT , a modified version that uses topic models ( ) and an extended version that includes entity-specific features ( ) . We further apply the discourse-new model by and the pronoun-b',\n",
       " 'dual semantic– syntactic steps in the derivation . Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs ( ; TARGETCIT or resorted to pre-/post-processing strategies that modify semantic or syntactic structures ( Lluis and M ` arquez 2008 ; ) . In a constituency setting , explore different levels of couplin',\n",
       " 'ossible : A two-layered RNN would provide more expressive power , however , it is much harder to train because the resulting neural network becomes very deep and suffers from vanishing gradient problems . TARGETCIT proposed to give every single word a matrix and a vector . The matrix is then applied to the sibling node ’ s vector during the composition . While this results in a powerful composition function that ess',\n",
       " 'the semantic classes obtained in unsupervised semantic role labeling tasks . We then evaluate them with a classical metric used to evaluate these classes in unsupervised SRL ( as done for instance in ( TARGETCIT a ) and ( ) ) : purity and collocation . Purity measures the degree to which each cluster contains instances that share the same gold class , while collocation measures the degree to which instances',\n",
       " 'reported state-of-the art results by incorporating additional information from surrounding words ( ) , multilingual alignments ( ) , or overlapping context features in a log-linear model ( TARGETCIT , but they have only been run on Semitic languages and English ( and in the latter case , a very small corpus ) . Since they explicitly enumerate and sample from all possible segmentations of each word ( o',\n",
       " 'three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors ( as they are defined in that system ) . Many of the details have been reported previously ( TARGETCIT ; ) , so here we will only give brief summaries of these modules . It is important to note that this system was run without modification . That is , no training of new models or tuning was carried',\n",
       " 'translation equivalences , as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks ( ; Popovi´c and ; TARGETCIT , among others ) . 5.1 Micro-analysis : WADE We define Word Alignment Driven Evaluation , or WADE , which is a technique for analyzing MT system output at the word level , allowing us to ( 1 ) manually browse',\n",
       " 'd using Joshua ( ) . We tuned with minimum errorrate training ( ) using Z-MERT ( ) and present the mean BLEU score on test data over three separate runs ( ) . MBR reranking ( TARGETCIT was applied to Joshua ’ s 300-best ( unique ) output , and evaluation was conducted with case-insensitive BLEU with four references . The training data was produced by pairing a source sentence with each of',\n",
       " 'ree kernels , the output of the dependency parser had to be transformed into a single tree structure with a unique label per node and unlabelled edges , similar to a constituency parse tree . We followed TARGETCIT in using a tree representation which encodes partof-speech tags , dependency relations and words as sequences of child nodes ( see fig . 1 ) . 110 Figure 1 : Representation of the dependency tree fragment V',\n",
       " 'over various semantic relations ( e.g . < Obama , president , the United States > ) from natural language text . Traditional RE systems extract specific relations for prespecified name-entity types ( ; TARGETCIT ; ) . To train such systems , every relation needs manually annotated training examples , which supports limited scope and is difficult to extend . For this reason , proposed Open Informat',\n",
       " 'l not necessarily yield better translation performance ( ) . Therefore , many approaches have been proposed to learn word segmentation suitable for SMT . These approaches were either complicated ( TARGETCIT ; ) , or of high computational complexity ( ) . Moreover , they implicitly assumed that WSA and WSR should be equal . This requirement may lead to a suboptimal problem that word segmentatio',\n",
       " 'able to the classification system . Recall that existing approaches have relied primarily on morphosyntactic features as well as a few semantic features extracted from WordNet synsets and VerbOcean ’ s ( TARGETCIT semantic relations . On the other hand , we propose not only novel lexical and grammatical features , but also sophisticated features involving semantics and discourse . Most notably , we propose ( 1 ) seman',\n",
       " 'P after the first mention . 5 HTC schema as our task-specific filters for selecting just story characters . Moreover , we plan to explore other state-of-the-art coreference systems such as CherryPicker ( TARGETCIT . The NLP tools and techniques discussed above can be applied to cross-document coreference resolution as well ( see , for discussion of a meta document ) , although training the systems for narr',\n",
       " 'sed . An explicit sampler represents and samples the model parameters in addition to the word alignments , while in a collapsed sampler the parameters are integrated out and only alignments are sampled . TARGETCIT proposed a collapsed sampler for IBM Model 1 . However , their sampler updates parameters constantly and thus can not run efficiently on large-scale tasks . Instead , we take advantage of explicit Gibbs sa',\n",
       " 'naturally viewed as constraint optimization problems . We employ Integer Linear Programming ( ILP ) as an optimization framework that has been used successfully in other generation tasks ( e.g. , , TARGETCIT ) . Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process . Empirical results demonstrate that our final',\n",
       " 'ajority of emotion-bearing features , e.g. , emoticons , repeated letters , exasperation , are used more by female than male users , which is consistent with results reported in other recent work ( ; TARGETCIT ; ) . Other related work is that of , who studied stylistic differences between male and female reviewers writing product reviews , and , who applied positive , negative and emoti',\n",
       " 'IT ; Bellare and McOTHERCIT ) . Our work was inspired by who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia . Previous works ( ; TARGETCIT have pointed out that the DS assumption generates noisy labeled data , but did not directly address the problem . applied a rule-based method to the problem by using popular entity types and ke',\n",
       " 'be a primary concern , and previous work also defines events to be the same if they have the same surface verb , in some cases with a restriction that the dependency relations should also be the same ( TARGETCIT ; ) . Word sense ambiguities are also reduced in specific genres ( Action and Romance ) of film scenes . Our method for estimating the likelihood of a CONTINGENT relations between events consists',\n",
       " 'from machine translation to information extraction . Supervised learning of taggers from POS-annotated training text is a well-studied task , with several methods achieving near-human tagging accuracy ( TARGETCIT ; ) . However , while English and a handful of other languages are fortunate enough to have comprehensive POSannotated corpora such as the Penn Treebank ( ) , most of the world ’ s languages',\n",
       " 'averaging . al . ( 2011 ) , and Wlr with 2 + E , where I ∈ Rd×d is an identity matrix . Here , E is zero-mean gaussian random variable with a variance of 0.01 . The initialization of Wlr is the same as that of TARGETCIT . The remaining model parameters were initialized with 0 . We tuned hyperparameters in our model using the validation set for each experimental setting . The hyperparameters include the regularization p',\n",
       " 'ta is available at http : //statmt.org/wmt10 . The results are shown in the alignment F1 column of Table 1 . We used balanced F-measure rather than alignment error rate as our metric ( ) . Following TARGETCIT , we also measured the average fertility , ˜φsing. , of once-seen source words in the symmetrized alignments . Our alignments show smaller fertility for once-seen words , suggesting that they suffer from',\n",
       " 'of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic ( ) , Basque ( ) , Croatian ( ) , French ( ) , German ( TARGETCIT , Hebrew ( Tsarfaty and Sima ’ an , 2010 ; Goldberg and 1http : //alpage.inria.fr/iwpt09/panel.en . html 2See http : //www.spmrl.org/ and related workshops . ) , Hindi ( ) , Korean ( ) and Sp',\n",
       " 'duced symmetrised alignments compared to a human aligned dataset . Moses ( ) was used for the training of the SMT system and the symmetrisation ( using the grow-diag-final procedure ) , with MERT ( TARGETCIT used for tuning of the weights , and SRILM ( ) to build the language model ( 5-grams based ) . The corpus used for training and evaluation was the Chinese FBIS corpus . MT02 was used for tuning , and',\n",
       " 'asure of semantic relatedness between pairs of words . We model semantic relatedness between two words using the Information Content ( IC ) of the pair in a method similar to the one used by and TARGETCIT . IC ( w1 , d , w2 ) = log f ( w1 , d , w2 ) f ( * , d , * ) f ( w1 , d , * ) f ( * , d , w2 ) Here , d can generally represent some form of relation between w1 and w2 . and used dependency relation between words , w',\n",
       " 'ation systems , and have been successfully applied to tasks such as multi-document summarization ( ) , query expansion ( ) , question answering ( McOTHERCIT ) , sentence compression ( ; TARGETCIT , and simplification ( ) . Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora . Several approaches rely on bilingual parallel data ( OTHERCI',\n",
       " 's suggests an intrinsically phrase-based alignment model . The token aligner jacana-align ( ) has achieved state-of-the-art result on the task of monolingual alignment , based on previous work of TARGETCIT . It employs a Conditional Random Field ( ) to align tokens from the source sentence to tokens in the target sentence , by treating source tokens as “ observation ” and target tokens as “ hidden st',\n",
       " 'a Perl software package that measures semantic similarity ( or relatedness ) between a pair of word senses by returning a numeric value that represents the degree to which they are similar or related ( TARGETCIT . Additionally , we developed a custom-built method that assumes that two words are semantically related if any kind of pointer exists between any occurrence of the words root form in WordNet . For deta',\n",
       " 'eating annotated resources for computational linguistics . provide an overview of various tasks for which MTurk has been used , and offer a set of best practices for ensuring high-quality data . TARGETCIT a ) studied the quality of crowdsourced translations , by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC . They demons',\n",
       " 'using factored decoding ( ) to disambiguate connectives , with small improvements . Lexical consistency has been addressed by the use of post-processing ( ) , multi-pass decoding ( ; TARGETCIT , and cache models ( ) . addressed the issue of tense selection for translation from Chinese , by the use of inter-sentential tense n-grams , exploiting information from previously transl',\n",
       " '96 後 hòu 9 1028 215 22051 3751 裡 lǐ 9 1086 97 0 0 外 wài 28 1254 154 4370 1918 東 dōng 139 33 0 0 424 西 xa 147 66 3 0 874 南 nán 118 20 0 0 390 北 běi 199 78 0 0 731 2.4 Clustering using LDA and Hive Plot TARGETCIT :437 ) points out that “ much of the lexical ambiguity of verbs and prepositions is eliminated because the semantic load is spread more evenly throughout the lexicon to the other lexical categories. ” He',\n",
       " 'arser to make accurate parsing decisions and capture similarities between phrases and sentences . Any PCFG-based parser can be improved with an RNN . We use a simplified version of the Stanford Parser ( TARGETCIT a ) as the base PCFG and improve its accuracy from 86.56 to 90.44 % labeled F1 on all sentences of the WSJ section 23 . The code of our parser is available at nlp.stanford.edu . 2 Related Work The CVG is',\n",
       " 't these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions ( ) , and can even provide a strong signal for unsupervised coreference ( ; TARGETCIT ; ) . A second problem , that has received significantly less attention in the literature , is that the pairwise coreference models scale poorly to large collections of mentions especially when t',\n",
       " '1 : Parsing tree of the terms lipid storage and retention of lipids processing textual data , this algorithm has been previously applied in different contexts such as semantic disambiguation ( ; TARGETCIT , summarization ( ) and , more recently , for the identification of synonyms ( ) . This last work takes into account the usage of a given word in corpora and its known synonyms from lexical',\n",
       " ') , to train g2p systems ( ) . Further , other algorithms using such dictionaries also use translation phrase tables , but not for translation tasks . For example , data-driven paraphrasing methods ( TARGETCIT use translation phrase-tables as a “ pivot ” to learn sets of phrases which translated to the same target phrase . In a similar manner , with a pronunciation dictionary instead of a phrsetable , pivoting c',\n",
       " 'scores to nodes in the parse tree . A similar integrated contribution of lexical information ( i.e . word vectors ) and syntactic constituency is proposed in semantic extensions of TKs , as introduced in ( TARGETCIT . As they offer a framework to define similarity metrics strongly tied to the syntactic structure of entire sentences , they will be hereafter discussed . Tree Kernels . Kernels are representationally ef',\n",
       " 'Linguistic Structure , pages 1–7 , Montr´eal , Canada , June 3-8 , 2012. c�2012 Association for Computational Linguistics CONTAINER Apply_Heat Figure 1 : An example of a semantic dependency graph . ; TARGETCIT b ; ) . However , all these approaches have focused on PropBank-style representations . This may seem somewhat unnatural as FrameNet representations , though arguably more powerful , are harder to l',\n",
       " 'ions , but discover all types of relations found in the text . The relations represent clusters over strings of words ( ) , syntactic patterns between entities ( ) , or logical expressions ( TARGETCIT . Another learning paradigm is distant supervision which does not require labeled data but instead access to a relational database such as Freebase ( ) . The idea is to take entities that appear',\n",
       " 'an . Each of these can be though of as an implicit function application ( in the last case , the identity function ) . 3.3 Differences From Previous Work While the grammar formalism is strongly inspired by TARGETCIT , a number of key differences are implemented to both simplify the framework , and make inference more efficient . Sequence Grounding The most timeconsuming and conceptually nuanced aspect of temporal i',\n",
       " 'ate most systems are extractive and proceed by deleting words from the input ( , inter alia ) . To decide which words , dependencies or phrases can be dropped , ( i ) rule-based approaches ( ; TARGETCIT ; ) , ( ii ) supervised models trained on parallel data ( ; McOTHERCIT , inter alia ) and ( iii ) unsupervised methods which make use of statistics collected from non-parallel data ( )',\n",
       " 'resent work in context by relating it to existing work on mining literature and chat disentanglement . 3 Relationship to Existing Work Most similar to MND is the task of chat disentanglement ( ; TARGETCIT , wherein utterances ( perhaps overheard at a cocktail party ) are to Actor Fabula Story Focalizer 2 Figure 1 : A schematic of the narratology theory . The dotted line between author and fabula implies th',\n",
       " 'nments in training data and alignments accepted by a synchronous grammar ( learned from data ) . This is useful for literature on learning from word aligned parallel corpora ( e.g. , ( ; DeOTHERCIT ; TARGETCIT ; ; Mylonakis and Sima ’ an , 2011 ; ; McOTHERCIT ) ) . A theoretical , formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as',\n",
       " 'inear sequence . However , to apply dependency cohesion constraint , the subtree span of a head node is computed based on the alignments of its children , so children must be aligned before the head node . TARGETCIT propose a hierarchical search procedure to traverse all nodes in a phrase structure tree . Similarly , we define a bottom-up topological order ( BUT-order ) to traverse all words in the source side depend',\n",
       " 'ion , etc . Thus , considering the properties of debates might further improve the segmentation performance . 5.3 Evaluation on Written Texts We further tested TSM on two written text datasets , Clinical ( TARGETCIT and Fiction ( ) . The statistics are shown in Table 3 . Each document in the Clinical dataset is a chapter of a medical textbook . Section breaks are selected to be the true topic boundaries . For',\n",
       " 'intervening MTUs to form a bigger unit ti in Figure 2 ( c ) . A solution that uses split-rules is proposed by but has not been adopted in Ncode ( ) , the state-of-the-art TSM Ngram system . TARGETCIT dealt with this problem by applying a post-processing ( PP ) heuristic that modifies the alignments to remove such cases . When a source word is aligned to a discontinuous target-cept , first the link to',\n",
       " 'es such as WordNet ( ) and linguistic theories such as Centering Theory ( ) . They partially succeed in improving performance , but there is still room for further improvement ( duOTHERCIT ; TARGETCIT ; ) . Unlike past attempts relying on heuristic feature engineering , we take a cognitive science approach to improving system performance . In stead of employing existing resources and theories ,',\n",
       " 'rules derived from its backoff model , which diminishes the benefits obtained from large tree fragments . On the other hand , current state-of-the-art parsers use symbol refinement techniques ( ; TARGETCIT . Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols ( e.g . NP and VP ) into subcategories , rather than extracting large tree fragme',\n",
       " 'oversimplification . This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches . Shallow ORE. TextRunner ( ) and its successor ReVerb ( TARGETCIT are based on the idea that most relations are expressed using few syntactic patterns . ReVerb , for example , detects only three types of relations ( “ verb ” , “ verb+preposition ” and “ verb+noun+preposition ”',\n",
       " 'ling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context . There exist several studies that enrich immediate context with large corpus statistics . TARGETCIT proposed a method to combine sense similarity with distributional similarity and configured predominant sense score . Distributional similarity was used to weight the influence of context words , based',\n",
       " 'corpora such as LDC Gigaword corpora were not fully utilized due to memory limitations of computers and even with large memory machines , the LM corpora is split into pieces , interpolated , and merged ( TARGETCIT or the LM order is decreased to use up to 4-grams ( ) or low frequency n-gram counts are omitted and better smoothing techniques are developed ( ) . Using only the given training data for',\n",
       " 'ely by genre for the action and romance genres of the film corpus . Pointwise Mutual Information . The majority of related work uses pointwise mutual information ( PMI ) in some form or another ( ; TARGETCIT . Given a set of events ( a verb and its collected set of subjects and objects ) , we calculate the PMI using the standard definition : P ( e1 , e2 ) pmi ( e1 , e2 ) = log ( 1 ) P ( e1 ) P ( e2 ) in which e1 and e2 are tw',\n",
       " 'rized alignments using the method described by . All data was tokenized and lowercased ; German compounds were split ( ) . For word alignment of the news-commentary data , we used GIZA++ ( TARGETCIT ; for aligning the Europarl data , we used the Berkeley aligner ( ) . Before training , we collect all the grammar rules necessary to 4Note that by definition of ||W||1,2 , standard f1 regularizati',\n",
       " 'over the IHMM model . and test sets are collected as the input of the system combination . Our baseline systems are described as follows . Two main baseline systems are IHMM based and incremental IHMM ( TARGETCIT . The first system differs from our method just in hypothesis alignment algorithm , and the second combines the first and second module of the system combination pipeline . Because our method utilizes b',\n",
       " 'er # 2 : simple tree anomaly model Another method we use for building parse revisions is based on a method to detect anomalies in parse structures ( APS ) using n-gram sequences of dependency structures ( TARGETCIT ; ) . The method checks whether the same head category ( e.g. , verb ) has a set of dependents similar to others of the same category ( ) . To see this , consider the partial tree in figure 2',\n",
       " 'ementally produced ) label file , it can not easily be used in an incremental system . A predecessor of our iSS component ( which was not yet fully incremental on the HMM level ) is described in detail in ( TARGETCIT a ) . 3 Incremental and Adaptive NLG 3.1 The SPUD microplanning framework The NLG component presented here is based on the SPUD microplanning framework ( ) and realised in DeVault ’ s ( 2008 ) implem',\n",
       " 'orpus still contains a fair amount of noisy data based on our random sampling . 342 dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3 baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19 ( TARGETCIT 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18 -0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64 -0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69 -1M 41.28 37.41 35.28 29.65 31.73 24.23 23 .',\n",
       " 'text together with the demographics variable . However , they did not analyse how social environment affects language , although very similar questions have been recently posed ( but not yet answered ) by TARGETCIT . In our work we attempt to address precisely this issue . In particular , we consider the task of user gender prediction on YouTube and contrast two information sources : ( 1 ) the comments written by the',\n",
       " 'ting language models using perplexity is fairly well-established ( e.g . ) , but for phrase-tables it is unclear whether perplexity minimisation ( ) or linear or log-linear interpolation ( TARGETCIT ; ) is the best approach . Also , other authors ( ) have tried to weight the input sentences or extracted phrases before the phrase tables are built . In this type of approach , the main pr',\n",
       " 'words into views and cluster words inside each view . In our case , each sense of a path can be seen as one view . However , we allow different views to be merged since some views overlap with each other . TARGETCIT cluster pairs of named entities according to the similarity of context words intervening between them . uses topic models to perform dimensionality reduction on features when clustering entity',\n",
       " 'ing ( ) to multilingual setting . Instead of using machine translation engines to translate labeled text , the authors use it to construct the word translation oracle for pivot words translation . TARGETCIT focus on the task of jointly improving the performance of sentiment classification on two languages ( e.g . English and Chinese ) . the authors use an unlabeled parallel corpus instead of machine transla',\n",
       " 'ta . The most commonly used features for grammatical error correction are lexical and POS N-grams , and chunk features . We adopt the features from previous work by , and for our system . TARGETCIT show that parse features can further increase performance , and we use the dependency parse features based on their work . For all the above features , the observed article or preposition used by the wri',\n",
       " 'y , electronic glossaries have been shown to be key resources not only for humans , but also in Natural Language Processing ( NLP ) tasks such as Question Answering ( ) , Word Sense Disambiguation ( TARGETCIT ; ) and ontology learning ( ) . Today large numbers of glossaries are available on the Web . However most such glossaries are small-scale , being made up of just some hundreds of definitio',\n",
       " 'of not needing to sort all hypotheses , it has proven difficult to choose proper values for β . Due to this , all experiments presented in this paper only use HISTOGRAM PRUNING . 5 Iterative Beam Search ( TARGETCIT b ) propose a so called “ iterative EM algorithm ” . The basic idea is to run a decipherment algorithm—in their case an EM algorithm based approach—on a subset of the vocabulary . After having obtained the',\n",
       " 'in the following , pointing out the differences and extensions to earlier releases ( ) . 2 An Incremental Processing Architecture INPROTK realises the X-model of incremental processing ( ; TARGETCIT , where incremental systems are conceptualised as consisting of a network of processing modules . Each module has a left buffer , a processor , and a right buffer , where the normal mode of processing is',\n",
       " 'composed of 42 tags and reflects only number for nouns and some tense information for verbs whereas the ERTS comprises 115 tags and enriches the RTS with gender , number , and definiteness information . TARGETCIT b ; 2007a ) shows that using the ERTS improves results for higher processing tasks such as base phrase chunking of Arabic . 4.3 Standard Features This group includes two features that have been employed',\n",
       " 's have been developed specifically for the large-scale analysis of CHILDES . These tools enable further computational study such as the automatic calculation of the language development metrics IPSYN ( TARGETCIT and D-Level ( ) , or the automatic formulation of novel language development metrics themselves ( ) . The availability of child language is also key to the design of computational models o',\n",
       " 's article , we focus on linear complexity finite-state methods for deriving constraints on the chart . Recent work has also examined methods for constraining each of the O ( N2 ) chart cell independently ( TARGETCIT , permitting a finer-grained pruning ( e.g. , not just “ open ” or “ closed ” but an actual beam width prediction ) and the use of features beyond the scope of our tagger . We discuss this and other extension',\n",
       " 'e decoding was performed using a specially modified version of the OCTAVIAN decoder ( ) , an in-house multi-stack phrase-based decoder that operates on the same principles as the MOSES decoder ( TARGETCIT . This component of the system is implemented as a log-linear combination of 4 different models : a joint source-channel model ; a target language model ; a grapheme insertion penalty mode ; and a graphem',\n",
       " 'ly , the challenges that a rich morphology poses for data-driven parsing have received growing interest . A direct effect of morphological richness is , for instance , data sparseness on a lexical level ( TARGETCIT . A rather indirect effect is that morphological richness often relaxes word order constraints . The principal intuition is that a rich morphology encodes information that otherwise has to be conveyed',\n",
       " 'l relationships between two events , as seen in ( ) . Further work in that domain extended this start by identifying additional features that better predicted those temporal relations . ( ; TARGETCIT . In this work , we are primarily interested in applying event ordering techniques to documents less structured than news articles , specifically biographies . It is the intention of our work to validate',\n",
       " 'space of comparable corpora , we plan to incorporate context-based approaches with the RF classification method . Context-based approaches , such as distributional vector similarity ( Fung and McOTHERCIT ; TARGETCIT , can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity . Finally , the proposed method can be also used to online augment the phrase t',\n",
       " 'the source language . Reordering rules are defined over this parse either through machine learning techniques ( Xia and McOTHERCIT ; Khalilov and Sima ’ an , 2011 ) or linguistically motivated manual rules ( TARGETCIT ; ) . However , as building a parser for each source language is a resourceintensive undertaking , there has also been some interest in developing reordering rules without the use of a parser ( OT',\n",
       " 'sion , with the standard parameter settings . Berkeley ( ) . An unlexicalised parser with a grammar constructed with automatic state splitting . implementation of . BUBS ( ; TARGETCIT . A ‘ grammar-agnostic constituent parser , ’ which uses a Berkeley Parser grammar , but parses with various pruning techniques to improve speed , at the cost of accuracy . . A generative parser wit',\n",
       " 'to a sequence of basic reordering steps . Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated . The linear distortion model ( TARGETCIT encourages monotonic translations by penalizing source position jumps proportionally to their length . If used alone , this model is inadequate for language pairs with different word orders . tr',\n",
       " 'ity MDL-based search may not be as effec168 Method P R F Adaptors grammar , colloc3-syllable 86.1 88.4 87.2 Regularized compression + MDL , G2 ( b ) — 79.1 81.7 80.4 Regularized compression + MDL TARGETCIT 76.9 81.6 79.2 Adaptors grammar , colloc 78.4 75.7 77.1 Particle filter , unigram B¨orschinger and – – 77.1 Regularized compression + MDL , G1 ( b ) — 73.4 80.2 76.8 Bootstrap voting expe',\n",
       " 'pproach to identify ontology-derived features that can prove useful for sense induction . Bayesian approaches to sense induction have recently been shown to perform well in the WSI task . In particular , TARGETCIT have adapted the Latent Dirichlet Allocation ( LDA ) generative topic model to WSI by treating each occurrence context of an ambiguous word as a document , and the derived topics as sense-selecting conte',\n",
       " 'nd 2005 training sets ) and use the standard train/test splits reported in and . In ACE05-ALL , we have the full ACE 2005 training set and use the standard train/test splits reported in TARGETCIT and . Note that most previous work does not report ( or need ) a standard development set ; hence , for tuning our features and its hyper-parameters , we randomly split the original training data i',\n",
       " 'require an additional and expensive manual annotation effort . Previous work has primarily focused on automatic content scoring of short answers , ranging from a few words to a few sentences ( ; TARGETCIT . On the other hand , scoring of individual sentences with respect to their linguistic quality , specifically in learner texts , has received considerably less attention . devised guidelines for',\n",
       " 'lignments accepted by a synchronous grammar ( learned from data ) . This is useful for literature on learning from word aligned parallel corpora ( e.g. , ( ; DeOTHERCIT ; Mylonakis and Sima ’ an , 2011 ; TARGETCIT ; McOTHERCIT ) ) . A theoretical , formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well . We exemplify our claims by providing yet an',\n",
       " 'also a great challenge to generate a qualified summary based on the different types of sub-events . The component summaries should be correlative across different dates based on the global collection ( TARGETCIT a ) . Mei and Zhai ( ) proposed to use theme or topic to model different sub-events , which is to some extent similar to our method . To be different , in this paper we introduce “ local/global ” prop',\n",
       " 'and practitioners rely on a variety of measures , such as number of different words , type-token ratio , distribution of part-of-speech tags , and mean length of sentences and words per minute ( ; TARGETCIT ; ) , to name a few . Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level . These measures are helpful i',\n",
       " 'g of the Association for Computational Linguistics , pages 1004–1013 , Sofia , Bulgaria , August 4-9 2013. c�2013 Association for Computational Linguistics ROUGE evaluation metric used for summarization ( TARGETCIT . In addition , in the previous conceptbased ILP method , the constraints are with respect to the appearance of language concepts , hence it can not distinguish the importance of different language concep',\n",
       " 'ining MT systems . RAMPION minimizes lossramp 3 , which we found in preliminary experiments to work better than other loss functions tested.8 System and Datasets We use the Moses phrasebased MT system ( TARGETCIT and consider Urdu— * English ( UR— * EN ) , Chinese— * English ( ZH— * EN ) translation , and Arabic— * English ( AR— * EN ) translation.9 We trained a Moses system using default settings and features , except for setting',\n",
       " 'nguage results from an inability to speak anything else . 2.2 Length limits In the case of Twitter , the limit of 140 characters for each message is frequently cited as an explanation for bad language ( TARGETCIT . Does Twitter ’ s character limit cause users to prefer shorter words , such as u instead of you ? If so , one might expect shortening to be used most frequently in messages that are near the 140-characte',\n",
       " 'ity and as a common word . To overcome these two problems , we use cross-lingual features to improve NER using large bilingual resources , and we incorporate confidences to avoid having a binary feature . TARGETCIT used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages . Transliteration Mining ( TM ) has been used to enrich MT phrase tables or',\n",
       " 'of volume and time ) as an indicator of MT output quality . Good results in QE have been achieved by adding linguistic information such as shallow parsing , POS tags ( ) , or dependency relations ( TARGETCIT ; ) as features . However , in general these approaches do not distinguish between fluency ( i.e . syntactic correctness of the output translation ) and adequacy , and mostly rely on fluency-oriente',\n",
       " 'is to select team members M一 步 是 YKA for learning translation rules ( ) , or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries ( ; TARGETCIT . The most closely related MT system is that of , who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of tra',\n",
       " 'volve article mistakes , 14 % of errors are personal and possessive pronouns.9 Most of the determiner errors involve omitting an article . Similar error patterns have been observed in other ESL corpora ( TARGETCIT a ) . Our system focuses on article errors . Because the majority of determiner errors are omissions , it is very important to target this subset of mistakes . One approach would be to consider every space',\n",
       " 've no conversational character but are task oriented . Hence , automatic approaches are the preferred choice . Famous work on determining the satisfaction level automatically is the PARADISE framework by TARGETCIT . Assuming a linear dependency between objective measures and User Satisfaction ( US ) , a linear regression model is applied to determine US on the dialogue level . This is not only very costly , as dialo',\n",
       " '. These are : an augmented version of WN-A , which takes into account emotion bearing concepts which may have been present in headlines at the time of the task , a text-normalization unit , a CCG parser ( TARGETCIT , and a lexical lookup module , dependent on the output of the contextual valence shifters , which is used to determine whether an emotional term appears in the valence-classified headline . The valence',\n",
       " 'previous work as alternatives . has showed that empty categories can be added to the skeletal parses with reasonable accuracy with a simple pattern-matching algorithm in a postprocessing step . TARGETCIT b ; 2003a ) achieved generally superior accuracy using a machine learning framework without having to refer to the syntactic structure in the skeletal parses . They described their approach as a pre-proc',\n",
       " 'o relevant and critical to preserving endangered languages . We have begun extending the classifier to support additional Arabic script languages ( e.g. , Farsi , Urdu ) , leveraging resources from others ( TARGETCIT . Many other open questions remain regarding the annotation process , the visualizations , and the human expert . Which classified examples should the language expert review ? When should an annotator adj',\n",
       " 's are scaled so that its sample average and variance matches that of the average ( 3b ) . Judge Outlier Removal Removing judges whose inter-rater agreement with the average is less than 0.5 ; motivated by TARGETCIT : “ Given the high quality of the annotations among the turkers , we could alternatively use the correlation between the turkers itself to detect poor quality annotators ” . Weighted Voting Each judge ’ s j',\n",
       " 'ts by employing the Pitman-Yor process . improve the inference using iterated learning of increasingly longer sentences . Further improvements were achieved by better dealing with punctuation ( TARGETCIT b ) and new “ boundary ” models ( ) . 282 Other approaches to unsupervised dependency parsing were described e.g . in ( Søgaard , 2011 ) , ( ) , and ( ) . There also exist “ less unsupervised',\n",
       " 'parsing approaches for semantic interpretation of route descriptions have been presented and evaluated for route following through human participants and/or robots in real and/or virtual environments TARGETCIT . Most of these works have focused on interpreting manually transcribed or human written route descriptions . Understanding verbal route descriptions has not received much attention . In [ 4 ] an ASR syst',\n",
       " 'onment and finds that references to target objects are often not made when they first become visible . Instead interaction partners are navigated to a spot from where an easier description is possible . TARGETCIT develop a planning-based approach of this behavior . But once their system decides to generate a referring expression , it is delivered in one unit . , on the other hand , proposes a game-theoreti',\n",
       " 'We compare our system with state-of-the-art systems for both English and Spanish . To the best of our knowledge , no prior work exists for the other four languages . We evaluate in the same framework as TARGETCIT . We compare to previous system scores when constrained to make a prediction on every example ; if no guess is made , the output is considered incorrect . This in general yields lower results for those s',\n",
       " 'tion over multiple foreign language translations for a paraphrase pattern . After the SCFG has been extracted , it can be used within standard machine translation machinery , such as the Joshua decoder ( TARGETCIT . Figure 3 shows an example for a synchronous paraphrastic derivation produced as a result of applying our paraphrase grammar in the decoding process . The approach outlined relies on aligned bilingual',\n",
       " 'IT , where arcs are created involving the topmost stack node and the first buffer node , the system has also been presented in an equivalent form with arcs built between the two top nodes in the stack ( TARGETCIT . This variant can also be described as a divisible transition system , with LEFT-ARCASi = UNSHIFT ; LEFT-ARC ; REDUCE ; SHIFT and RIGHT-ARCASi = UNSHIFT ; RIGHT-ARC ; SHIFT ; REDUCE.5 Example 4 Nivre ’ s ( 200',\n",
       " 'e functions in actual language use . It can highlight important information , it can increase the probability of the message being understood ( ) and it is often used to repair misunderstanding ( TARGETCIT . In incremental microplanning , redundant information can be present both within one sub-utterance chunk ( e. g. , ‘ tomorrow , March 26 , ... ’ vs. ‘ tomorrow ... ’ ) or across IMPTs . For the former case , we',\n",
       " 'e either extractive or abstractive . Researchers mainly focus on the former which extracts the information deemed most important to the summary . Various techniques have been used for this type of MDS ( TARGETCIT ; ) . Graph-based text summarization techniques have been widely used for years . The algorithms , used in TextRank ( ) and LexPageRank ( ) , which are meant to compute sentence impo',\n",
       " 'to represent these instances . In this paper , we build a baseline system that uses maximum entropy models as the classification algorithm . For generation of training instances , we follow the method of TARGETCIT . For each predicted mention m , we generate a positive mention pair between m and its closest preceding antecedent , and negative mention pairs by pairing m with each of its preceding predicted mention',\n",
       " '3 1.26 Other 38 1.43 Table 2 : Syntactic nodes for coarse-grained focus . 4 Annotating Fine-Grained Focus We have annotated fine-grained focus of negation on top of the coarse-grained focus annotated by TARGETCIT . In this paper , we concentrate on negations whose coarse-grained focus is a prepositional phrase ( PP ) , adverbial phrase ( ADVP ) or subordinate clause ( SBAR ) . Excluding cases in which the verb is the c',\n",
       " 'In this work , we consider the underlying latent topics of the documents ( ) . Topic modeling has received some use in SMT , for instance Bilingual LSA adaptation ( ) , and the BiTAM model ( TARGETCIT , which uses a bilingual topic model for learning alignment . In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automat',\n",
       " 'e language into the written form of another language . Often this involves some level of phonetic analysis in one or both languages . Notable recent work in this vein includes research by Sproat et al ( TARGETCIT on transliteration between Chinese and English using comparable corpora , and Ravi and Knight ( ) who take a decipherment approach to this problem . Our work differs from all previous work on gra',\n",
       " 'ist distributed with the ROUGE summarization evaluation package . 195 Model Accuracy ( % ) CFG 72.6 2DOP 73.5 2DOP ( F ) 76.8 BTSG 78.4 Table 1 : Classification accuracy The CFG result represents the work of TARGETCIT a ) , the previous best result for this task . While in their work they report 80 % accuracy with the CFG model , this is for a single sampling of the full data set . We observed a large variance in classif',\n",
       " 'ts for our experiments using Moses ( ) with Lattice Minimum Bayes-Risk Decoding5 ( ) in combination with Batch Mira ( ) for tuning . Below are results for experiments with Joshua ( TARGETCIT using Viterbi decoding ( i.e . no MBR ) and PRO ( ) for tuning . were done on Joshua ( ) , using the Viterbi best derivation . The second set of experiments was done on Moses ( ) using',\n",
       " 'approve endorses Table 7 : Two examples of oov translations found by our method . 5 Related work There has been a long line of research on learning translation pairs from non-parallel corpora ( ; TARGETCIT ; ) . Most have focused on extracting a translation lexicon by mining monolingual resources of data to find clues , using probabilistic methods to map words , or by exploiting the cross-language',\n",
       " '5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney ( ) . Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO ( TARGETCIT ; ) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation ( MEMT ) package was used for system combination ( ) . We used BOLT dev12 dev as a development test se',\n",
       " 'roblem of word boundary ambiguity . As a result of these efforts , the performance of state-of-the-art supervised POS tagging shows over 97 % of accuracy ( ; Gim´enez and M ` arquez , 2004 ; ; TARGETCIT ; ) . Due to the high accuracy of supervised approaches for POS tagging , it has been deemed that there is no room to improve the performance on POS tagging in supervised manner . Thus , recent st',\n",
       " 'whole text being translated . For example , demonstrated that repetition of content words is a predictor of translation quality , with poorer translations failing to repeat words appropriately . TARGETCIT and present caching of translations from earlier sections of a document to facilitate the translation of its later sections . In scholarship that deals with properties of human translation of',\n",
       " 'caterpillar appears in the other sentence ; 1http : //www.cs.technion.ac.il/˜gabr/ resources/data/wordsim353/wordsim353.html 442 7 . Words are POS-tagged using Penn Treebank compatible POS-taggers : NLTK ( TARGETCIT for simple , and OpenNLP2 for syntax ; 8 . Stopwords are removed using a list of 36 stopwords ( simple ) . While we acknowledge that some of the preprocessing steps we take may not be common , we did not hav',\n",
       " 'tween eye gaze and language comprehension/production ( ) . Compared to the studies on language and eye gaze , the role of gaze in general problem solving settings has been less studied ( ; TARGETCIT . Since our current interest , text annotation , can be considered a problem solving as well as language comprehension task , we refer to them when defining our prob214 Proceedings of the 7th Linguistic',\n",
       " 'ation ( such as word-form , lemma , category , subcategory or morphological features ) . We will use one of its latest versions ( MaltParser version 1.7 ) . To fine-tune Maltparser we have used MaltOptimizer ( TARGETCIT a ; ) . This tool is an interactive system that first performs an analysis of the training set in order to select a suitable starting point for optimization and then guides the user through the',\n",
       " 'mostly focused on identifying material that could be elided and on defining procedures capable of producing input structures for surface realisation that support the generation of elliptic sentences . TARGETCIT developed a sentence planner which generates elliptic sentences in 3 steps . First , input data are grouped according to their similarities . Second , repeated elements are marked . Third , constraints are',\n",
       " '-order features and achieved the best reported result among the supervised parsers . Suzuki2009 ( ) reported the best reported result by combining a Semisupervised Structured Conditional Model ( TARGETCIT with the method of ( ) . However , their decoding complexities were higher than ours and we believe that the performance of our parser can be further enhanced by integrating their methods with ou',\n",
       " 'to relevant news stories , but currently do so with low coverage and consistency ; an event linker can add referentially-precise hyperlinks to news . The event linking task parallels entity linking ( NEL ; TARGETCIT , considering a news archive as a knowledge base ( KB ) of events , where each article exclusively represents the zero or more events that it first reports . Coupled with an appropriate event extractor , e',\n",
       " ', because they both contain [ si−1 , ti−1 ] and the approximate top is the largest block that does so . Therefore , the approximation errs on the side of adjacency , meaning it can only make mistakes when 5 TARGETCIT 7 ) provide an efficient algorithm for * -reduction that uses additional book-keeping so that the number of permutation checks as one traverses the entire sequence is linear in aggregate ; however , we im',\n",
       " 'ction , letterto-phone conversion and transliteration between different scripts . In particular , our substring-based approach to spelling correction is motivated by the success on transliteration ( e.g. , TARGETCIT ; ) and letter-tophoneme conversion ( e.g. , ) . One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors',\n",
       " 'inion words ( ) . There are two lines of work on sentiment polarity lexicon induction : corpora-based ( Hatzivassiloglou and McOTHERCIT ) and dictionary-based ( ; Agerri and Garc´ıaOTHERCIT ; TARGETCIT . Our work falls into the latter . Most of these works use the lexical relations defined in WordNet ( e.g. , synonym , antonym ) to derive sentiment lexicons . To our knowledge , none of the earlier works st',\n",
       " 'stracts section of the Bioscope corpus correspond to speculations . Early work on speculative language detection tried to classify a sentence either as speculative or non-speculative ( see , for example , TARGETCIT ) . This approach does not take into account the fact that hedging usually affects propositions or claims ( ) and that sentences often include more than one of them . When the Bioscope corpus ( OT',\n",
       " 's with a lot of entity-based features . Addressing the aforementioned challenges is a subject for future work . There has been an increasing interest in knowledge-rich co-reference resolution ( ; TARGETCIT . We use Wikipedia differently from ( ) who focus on using WikiRelate , a Wikipedia-based relatedness metric ( ) . ( ) used the union of all possible interpretations a mention may h',\n",
       " 'can be seen as intersecting the grammar with a fixed source-language string and a finite-state machine constraining the target-language string . The widely used decoding algorithms for SCFG ( ; TARGETCIT ; ) search for the highest-scoring translation when combining scores from a weighted SCFG and a weighted finite-state language model . As with SCFG , language-model–integrated decoding for weigh',\n",
       " 'ic records ( i.e . MRs ) and trains a hierarchical semi-Markov generative model using EM , and then finds a Viterbi alignment between NL words and records and their constituents . Several recent projects ( TARGETCIT ; ) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards . These systems do not even need the ambiguous supervision obtained from observing huma',\n",
       " 'ence that other interpretations it originally entertained were not what the user intended . This links each user utterance with a corresponding utterance plan that can be used for subsequent learning ( TARGETCIT . The understanding module parses utterances using the grammar and resolves them using the con32 Figure 2 : COREF system architecture , showing representations and knowledge shared across modules : utter',\n",
       " 'Training Corpus For the extraction of the tree transformations , we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [ ] parsed by the Stanford constituency parser [ TARGETCIT ] . Language Model For this experiment a 3-gram language model is used to rank candidate simplifications . The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple senten',\n",
       " 'ange of similarity functions , including Jaccard and the cosine between two vectors T ( w ) and T ( w0 ) ( cf . Equation ( 5 ) in Figure 1 ) . In the context of lexical semantics , the similarity measure defined by TARGETCIT has been very successful . This measure ( cf . Equation ( 6 ) in Figure 1 ) takes into account syntactic dependencies ( d ) in its co-occurrence model . In this case , the set T ( w ) of cooccurrences of w contain',\n",
       " 'settings : We simply reused the settings of most previous studies . We used CoNLL ’ 03 data ( ) for NER , and the Penn Treebank ( PTB ) III corpus ( ) converted to dependency trees for DEPAR ( TARGETCIT . s.t . u E SN , where b = w + α ( 5 ) Step3 α-update : α ( t+1 ) = α ( t ) + ξ ( w ( t+1 ) − u ( t+1 ) ) ( 6 ) Step4 convergence check : 20 Our decoding models are the Viterbi algorithm on CRF ( ) , and the secondord',\n",
       " 'Feature weights were trained with Minimum Error-Rate Training ( MERT ) ( ) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit ( TARGETCIT . Experimental results are reported for the newstest2009 test set , a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens . 4.1 Stability An important difference be',\n",
       " 'ced corpora , and we encourage the evaluation of other methods on our data set . 4 Related Work There is plenty of research into metaphors . While many are mainly interested in their general properties ( TARGETCIT ; ) , we focus on the ones that evaluate their results empirically . use a similar approach to identify metaphors , but focus on frames . Their corpus is with about 900 instances relative',\n",
       " 'tarted speaking , to more complex responses such as emotional reactions to the content of what users are saying . The current demonstration extends our previous demonstration of incremental processing ( TARGETCIT in several important respects . First , it includes additional indicators , as described in ( DeOTHERCIT ) . Second , it is applied to a new domain , an extension of that presented in ( Pl¨uss et al. , 2011 ) . F',\n",
       " 'ecommendations based on the SPaRKy system ( ) , and has received high ratings in the past . SPaRKy uses global utterance features . • n-grams represents a simple 5-gram baseline that is similar to TARGETCIT ’ s system . We will sample from the most likely slot realisations that do not contain a repetition and include exactly the required slot values . Local context only is taken into account . 4.1 Human Rati',\n",
       " 'of L3M that is more expressive in that it is capable of considering mention-entity interactions using scores at the mention-pair granularity . We augment this model with a temperature-like parameter ( TARGETCIT to provide additional flexibility . CL3M augments L3M with knowledge-based constraints following ( ) . This capability is very desirable as shown by the success of the rule-based deterministic ap',\n",
       " 't , we use a one vs. all classification approach with a SVM classifier and tree kernels . The tree kernel is a function between two trees that computes a normalized similarity score in the range [ 0,1 ] ( TARGETCIT . For our task , we use an implementation of tree kernels for syntactic parse trees ( ) that is built on top of the SVM-Light library ( ) in a similar manner to that presented in ( OTHERCI',\n",
       " 'ions with the purpose of obtaining some priors to separate topics and C-expressions . In sentiment analysis , researchers have jointly modeled topics and sentiment words ( ; Titov and McOTHERCIT ; TARGETCIT ; ) . Our model is more related to the ME-LDA model in ( ) , which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words . We also use such a variable .',\n",
       " 'd , particularly when it is possibly to assign multiple labels per token as is the case here ( which we discuss in Section 3.4.3 ) . We apply two relatively simple strategies . We noted in Section 2.2 that TARGETCIT found that the semantic file was useful . This is the coarse lexicographic category label , elsewhere denoted supersense ( ) , which is the terminology we use . Nouns are divided into 26 coarse cat',\n",
       " 'specific hypothesis tests based on external verb classes and SCF inventories . The BioLexicon system extracts each verb instance ’ s GRs using the lexicalized Enju parser tuned to the biomedical domain ( TARGETCIT . Each unique GR-set considered a potential SCF , and an experimentally-determined threshold is used to filter low-frequency SCFs . Note that both methods require extensive manual work : the Preiss syste',\n",
       " 'st discriminatively-trained model that both makes joint decisions over an entire document and models specific entity-level properties , rather than simply enforcing transitivity of pairwise decisions ( TARGETCIT ; ) . We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties : synthetic oracle properties , entity phi features ( number , gender , animacy ,',\n",
       " 'riation in the profile of annotations for individuals , all of the annotators showed reasonable correlation on the graded task and at a level in excess of that achieved on other graded semantic tasks ( TARGETCIT . There will inevitably be one annotator that has the lowest correlation with the others on any given task , but we found that this was not the same annotator on every task . For example , C on WSsim-2 h',\n",
       " 'ing the comparison with the PDTB result , the 12-value structure is collapsed into 5 values : TEMPORAL , CONTINGENCY , COMPARISON , EXPANSION , and EntRel/NoRel , which must be different from the 5 values in TARGETCIT , judging from the descriptions . tation to only three specific situations so that most loose and/or hard-to-delimit relations across paragraph boundaries are excluded . This restriction appears to be q',\n",
       " 'ue to the number of embedded hypotheses , which prevents the use of bruteforce approaches . When using BLEU , or rather sentence-level approximations thereof , the problem is in fact known to be NP-hard ( TARGETCIT . This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path . Similar ( or worse',\n",
       " 'oint model , it is useful in that it is done before the actual translation process , so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses ( TARGETCIT . For tree-to-string systems , on the other hand , it is necessary to have available or create a decoder that is equipped with this functionality , which becomes a bottleneck in the research and developm',\n",
       " 'ble combinations , or each text paired with n others by random . Standard datasets for which 123 readers come pre-packaged include , among others , the SemEval-2012 STS data ( ) , the METER corpus ( TARGETCIT , or the RTE 1–5 data ( ) . As far as license terms allow redistribution , the datasets themselves are integrated into the framework . Similarity Scorer The Similarity Scorer allows to integrate a',\n",
       " 'Linguistics for content-scoring , is to rely on a variety of text similarity measurements to compare a response with either pre-defined correct answers or a group of responses rated with a high score ( TARGETCIT . Compared to the first group , such methods can bypass a labor intensive pattern-building step . A widely used approach to measuring text similarity between two text strings is to convert each text str',\n",
       " 'ce translation in a pairwise fashion . Unfortunately , we do not have access to the data used in those papers , so a direct comparison is not possible . Instead , we collected the English output of Moses ( TARGETCIT , using both French and German as source language , trained on the Europarl corpus used by WMT 2009.10 We also collected the output of Joshua ( ) trained on 500K sentences of GALE Chinese-Englis',\n",
       " 'o build model for main task . That is , for each discourse relation , we build a binary classifier . 5 Experiments and Results 5.1 Experiments Although previous work has been done on PDTB ( ) and ( TARGETCIT , we can not make a direct comparison with them because various experimental conditions , such as , different classification strategies ( multi-class classification , multiple binary classification ) , diffe',\n",
       " '. c�2012 Association for Computational Linguistics search on the topic has been done , including the use of statistical translations of sentences into many languages as features for supervised models ( TARGETCIT ; ) , and the projection of monolingual knowledge onto another language ( ) . Yet the above two goals , i.e. , disambiguating in an arbitrary language and using lexical and semantic knowled',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fb4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2f8123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:32<00:00,  3.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Creating BM25 fetcher\n",
    "bm25 = BM25Okapi(candidates)\n",
    "fetch_count = 10\n",
    "outputs = []\n",
    "\n",
    "for datapoint in tqdm.tqdm(test[0:10]):\n",
    "    query = datapoint['cite_context']\n",
    "    doc_scores = bm25.get_scores(query)\n",
    "    best_docs = np.ndarray.tolist(np.argsort(doc_scores)[:: -1][0: fetch_count])\n",
    "    outputs.append(best_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a02c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing Extractions\n",
    "\n",
    "def view(point):\n",
    "    global test, outputs, index\n",
    "    print('Query cite span: ' + str(test[point]['cite_span']))\n",
    "    print('Query cite context: ' + str(test[point]['cite_context']))\n",
    "    print('True citation: ' + str(test[point]['paper']['title']))\n",
    "    \n",
    "    for serial, item in enumerate(outputs[point]):\n",
    "        print('Fetched span ' + str(serial + 1) + ':')\n",
    "        print('Span: ' + index[item]['cite_span'])\n",
    "        print('Suggested Citation: ' + index[item]['paper']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b8ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query cite span: These measures have been shown to correlate best with human judgments in general , but among the automatic measures , ROUGE-1 and ROUGE-2 also correlate best with the Pyramid ( ; ) and Responsiveness manual metrics ( ) .\n",
      "Query cite context: retaining all stopwords . These measures have been shown to correlate best with human judgments in general , but among the automatic measures , ROUGE-1 and ROUGE-2 also correlate best with the Pyramid ( TARGETCIT ; ) and Responsiveness manual metrics ( ) . Moreover , ROUGE-1 has been shown to best reflect human-automatic summary comparisons ( ) . For single concept systems , the results are s\n",
      "True citation: Evaluating Content Selection in Summarization: The Pyramid Method\n",
      "Fetched span 1:\n",
      "Span: ROUGE variations ( ROUGE1 , ROUGE-2 , ROUGE-3 , ROUGE-4 ) ( and the AutoSummENG-MeMoG ( ) and NPowER ( ) methods were used to automatically evaluate the summarization systems .\n",
      "Suggested Citation: ROUGE: A Package for Automatic Evaluation of Summaries\n",
      "Fetched span 2:\n",
      "Span: We adopt the ROUGE-1 and ROUGE-SU4 metrics from ( , and also use ROUGE2 .\n",
      "Suggested Citation: Multi-Document Summarisation Using Generic Relation Extraction\n",
      "Fetched span 3:\n",
      "Span: The models used were , ROUGE variations ( ROUGE1 , ROUGE2 , ROUGE-SU4 ) ( , the MeMoG variation ( ) of AutoSummENG ( ) and NPowER ( ) .\n",
      "Suggested Citation: ROUGE: A Package for Automatic Evaluation of Summaries\n",
      "Fetched span 4:\n",
      "Span: claimed to have demonstrated that ROUGE correlates well with human summaries , both , and have cast doubt upon this .\n",
      "Suggested Citation: Evaluating Automatic Summaries of Meeting Recordings\n",
      "Fetched span 5:\n",
      "Span: f measures ROUGE-N ( N=1,2,3,4 ) , ROUGE-L , ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram , word-sequences , and word-pairs between the extract and the abstract summaries ( .\n",
      "Suggested Citation: ROUGE: A Package for Automatic Evaluation of Summaries\n",
      "Fetched span 6:\n",
      "Span: Shown are the ROUGE-2 and ROUGE SU-4 recalls with the default options from the ROUGE toolkit ( ) ; Pyramid scores ( ; and linguistic quality scores , scored between 1 ( very bad ) to 5 ( very good ) .\n",
      "Suggested Citation: Evaluating Content Selection in Summarization: The Pyramid Method\n",
      "Fetched span 7:\n",
      "Span: In all our experiments we used the ROUGE ( evaluation package and its ROUGE1 , ROUGE-2 , and ROUGE-SU4 recall scores .\n",
      "Suggested Citation: ROUGE: A Package for Automatic Evaluation of Summaries\n",
      "Fetched span 8:\n",
      "Span: We use Support Vector Machines ( ) with RBF kernel and order1 Conditional Random Fields ( ) — trained with the same features as ( to identify the summary-worthy tokens to include in the abstract .\n",
      "Suggested Citation: Summarizing Decisions in Spoken Meetings\n",
      "Fetched span 9:\n",
      "Span: We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE ( .\n",
      "Suggested Citation: ROUGE: A Package for Automatic Evaluation of Summaries\n",
      "Fetched span 10:\n",
      "Span: Combining L1 ( S ) and R1 ( S ) , our system outperforms the best system in DUC-04 significantly , and it also outperforms several recent systems , including a concept-based summarization approach ( , a sentence topic model based system ( ) , and our MMR-styled submodular system ( ) .\n",
      "Suggested Citation: Text Summarization Model based on Maximum Coverage Problem and its Variant\n"
     ]
    }
   ],
   "source": [
    "view(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591d2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
