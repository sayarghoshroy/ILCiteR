{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_gJUXFnD2SVvmydFVLRQiMWEjeQW00Ap",
      "authorship_tag": "ABX9TyO0FcIOtZRiNXdKMQgs2yMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anon/ILCiteR/blob/main/cite_reco_s2orc_gather_meta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTBGDUAhi92D"
      },
      "outputs": [],
      "source": [
        "# Course of action:\n",
        "# Gather papers from the metadata parses\n",
        "# Match papers in the PDF parses using their 'paper_id'\n",
        "\n",
        "import json\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ2eezZ-y5Z2",
        "outputId": "554b0e57-3398-45d9-a7be-3f9e2397547a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'drive/My Drive/cite_reco_s2orc/'"
      ],
      "metadata": {
        "id": "4iNYps3ajocB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names = ['summarization', 'machine translation', 'named entity recognition', 'sentiment analysis']\n",
        "topic_abbs = ['summ', 'mt', 'ner', 'sa']"
      ],
      "metadata": {
        "id": "UMYQFM-PjDPJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out/'\n",
        "look_for_file = 'found_paper_IDs.txt'\n",
        "topic_papers = [[], [], [], []]\n",
        "look_for_list = []\n",
        "\n",
        "def add_to_look_for_file(paper_ID):\n",
        "  global location, look_for_file\n",
        "  with open(location + out_dir + look_for_file, 'a+') as f:\n",
        "    f.write(str(paper_ID) + '\\n')\n",
        "  return\n",
        "\n",
        "def add_to_topic_metas(meta_record, topic_id):\n",
        "  global topic_abbs\n",
        "  with open(location + out_dir + topic_abbs[topic_id] + '.jsonl', 'a+') as f:\n",
        "    f.write(str(meta_record) + '\\n')\n",
        "  return"
      ],
      "metadata": {
        "id": "UZj-9HgAmja4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  return ' '.join(tokens).strip()\n",
        "\n",
        "def merge_dicts(a, b):\n",
        "    merged = {**a, **b}\n",
        "    return merged\n",
        "\n",
        "def process_paper(paper_meta):\n",
        "  global topic_names, topic_papers, look_for_list\n",
        "  if not paper_meta['has_pdf_parse']:\n",
        "    return False\n",
        "\n",
        "  if not paper_meta['abstract']:\n",
        "    return False\n",
        "\n",
        "  abstract_text = normalize_text(paper_meta['abstract'])\n",
        "  outcome = False\n",
        "\n",
        "  for index, topic in enumerate(topic_names):\n",
        "    if topic in abstract_text:\n",
        "      print('Topic:', topic)\n",
        "      print('Title:', paper_meta['title'])\n",
        "      print('Normalized Abstract:', abstract_text)\n",
        "      print()\n",
        "\n",
        "      topic_papers[index].append(paper_meta)\n",
        "      add_to_topic_metas(paper_meta, index)\n",
        "\n",
        "      look_for_list.append(paper_meta['paper_id'])\n",
        "      add_to_look_for_file(paper_meta['paper_id'])\n",
        "\n",
        "      outcome = True\n",
        "  return outcome\n",
        "\n",
        "def is_CS_paper(paper_meta):\n",
        "  if not paper_meta['mag_field_of_study']:\n",
        "    return False\n",
        "  if 'Computer Science' not in paper_meta['mag_field_of_study']:\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "R5QYmoNckJoG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_file_prefix = 'metadata_'\n",
        "\n",
        "line_count = 0\n",
        "successes = 0\n",
        "cs_papers = 0\n",
        "\n",
        "limit = 100000\n",
        "\n",
        "with open(location + metadata_file_prefix + '1.jsonl') as m:\n",
        "  while line_count < limit:\n",
        "    m_line = m.readline()\n",
        "    if not m_line:\n",
        "      break\n",
        "\n",
        "    line_count += 1\n",
        "    paper_meta = json.loads(m_line)\n",
        "\n",
        "    if not is_CS_paper(paper_meta):\n",
        "      continue\n",
        "\n",
        "    cs_papers += 1\n",
        "\n",
        "    if process_paper(paper_meta):\n",
        "      successes += 1\n",
        "\n",
        "print('Number of fetches:', successes)\n",
        "print('Number of CS papers: ' + str(cs_papers) + '/' +str(line_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QuguaVyjepd",
        "outputId": "2cdaa5c8-8256-4b05-99dc-ce4f39287516"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: machine translation\n",
            "Title: Towards conceptual generalization in the embedding space\n",
            "Normalized Abstract: humans are able to conceive physical reality by jointly learning different facets thereof . to every pair of notions related to a perceived reality may correspond a mutual relation , which is a notion on its own , but one-level higher . thus , we may have a description of perceived reality on at least two levels and the translation map between them is in general , due to their different content corpus , one-to-many . following success of the unsupervised neural machine translation models , which are essentially one-to-one mappings trained separately on monolingual corpora , we examine further capabilities of the unsupervised deep learning methods used there and apply some of these methods to sets of notions of different level and measure . using the graph and word embedding-like techniques , we build one-to-many map without parallel data in order to establish a unified vector representation of the outer world by combining notions of different kind into a unique conceptual framework . due to their latent similarity , by aligning the two embedding spaces in purely unsupervised way , one obtains a geometric relation between objects of cognition on the two levels , making it possible to express a natural knowledge using one description in the context of the other .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Meta-Learning for Few-Shot NMT Adaptation\n",
            "Normalized Abstract: we present meta-mt , a meta-learning approach to adapt neural machine translation ( nmt ) systems in a few-shot setting . meta-mt provides a new approach to make nmt models easily adaptable to many target domains with the minimal amount of in-domain data . we frame the adaptation of nmt systems as a meta-learning problem , where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks . we evaluate the proposed meta-learning strategy on ten domains with general large scale nmt systems . we show that meta-mt significantly outperforms classical domain adaptation when very few in-domain examples are available . our experiments shows that meta-mt can outperform classical fine-tuning by up to 2.5 bleu points after seeing only 4 , 000 translated words ( 300 parallel sentences ) .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning\n",
            "Normalized Abstract: many machine translation ( mt ) evaluation metrics have been shown to correlate better with human judgment than bleu . in principle , tuning on these metrics should yield better systems than tuning on bleu . however , due to issues such as speed , requirements for linguistic resources , and optimization difficulty , they have not been widely adopted for tuning . this paper presents port , a new mt evaluation metric which combines precision , recall and an ordering metric and which is primarily designed for tuning mt systems . port does not require external resources and is quick to compute . it has a better correlation with human judgment than bleu . we compare port-tuned mt systems to bleu-tuned baselines in five experimental conditions involving four language pairs . port tuning achieves consistently better performance than bleu tuning , according to four automated metrics ( including bleu ) and to human evaluation : in comparisons of outputs from 300 source sentences , human judges preferred the port-tuned output 45.3 % of the time ( vs. 32.7 % bleu tuning preferences and 22.0 % ties ) .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Perseus: An Interactive Large-Scale Graph Mining and Visualization Tool\n",
            "Normalized Abstract: given a large graph with several millions or billions of nodes and edges , such as a social network , how can we explore it efficiently and find out what is in the data ? in this demo we present perseus , a large-scale system that enables the comprehensive analysis of large graphs by supporting the coupled summarization of graph properties and structures , guiding attention to outliers , and allowing the user to interactively explore normal and anomalous node behaviors . : : : : : : specifically , perseus provides for the following operations : 1 ) it automatically extracts graph invariants ( e.g. , degree , pagerank , real eigenvectors ) by performing scalable , offline batch processing on hadoop ; 2 ) it interactively visualizes univariate and bivariate distributions for those invariants ; 3 ) it summarizes the properties of the nodes that the user selects ; 4 ) it efficiently visualizes the induced subgraph of a selected node and its neighbors , by incrementally revealing its neighbors . : : : : : : in our demonstration , we invite the audience to interact with perseus to explore a variety of multi-million-edge social networks including a wikipedia vote network , a friendship/foeship network in slashdot , and a trust network based on the consumer review website epinions.com .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Automatic Acquisition of Paraphrases Using Bilingual Dependency Relations\n",
            "Normalized Abstract: abstract ⎯ this letter introduces a new method to automatically acquire paraphrases using bilingual corpora . it utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language ’ s sentence based on statistical alignment techniques . since the proposed paraphrasing method can clearly disambiguate the sense of the original phrases using the bilingual context of dependency relations , it would be possible to obtain interchangeable paraphrases under a given context . through experiments with parallel corpora of korean and english language pairs , we demonstrate that our method effectively extracts paraphrases with high precision , achieving success rates of 94.3 % and 84.6 % , respectively , for korean and english . keywords ⎯ paraphrase , bilingual dependency parsing , alignment , sense disambiguation , dependency relation . i. introduction approaches based on bilingual corpora are promising for the automatic acquisition of translation knowledge . phrase-based statistical machine translation ( smt ) models have advanced the state of the art in machine translation by expanding the basic unit from words to phrases [ 1 ] , [ 2 ] . however , phrase-based smt techniques suffer from data sparseness problems , such as unreliable translation probabilities of low-frequency phrases and low coverage , in that many phrases encountered at run-time are not observed in the training data . an alternative to these problems is to use paraphrases . in this study , we introduce a method of automatically acquiring paraphrases to smooth the translation parameters and to increase the coverage of translation knowledge . one previous approach\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Sentence-level sentiment analysis in Czech\n",
            "Normalized Abstract: this paper presents initial research in the area of sentiment analysis in czech . we will describe a method for annotating czech evaluative structures and analyze existing results for both manual and automatic annotation of the plain text data which represents the basis for further subjectivity clues learning .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Detection and Summarization of Salient Events in Coastal Environments\n",
            "Normalized Abstract: the monitoring of coastal environments is of great interest to biologists and environmental protection organizations with video cameras being the dominant sensing modality . however , it is recognized that video analysis of maritime scenes is very challenging on account of background animation ( water reflections , waves ) and very large field of view . we propose a practical approach to the detection of three salient events , namely boats , motor vehicles and people appearing close to the shoreline , and their subsequent summarization . our approach consists of three fundamental steps : region-of-interest ( roi ) localization by means of behavior subtraction , roi validation by means of feature-covariance-based object recognition , and event summarization by means of video condensation . the goal is to distill hours of video data down to a few short segments containing only salient events , thus allowing human operators to expeditiously study a coastal scene . we demonstrate the effectiveness of our approach on long videos taken at great point , nantucket , massachusetts .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Multiword Expression Translation Using Generative Dependency Grammar\n",
            "Normalized Abstract: the multi-word expressions ( mwe ) treatment is a very difficult problem for the natural language processing in general and for machine translation in particular . this is true because each word of a mwe can have a specific meaning but the expression can have a totally different meaning both in source and in target language of a translation . the things are complicated also by the fact that the source expression can appear in the source text under a very different form from its form in a bilingual mwe dictionary ( it can have some inflections ) and , most of all , it can have some extensions ( some mwe words can have associated new words that do not belong to the mwe ) . the paper show how this kind of problems can be treated and solved using generative dependency grammar with features .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation\n",
            "Normalized Abstract: minimum error rate training ( mert ) is a widely used training procedure for statistical machine translation . a general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions . this paper introduces coordinate system selection ( rss ) into the search algorithm for mert . contrary to previous approaches in which every dimension only corresponds to one independent feature function , we create several coordinate systems by moving one of the dimensions to a new direction . the basic idea is quite simple but critical that the training procedure of mert should be based on a coordinate system formed by search directions but not directly on feature functions . experiments show that by selecting coordinate systems with tuning set results , better results can be obtained without any other language knowledge .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Graph-based Interactive Data Federation System for Heterogeneous Data Retrieval and Analytics\n",
            "Normalized Abstract: given the increasing number of heterogeneous data stored in relational databases , file systems or cloud environment , it needs to be easily accessed and semantically connected for further data analytic . the potential of data federation is largely untapped , this paper presents an interactive data federation system ( https : //vimeo.com/319473546 ) by applying large-scale techniques including heterogeneous data federation , natural language processing , association rules and semantic web to perform data retrieval and analytics on social network data . the system first creates a virtual database ( vdb ) to virtually integrate data from multiple data sources . next , a rdf generator is built to unify data , together with sparql queries , to support semantic data search over the processed text data by natural language processing ( nlp ) . association rule analysis is used to discover the patterns and recognize the most important co-occurrences of variables from multiple data sources . the system demonstrates how it facilitates interactive data analytic towards different application scenarios ( e.g. , sentiment analysis , privacy-concern analysis , community detection ) .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: SemEval-2015 Task 12: Aspect Based Sentiment Analysis\n",
            "Normalized Abstract: semeval-2015 task 12 , a continuation of semeval-2014 task 4 , aimed to foster research beyond sentenceor text-level sentiment classification towards aspect based sentiment analysis . the goal is to identify opinions expressed about specific entities ( e.g. , laptops ) and their aspects ( e.g. , price ) . the task provided manually annotated reviews in three domains ( restaurants , laptops and hotels ) , and a common evaluation procedure . it attracted 93 submissions from 16 teams .\n",
            "\n",
            "Topic: named entity recognition\n",
            "Title: Arabic Named Entity Recognition using Optimized Feature Sets\n",
            "Normalized Abstract: the named entity recognition ( ner ) task has been garnering significant attention in nlp as it helps improve the performance of many natural language processing applications . in this paper , we investigate the impact of using different sets of features in two discriminative machine learning frameworks , namely , support vector machines and conditional random fields using arabic data . we explore lexical , contextual and morphological features on eight standardized data-sets of different genres . we measure the impact of the different features in isolation , rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set . our system yields a performance of fβ=1-measure=83.5 on ace 2003 broadcast news data .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Topic Augmented Generator for Abstractive Summarization\n",
            "Normalized Abstract: steady progress has been made in abstractive summarization with attention-based sequence-to-sequence learning models . in this paper , we propose a new decoder where the output summary is generated by conditioning on both the input text and the latent topics of the document . the latent topics , identified by a topic model such as lda , reveals more global semantic information that can be used to bias the decoder to generate words . in particular , they enable the decoder to have access to additional word co-occurrence statistics captured at document corpus level . we empirically validate the advantage of the proposed approach on both the cnn/daily mail and the wikihow datasets . concretely , we attain strongly improved rouge scores when compared to state-of-the-art models .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Hierarchical Modeling and Adaptive Clustering for Real-Time Summarization of Rush Videos\n",
            "Normalized Abstract: in this paper , we provide detailed descriptions of a proposed new algorithm for video summarization , which are also included in our submission to trecvid'08 on bbc rush summarization . firstly , rush videos are hierarchically modeled using the formal language technique . secondly , shot detections are applied to introduce a new concept of v-unit for structuring videos in line with the hierarchical model , and thus junk frames within the model are effectively removed . thirdly , adaptive clustering is employed to group shots into clusters to determine retakes for redundancy removal . finally , each most representative shot selected from every cluster is ranked according to its length and sum of activity level for summarization . competitive results have been achieved to prove the effectiveness and efficiency of our techniques , which are fully implemented in the compressed domain . our work does not require high-level semantics such as object detection and speech/audio analysis which provides a more flexible and general solution for this topic .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Semantic based Chinese sentence sentiment analysis\n",
            "Normalized Abstract: a sentiment analysis ( sa ) method for chinese is presented in this paper . first , natural language processing ( nlp ) technique is used to analyze the interdependence relationship between components of the sentence , then the syntax components are detected which affect the sentence sentiment . finally , a sentence sentiment is calculated . the sentiment of some terms is variable . to improve the precision and recall of our method , domain ontology is used to analyze the sentiment of those terms .\n",
            "\n",
            "Topic: named entity recognition\n",
            "Title: Inducing Gazetteer for Chinese Named Entity Recognition Based on Local High-Frequent Strings\n",
            "Normalized Abstract: gazetteers , or entity dictionaries , are important for named entity recognition ( ner ) . although the dictionaries extracted automatically by the previous methods from a corpus , web or wikipedia are very huge , they also misses some entities , especially the domain-specific entities . we present a novel method of automatic entity dictionary induction , which is able to construct a dictionary more specific to the processing text at a much lower computational cost than the previous methods . it extracts the local high-frequent strings in a document as candidate entities , and filters the invalid candidates with the accessor variety ( av ) as our entity criterion . the experiments show that the obtained dictionary can effectively improve the performance of a high-precision baseline of ner .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Insights from CL-SciSumm 2016: the faceted scientific document summarization Shared Task\n",
            "Normalized Abstract: we describe the participation and the official results of the 2nd computational linguistics scientific summarization shared task ( cl-scisumm ) , held as a part of the birndl workshop at the joint conference for digital libraries 2016 in newark , new jersey . cl-scisumm is the first medium-scale shared task on scientific document summarization in the computational linguistics ( cl ) domain . participants were provided a training corpus of 30 topics , each comprising of a reference paper ( rp ) and 10 or more citing papers , all of which cite the rp . for each citation , the text spans ( i.e. , citances ) that pertain to the rp have been identified . participants solved three sub-tasks in automatic research paper summarization using this text corpus . fifteen teams from six countries registered for the shared task , of which ten teams ultimately submitted and presented their results . the annotated corpus comprised 30 target papers -- currently the largest available corpora of its kind . the corpus is available for free download and use at https : //github.com/wing-nus/scisumm-corpus .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Exploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing\n",
            "Normalized Abstract: downstream processing of machine translation ( mt ) output promises to be a solution to improve translation quality , especially when the mt system ’ s internal decoding process is not accessible . both rule-based and statistical automatic postediting ( ape ) methods have been proposed over the years , but with contrasting results . a missing aspect in previous evaluations is the assessment of different methods : i ) under comparable conditions , and ii ) on different language pairs featuring variable levels of mt quality . focusing on statistical ape methods ( more portable across languages ) , we propose the first systematic analysis of two approaches . to understand their potential , we compare them in the same conditions over six language pairs having english as source . our results evidence consistent improvements on all language pairs , a relation between the extent of the gain and mt output quality , slight but statistically significant performance differences between the two methods , and their possible complementarity .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: A Sentiment Analysis Approach based on Arabic Social Media Platforms\n",
            "Normalized Abstract: apart from the major outstanding research issues facing arabic social media sentiment analysis which includes handling of vernacular arabic , slang vocabulary and shorthand writings . there is also a lack of comprehensive framework for arabic social media sentiment analysis as existing works often focus on particular platforms ( like twitter and facebook ) . as such , models developed on one platform often perform poorly on other platforms due to lack of a representative feature space . to this regard we adopted a comprehensive approach utilizing a broad array of arabic social media platforms to establish more generalized sentiment models using random subspace ensembles of mlp base learners . more importantly , we introduced a new sentiment classification scale and we classified sentiments as highly positive ( hp ) , fairly positive ( fp ) , no sentiment ( ns ) , fairly negative ( fn ) and highly negative ( hn ) . the approach has been tested in a series of experiments and the results demonstrate significant improvements in terms of both classification accuracy and generalizing ability .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Sentiment Analysis Using Autoregressive Language Modeling and Broad Learning System\n",
            "Normalized Abstract: in the era of big data , mining the emotional tendency of opinions through natural language processing technology is meaningful for the timely understanding of human behavior data . nowadays , generalized autoregressive pre-training language modeling ( xlnet ) can not only capture bidirectional contextual knowledge but also learn the word dependency . however , its sentence-level representation did n't take broad features into account . in this paper , we design a novel architecture , called broad autoregressive language model ( broxlnet ) , to automatically process sentiment analysis task . broxlnet integrates the advantage of generalized autoregressive language modeling and broad learning system , which has the ability of extracting deep contextual features and randomly searching high-level contextual representation in broad spaces . we evaluated our algorithm on binary stanford sentiment treebank dataset . compared with the state-of-the-art methods , e.g. , bilstm , elmo , openai gpt , bert and xlnet , broxlnet achieved the best result of 94.0 % in sentiment analysis task of binary stanford sentiment treebank . the result demonstrated the excellent classifying ability of broxlnet in sentiment analysis .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Summarizing Meeting Transcripts Based on Functional Segmentation\n",
            "Normalized Abstract: in this paper , we aim to improve meeting summarization performance using discourse specific information . since there are intrinsically different characteristics in utterances in different types of function segments , e.g. , monologue segments versus discussion ones , we propose a new summarization framework where different summarizers are used for different segment types . for monologue segments , we adopt the integer linear programming-based summarization method ; whereas for discussion segments , we use a graph-based method to incorporate speaker information . performance of our proposed method is evaluated using the standard ami meeting corpus . results show a good improvement over previous state-of-the-art algorithms according to various evaluation metrics and different compress ratios .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: The NICT/ATR Speech Translation System for IWSLT 2008\n",
            "Normalized Abstract: this paper describes the national institute of information and communications technology/advanced telecommunications research institute international ( nict/atr ) statistical machine translation ( smt ) system used for the iwslt 2008 evaluation campaign . we participated in the chinese‐ english ( challenge task ) , english‐chinese ( challenge task ) , chinese‐english ( btec task ) , chinese‐spanish ( btec task ) , and chinese‐english‐spanish ( pivot task ) translation tasks . in the english‐chinese translation challenge task , we focused on exploring various factors for the english‐chinese translation because the research on the translation of english‐chinese is scarce compared to the opposite direction . in the chinese‐english translation challenge task , we employed a novel clustering method , where training sentences similar to the development data in terms of the word error rate formed a cluster . in the pivot translation task , we integrated two strategies for pivot translation by linear interpolation .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Applying Statistical Methods To Machine Translation\n",
            "Normalized Abstract: the goal of our project is to demonstrate the effectiveness of statistical techniques in machine translation by improving the state of the art in large-vocabulary french-to-english translation .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: The 2006 LIMSI Statistical Machine Translation System for TC-STAR\n",
            "Normalized Abstract: this paper presents the limsi statistical machine translation system developed for 2006 t c-star evaluation campaign . we describe an a * -decoder that generates translation lattices using a word-based translation model . a lattice is a rich and compact representation of alternative translations that includes the probability scores of all the involved sub-models . these lattices are then used in subsequent processing steps , in particular to perform sentence splitting and joining , maximum bleu training and to use improved statistical target language models .\n",
            "\n",
            "Topic: machine translation\n",
            "Title: Exploiting the Translation Context for Multilingual WSD\n",
            "Normalized Abstract: we propose a strategy to support word sense disambiguation ( wsd ) which is designed specifically for multilingual applications , such as machine translation . co-occurrence information extracted from the translation context , i.e. , the set of words which have already been translated , is used to define the order in which disambiguation rules produced by a machine learning algorithm are applied . experiments on the english-portuguese translation of seven verbs yielded a significant improvement on the accuracy of a rule-based model : from 0.75 to 0.79 .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Near-lossless semantic video summarization and its applications to video analysis\n",
            "Normalized Abstract: the ever increasing volume of video content on the web has created profound challenges for developing efficient indexing and search techniques to manage video data . conventional techniques such as video compression and summarization strive for the two commonly conflicting goals of low storage and high visual and semantic fidelity . with the goal of balancing both video compression and summarization , this article presents a novel approach , called near-lossless semantic summarization ( nlss ) , to summarize a video stream with the least high-level semantic information loss by using an extremely small piece of metadata . the summary consists of compressed image and audio streams , as well as the metadata for temporal structure and motion information . although at a very low compression rate ( around f of h.264 baseline , where traditional compression techniques can hardly preserve an acceptable visual fidelity ) , the proposed nlss still can be applied to many video-oriented tasks , such as visualization , indexing and browsing , duplicate detection , concept detection , and so on . we evaluate the nlss on trecvid and other video collections , and demonstrate that it is a powerful tool for significantly reducing storage consumption , while keeping high-level semantic fidelity .\n",
            "\n",
            "Topic: summarization\n",
            "Title: Survey on Query Estimation in Data Streams\n",
            "Normalized Abstract: query estimation plays an important role in query optimization by choosing a particular query plan . performing query estimation becomes quite challenging in case of fast , continuous , online data streams . different summarization methods like sampling , histograms , wavelets , sketches , discrete cosine series etc . are used to store data distribution for query estimation . in this paper a brief survey of query estimation techniques in view of data streams is presented .\n",
            "\n",
            "Topic: summarization\n",
            "Title: A bag-of-importance model for video summarization\n",
            "Normalized Abstract: in this paper , we propose a novel local feature based approach , namely bag-of-importance ( boi ) model , for static video summarization , while most of the existing approaches characterize each video frame with global features to derive the importance of each frame . since local features such as interest points are more discriminative in characterizing visual content , we formulate static video summarization as a problem of identifying representative frames which contain more important local features , where the representativeness of each frame is the aggregation of the importance of the local features contained in the frame . in order to derive the importance of each local feature for a given video , we employ sparse coding to project each local feature into a sparse space , calculate the l2 norm of the sparse coefficients for each local feature , and generate the boi representation with the distribution of the importance over all the local features in the video . we further take the perceptual difference among spatial regions of a frame into account , a spatial weighting template is utilized to differentiate the importance of local features for the individual frames . with the proposed video summarization scheme , both the inter-frame and intra-frame properties of local features are exploited , which allows the selected frames capture both the dominant content and discriminative details within a video . experimental results on a dataset across several genres demonstrate that the proposed approach clearly outperforms the state-of-the-art method .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Are tweets the real estimators of election results?\n",
            "Normalized Abstract: a medium with a huge amount of information like twitter , can be analyzed for surveying the opinions or sentiments of the public towards future leaders . this paper focuses on predicting the winning party in up elections with the help of public opinion on twitter . the paper intends to understand whether tweets can be used as useful method in predicting the election results or is it just a social hype . twitter api is used to extract tweets about the up elections . public opinion is then ascertained with the help of sentiment analysis on the tweets . after sufficient amount of tweets are collected for analysis , we are simply expanding on positive tweets and getting 45.13 % for bjp , the highest percentage as compared to other parties . on comparing this result with the exit polls and the actual election results , the prediction of winning party by the twitter data analysis is correct .\n",
            "\n",
            "Topic: sentiment analysis\n",
            "Title: Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming\n",
            "Normalized Abstract: we treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task , without relying on any direct tree supervision . our approach relies on gumbel perturbations and differentiable dynamic programming . unlike previous approaches to latent tree learning , we stochastically sample global structures and our parser is fully differentiable . we illustrate its effectiveness on sentiment analysis and natural language inference tasks . we also study its properties on a synthetic structure induction task . ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees .\n",
            "\n",
            "Number of fetches: 30\n",
            "Number of CS papers: 8943/100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That's it"
      ],
      "metadata": {
        "id": "XmaK45rNqS02"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}