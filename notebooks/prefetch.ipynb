{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HFm0BagMHeITRRrhm-foS85YY0lfNg1l",
      "authorship_tag": "ABX9TyOoXLC3NeoRxZajzdKt82b+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anon/ILCiteR/blob/main/prefetch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oBH4SQ3YTEn0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install rank_bm25\n",
        "\n",
        "import json\n",
        "import tqdm\n",
        "from rank_bm25 import BM25Okapi, BM25Plus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'drive/My Drive/cite_reco_s2orc/full/'\n",
        "maps_loc = 'maps/'\n",
        "map_types = ['Test/', 'Database/']\n",
        "dump_loc = 'experiments/prefetch/'\n",
        "\n",
        "domains = ['ner', 'sa', 'summ', 'mt']\n",
        "test_counts = [200, 500, 1000]"
      ],
      "metadata": {
        "id": "JcYtnmzingBN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefetch_count = 50\n",
        "test_count_ID = 1\n",
        "select_algo = 1\n",
        "algo = ['BM25Okapi', 'BM25Plus'][select_algo]\n",
        "\n",
        "def get_database(domain):\n",
        "  global location, maps_loc, map_types\n",
        "\n",
        "  with open(location + maps_loc + map_types[1] + domain + '.json', 'r+') as f:\n",
        "    database = json.load(f)\n",
        "\n",
        "  return database\n",
        "\n",
        "def get_test_set(domain):\n",
        "  global location, maps_loc, map_types, test_counts, test_count_ID\n",
        "\n",
        "  with open(location + maps_loc + map_types[0] + domain + '_' + str(test_counts[test_count_ID]) + '.json', 'r+') as f:\n",
        "    test_set = json.load(f)\n",
        "\n",
        "  return test_set\n",
        "\n",
        "def dump_prefetch(domain, testcase_prefetched):\n",
        "  global location, maps_loc, map_types, test_counts, test_count_ID, algo\n",
        "\n",
        "  with open(location + dump_loc + domain + '_' + str(test_counts[test_count_ID]) + '_' + algo + '.json', 'w+') as f:\n",
        "    json.dump(testcase_prefetched, f)\n",
        "\n",
        "  return\n",
        "\n",
        "def tokenize(text):\n",
        "  # Input text was already tokenized using nltk.word_tokenize\n",
        "  return text.split(' ')\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "  return [tokenize(text) for text in texts]\n",
        "\n",
        "def make_candidate_texts(database):\n",
        "  candidate_texts = []\n",
        "  for context in database.keys():\n",
        "    candidate_texts.append(context)\n",
        "  return candidate_texts\n",
        "\n",
        "def create_BM25_model(candidate_texts):\n",
        "  global select_algo\n",
        "  if select_algo == 0:\n",
        "    bm25 = BM25Okapi(candidate_texts)\n",
        "  else:\n",
        "    bm25 = BM25Plus(candidate_texts)\n",
        "  return bm25\n",
        "\n",
        "def get_top_BM25_scores(query_context, bm25_model):\n",
        "  global prefetch_count\n",
        "  doc_scores = bm25_model.get_scores(query_context)\n",
        "  scores_indices = []\n",
        "  for index, score in enumerate(doc_scores):\n",
        "    scores_indices.append([score, index])\n",
        "  scores_indices = sorted(scores_indices, reverse = True)\n",
        "  return scores_indices[0 : prefetch_count]\n",
        "\n",
        "def run_prefetch(test_set, candidate_texts):\n",
        "  bm25_model = create_BM25_model(candidate_texts)\n",
        "  test_set_BM25_scores = []\n",
        "  for datapoint in tqdm.tqdm(test_set):\n",
        "    test_context = datapoint[0]\n",
        "    scores_indices = get_top_BM25_scores(tokenize(test_context), bm25_model)\n",
        "    test_set_BM25_scores.append(scores_indices)\n",
        "  tqdm.tqdm.write('')\n",
        "  tqdm.tqdm._instances.clear()\n",
        "  return test_set_BM25_scores"
      ],
      "metadata": {
        "id": "dnDgLtgLpLB0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_unit_test = True\n",
        "\n",
        "if run_unit_test:\n",
        "  database = get_database('ner')\n",
        "  test_set = get_test_set('ner')\n",
        "  candidate_texts = make_candidate_texts(database)\n",
        "  tokenized_candidate_texts = tokenize_texts(candidate_texts)\n",
        "  test_set_prefetched = run_prefetch(test_set, tokenized_candidate_texts)\n",
        "  dump_prefetch('ner', test_set_prefetched)\n",
        "\n",
        "  print('Testcase[0]:')\n",
        "  print(test_set[0])\n",
        "  print('Extracted_Evidence[0]:')\n",
        "  index = test_set_prefetched[0][0][1]\n",
        "  print(candidate_texts[index])\n",
        "  print('Suggested_Papers[0]:')\n",
        "  print(database[candidate_texts[index]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xW7OFMiq3-w",
        "outputId": "de0baffe-6699-49a5-b2ab-1c134d2acfd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testcase[0]:\n",
            "['and word embeddings from word2vec', [[{'title': 'Efficient estimation of word representations in vector space', 'authors': [{'first': 'T', 'middle': [], 'last': 'Mikolov', 'suffix': ''}, {'first': 'K', 'middle': [], 'last': 'Chen', 'suffix': ''}, {'first': 'G', 'middle': [], 'last': 'Corrado', 'suffix': ''}, {'first': 'J', 'middle': [], 'last': 'Dean', 'suffix': ''}], 'year': 2013, 'venue': '', 'link': '5959482'}, 1]]]\n",
            "Extracted_Evidence[0]:\n",
            "Character embeddings , character bigram embeddings and word embeddings are pretrained separately using word2vec\n",
            "Suggested_Papers[0]:\n",
            "[[{'title': 'Efficient estimation of word representations in vector space', 'authors': [{'first': 'Tomas', 'middle': [], 'last': 'Mikolov', 'suffix': ''}, {'first': 'Kai', 'middle': [], 'last': 'Chen', 'suffix': ''}, {'first': 'Greg', 'middle': [], 'last': 'Corrado', 'suffix': ''}, {'first': 'Jeffrey', 'middle': [], 'last': 'Dean', 'suffix': ''}], 'year': 2013, 'venue': '', 'link': '5959482'}, 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_domains = domains[1:]\n",
        "\n",
        "for domain in run_domains:\n",
        "  tqdm.tqdm.write('Domain: ' + str(domain))\n",
        "  tqdm.tqdm._instances.clear()\n",
        "  database = get_database(domain)\n",
        "  test_set = get_test_set(domain)\n",
        "  candidate_texts = make_candidate_texts(database)\n",
        "  tokenized_candidate_texts = tokenize_texts(candidate_texts)\n",
        "  test_set_prefetched = run_prefetch(test_set, tokenized_candidate_texts)\n",
        "  dump_prefetch(domain, test_set_prefetched)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZanLslr2nUx",
        "outputId": "06da5622-3e1b-4b13-a181-18ae99243f69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain: sa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [05:00<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Domain: summ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [06:50<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Domain: mt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [09:52<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That's it"
      ],
      "metadata": {
        "id": "cirC9iGX9PYU"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}