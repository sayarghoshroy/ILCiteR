{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SpChUNpqT7dVlENRv7Bmu0JEOAWI8ksX",
      "authorship_tag": "ABX9TyPkpvLBRZXPBUVhn4GZ2gDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anon/ILCiteR/blob/main/create_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VuqbWa7sfXsC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import tqdm\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.seed('1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfEdVpDFwpXE",
        "outputId": "738a491e-a60b-4741-bc00-bac83135aee6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'drive/My Drive/cite_reco_s2orc/full/'\n",
        "maps_loc = 'maps/'\n",
        "map_types = ['Database/', 'Eval/']\n",
        "test_loc = 'Test/'\n",
        "\n",
        "domains = ['ner', 'sa', 'summ', 'mt']"
      ],
      "metadata": {
        "id": "YvfLv2kJiIZI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count = 200\n",
        "\n",
        "def get_mappings_from_file(domain, map_type = 0):\n",
        "  # map_type: 0 for 'Database', 1 for 'Eval'\n",
        "  global location, maps_loc, map_types\n",
        "\n",
        "  with open(location + maps_loc + map_types[map_type] + domain + '.json', 'r+') as f:\n",
        "    map = json.load(f)\n",
        "\n",
        "  return map\n",
        "\n",
        "def dump_test_set(domain, test_set):\n",
        "  global location, maps_loc, text_loc, test_count\n",
        "\n",
        "  with open(location + maps_loc + test_loc + domain + '_' + str(test_count) + '.json', 'w+') as f:\n",
        "    json.dump(test_set, f)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "V-ZJdM4LiQPL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_unit_test = False\n",
        "\n",
        "# Unit test using 'ner' domain\n",
        "if run_unit_test:\n",
        "  database = get_mappings_from_file('ner', 0)\n",
        "  eval = get_mappings_from_file('ner', 1)"
      ],
      "metadata": {
        "id": "eqsK_lBwknee"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate contexts\n",
        "with open('./stopwords.txt', 'r') as f:\n",
        "  stopwords = f.read().split('\\n')\n",
        "\n",
        "bad_tokens = []\n",
        "for code in range(128):\n",
        "  bad_tokens.append(chr(code))\n",
        "\n",
        "for punct in string.punctuation:\n",
        "  bad_tokens.append(punct)\n",
        "\n",
        "bad_tokens += stopwords\n",
        "bad_tokens = set(bad_tokens)\n",
        "\n",
        "def validate_context(text):\n",
        "  # If all tokens within context are in bad_tokens (stopword OR punctuation or random characters)\n",
        "  # Then Invalidate\n",
        "  global bad_tokens\n",
        "\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  for token in tokens:\n",
        "    if token not in bad_tokens:\n",
        "      return True\n",
        "\n",
        "  return False"
      ],
      "metadata": {
        "id": "4V3Shy7Yv9FG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_ref_papers_list(database):\n",
        "  all_papers = []\n",
        "\n",
        "  for context in tqdm.tqdm(database.keys()):\n",
        "    papers = database[context]\n",
        "    for paper in papers:\n",
        "      paper_dict = paper[0]\n",
        "      support = paper[1]\n",
        "\n",
        "      found_at = -1\n",
        "      for index, candidate in all_papers:\n",
        "        if candidate == paper_dict:\n",
        "          found_at = index\n",
        "          break\n",
        "\n",
        "      if found_at != -1:\n",
        "        all_papers[found_at][1] += support\n",
        "      else:\n",
        "        all_papers.append([paper_dict, support])\n",
        "\n",
        "  tqdm.tqdm._instances.clear()\n",
        "  return all_papers\n",
        "\n",
        "if run_unit_test:\n",
        "  database_papers = construct_ref_papers_list(database)"
      ],
      "metadata": {
        "id": "Me3Opl16krZ7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paper_list_doability_score(paper_list):\n",
        "  score = 0\n",
        "  global database_papers\n",
        "\n",
        "  for candi_paper in paper_list:\n",
        "    for exists_paper in database_papers:\n",
        "      if exists_paper[0] == candi_paper[0]:\n",
        "        score = max(score, exists_paper[1])\n",
        "        break\n",
        "\n",
        "  return score\n",
        "\n",
        "def get_eval_point_scores():\n",
        "  global eval\n",
        "  eval_datapoints_with_scores = []\n",
        "  for context in tqdm.tqdm(list(eval.keys())):\n",
        "    paper_list = eval[context]\n",
        "    if not validate_context(context):\n",
        "      continue\n",
        "    score = get_paper_list_doability_score(paper_list)\n",
        "    eval_datapoints_with_scores.append([context, paper_list, score])\n",
        "\n",
        "  tqdm.tqdm._instances.clear()\n",
        "  return eval_datapoints_with_scores\n",
        "\n",
        "def make_test_set():\n",
        "  global test_count\n",
        "  eval_datapoints_with_scores = get_eval_point_scores()\n",
        "  sorted_datapoints = sorted(eval_datapoints_with_scores, key = lambda item: item[2], reverse = True)\n",
        "  return [item[0 : 2] for item in sorted_datapoints[0 : test_count]]"
      ],
      "metadata": {
        "id": "E80wRMnVksht"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit Test\n",
        "\n",
        "if run_unit_test:\n",
        "  test_set = make_test_set()\n",
        "  print('', flush = True)\n",
        "  print('test_set[0]:', flush = True)\n",
        "  # Highest doability score\n",
        "  print(test_set[0], flush = True)\n",
        "  dump_test_set('ner', test_set)"
      ],
      "metadata": {
        "id": "7KRezQImq-Yz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test sets for each domain\n",
        "\n",
        "run_domains = ['mt']\n",
        "\n",
        "for domain in run_domains:\n",
        "  print('Domain: ' + str(domain), flush = True)\n",
        "  database = get_mappings_from_file(domain, 0)\n",
        "  eval = get_mappings_from_file(domain, 1)\n",
        "\n",
        "  print('- Creating list of papers within the Database', flush = True)\n",
        "  database_papers = construct_ref_papers_list(database)\n",
        "\n",
        "  print('', flush = True)\n",
        "  print('- Fetching doability scores for contexts in the Evaluation set', end = '', flush = True)\n",
        "  test_set = make_test_set()\n",
        "  random.shuffle(test_set)\n",
        "  dump_test_set(domain, test_set)\n",
        "  print('', flush = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX6KkmOn0yxZ",
        "outputId": "78ae4cc3-f56e-46c8-a91c-79de0c7b9113"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain: mt\n",
            "- Creating list of papers within the Database\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 108692/108692 [45:28<00:00, 39.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Fetching doability scores for contexts in the Evaluation set"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 6139/6139 [08:34<00:00, 11.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That's it"
      ],
      "metadata": {
        "id": "0JptTvoU4yEV"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}