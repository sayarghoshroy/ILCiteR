{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tlZlQvEweTENm6Ez1LobgSTPH5s6K8RW",
      "authorship_tag": "ABX9TyNXFzE8pyTRlyhSKnSWSad8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anon/ILCiteR/blob/main/cite_reco_s2orc_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LlwsW5IqBcH8"
      },
      "outputs": [],
      "source": [
        "# Utility functions to create refined context to BIBREF mappings from PDF parses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import tqdm\n",
        "import string\n",
        "import spacy\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initializing spaCy's text features extractor\n",
        "proc = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "BODZL6ndBrlQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTT_q0X-B8_Y",
        "outputId": "13f0e150-f2e2-4351-e484-703fb926e7cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'drive/My Drive/cite_reco_s2orc/full/'\n",
        "parses_loc = 'domain_parses/'\n",
        "mappings_loc = 'mappings/'\n",
        "split_locs = ['Database/', 'Eval/']\n",
        "\n",
        "all_domains = ['ner', 'sa', 'summ', 'mt']\n",
        "domain_codes = [all_domains[3]]"
      ],
      "metadata": {
        "id": "o3Gr8pL2B-UY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  return ' '.join(tokens).strip()"
      ],
      "metadata": {
        "id": "fn44K1K2CAxT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction Procedure:\n",
        "# Create a list of section texts where cite span text has been replaced with the BIBREF#\n",
        "# Tokenize texts into sentences\n",
        "# If the sentence contains atleast one BIBREF:\n",
        "#   Group consecutive BIBREFs to create BIBGROUPS\n",
        "#   Fetch all mappings from coarse and fine contexts to specific BIBGROUPS (using functions in refine_context)\n",
        "#   Add all mappings to a database"
      ],
      "metadata": {
        "id": "fFVw7IvOFX40"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading parses\n",
        "domain = domain_codes[0]\n",
        "\n",
        "def get_parses_from_file(domain, split_type):\n",
        "  # split_type is either 0 for Database or 1 for Eval\n",
        "  global location, parses_loc, split_locs\n",
        "\n",
        "  with open(location + parses_loc + split_locs[split_type] + domain + '.json', 'r+') as f:\n",
        "    parses = json.load(f)\n",
        "\n",
        "  return parses\n",
        "\n",
        "def dump_mappings(domain, split_type, mappings):\n",
        "  # split_type is either 0 for Database or 1 for Eval\n",
        "  global location, parses_loc, split_locs\n",
        "\n",
        "  with open(location + mappings_loc + split_locs[split_type] + domain + '.json', 'w+') as f:\n",
        "    json.dump(mappings, f)\n",
        "\n",
        "  return\n",
        "\n",
        "parses = get_parses_from_file(domain, 1)"
      ],
      "metadata": {
        "id": "ZnQObtZJKD7m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the list of section texts\n",
        "\n",
        "def get_section_texts(paper):\n",
        "  section_texts = []\n",
        "\n",
        "  for section in paper['body_text']:\n",
        "    if not section['cite_spans'] or section['cite_spans'] == []:\n",
        "      continue\n",
        "    text = section['text']\n",
        "    spans = sorted(section['cite_spans'], key = lambda unit: unit['start'], reverse = True)\n",
        "    for cite_block in spans:\n",
        "      text = text[: cite_block['start']] + cite_block['ref_id'] + '#' + text[cite_block['end'] :]\n",
        "    section_texts.append(text)\n",
        "\n",
        "  return section_texts"
      ],
      "metadata": {
        "id": "aYGh1WAzCjOf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example paper parse\n",
        "paper = parses[0]\n",
        "section_texts = get_section_texts(paper)\n",
        "print('Number of sections having extractable datapoints:', len(section_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyWOSSBmTkwo",
        "outputId": "cdff6143-2307-4616-b7c7-cdc5145d14ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sections having extractable datapoints: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_refs(sentence):\n",
        "  refs = [matched[6: -1] for matched in re.findall('BIBREF[0-9]+#', sentence)]\n",
        "  return refs"
      ],
      "metadata": {
        "id": "xn3FdkoGXanW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test\n",
        "\n",
        "sentence = 'Importance of effective contract enforcement BIBREF5# to economic performance BIBREF15# BIBREF21#.'\n",
        "extract_refs(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5tCi2hdXTAq",
        "outputId": "4de2b713-6d43-47cf-e751-8f1f53bc775b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5', '15', '21']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating mappings from sentences to containing BIBREFs\n",
        "\n",
        "def get_sentence_entries(section_texts):\n",
        "  raw_entries = []\n",
        "\n",
        "  for section in section_texts:\n",
        "    sentences = nltk.sent_tokenize(section)\n",
        "    for sentence in sentences:\n",
        "      if 'BIBREF' in sentence:\n",
        "        refs = extract_refs(sentence)\n",
        "        if refs == []:\n",
        "          continue\n",
        "        raw_entries.append(sentence)\n",
        "\n",
        "  return raw_entries\n",
        "\n",
        "raw_entries = get_sentence_entries(section_texts)"
      ],
      "metadata": {
        "id": "2OfzGMCdQJgs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for Context Refinement\n",
        "\n",
        "def tokens_to_str(tokens):\n",
        "  return ' '.join(tokens).strip()\n",
        "\n",
        "def group_refs(sentence):\n",
        "  raw_tokens = nltk.word_tokenize(sentence)\n",
        "  curr_group = -1\n",
        "  in_group = False\n",
        "  proc_toks = []\n",
        "  l = len(raw_tokens)\n",
        "  index = 0\n",
        "  group_to_IDs = defaultdict(list)\n",
        "\n",
        "  while index < l:\n",
        "    token = raw_tokens[index]\n",
        "    if token.startswith('BIBREF'):\n",
        "      ID = int(token[6:])\n",
        "      index += 1\n",
        "      if in_group == False:\n",
        "        curr_group += 1\n",
        "        proc_toks.append('BIBGROUP' + str(curr_group))\n",
        "      else:\n",
        "        # Remove commas and other punctuations between BIBREFs\n",
        "        while len(proc_toks) > 0 and proc_toks[-1].startswith('BIBGROUP') == False:\n",
        "          proc_toks.pop()\n",
        "      in_group = True\n",
        "      group_to_IDs[curr_group].append(ID)\n",
        "    else:\n",
        "      proc_toks.append(token)\n",
        "      if token not in string.punctuation:\n",
        "        in_group = False\n",
        "    index += 1\n",
        "\n",
        "  if in_group:\n",
        "    # Remove Punctuations at the end\n",
        "    while len(proc_toks) > 0 and proc_toks[-1].startswith('BIBGROUP') == False:\n",
        "      proc_toks.pop()\n",
        "\n",
        "  return proc_toks, dict(group_to_IDs)\n",
        "\n",
        "def drop_REF_tags(tokens):\n",
        "  regular_tokens = []\n",
        "  for token in tokens:\n",
        "    if token.text.startswith('BIBGROUP'):\n",
        "      continue\n",
        "    regular_tokens.append(token.text)\n",
        "\n",
        "  return tokens_to_str(regular_tokens)\n",
        "\n",
        "def drop_particular_tags(tokens, drop_tags_set):\n",
        "  processed = []\n",
        "  for token in tokens:\n",
        "    if token in drop_tags_set:\n",
        "      continue\n",
        "    processed.append(token)\n",
        "  return processed\n",
        "\n",
        "valid_prev_deps = set(['compound', 'amod'])\n",
        "\n",
        "def make_fine_mappings(sentence):\n",
        "  global valid_prev_deps\n",
        "  extract = proc(sentence)\n",
        "  mappings = defaultdict(list)\n",
        "  all_groups = []\n",
        "  last_alnum_token = None\n",
        "\n",
        "  for index, node in enumerate(extract):\n",
        "    if node.text.isalnum():\n",
        "      last_alnum_token = node\n",
        "    if node.text.startswith('BIBGROUP'):\n",
        "      group_ID = int(node.text[8:])\n",
        "      all_groups.append(group_ID)\n",
        "      prev_index = index - 1\n",
        "      context_nodes = []\n",
        "      while prev_index > 0:\n",
        "        next_children_set = set([child for child in extract[prev_index + 1].children])\n",
        "        if extract[prev_index].dep_ in valid_prev_deps and extract[prev_index] in next_children_set:\n",
        "          context_nodes.append(extract[prev_index])\n",
        "          prev_index -= 1\n",
        "        else:\n",
        "          break\n",
        "      if context_nodes != []:\n",
        "        mappings[group_ID].append(tokens_to_str([node.text for node in context_nodes][:: -1]))\n",
        "\n",
        "  accounted_IDs = set([key for key in mappings.keys()])\n",
        "  all_groups_accounted = (len(all_groups) == len(accounted_IDs))\n",
        "  has_eos_ref = last_alnum_token is not None and last_alnum_token.text.startswith('BIBGROUP')\n",
        "\n",
        "  if not all_groups_accounted or has_eos_ref:\n",
        "    # Split the sequence of sentence tokens into a sequence of refined contexts\n",
        "\n",
        "    if has_eos_ref or len(all_groups) == 1:\n",
        "      if has_eos_ref:\n",
        "        use_ID = int(last_alnum_token.text[8:])\n",
        "      else:\n",
        "        # If the sentence only has one BIBGROUP\n",
        "        use_ID = all_groups[0]\n",
        "      clean_sent = drop_REF_tags(extract)\n",
        "      mappings[use_ID].append(clean_sent)\n",
        "      accounted_IDs.add(use_ID)\n",
        "\n",
        "  add_context_IDs = set([ID for ID in all_groups if ID not in accounted_IDs])\n",
        "\n",
        "  if len(add_context_IDs) != 0:\n",
        "    tokens = drop_particular_tags([unit.text for unit in extract], accounted_IDs)\n",
        "\n",
        "    running_tokens = []\n",
        "    for token in tokens:\n",
        "      if token.startswith('BIBGROUP'):\n",
        "        ID = int(token[8:])\n",
        "        if ID in add_context_IDs:\n",
        "          if running_tokens != []:\n",
        "            mappings[ID].append(tokens_to_str(running_tokens))\n",
        "            running_tokens = []\n",
        "      else:\n",
        "        running_tokens.append(token)\n",
        "\n",
        "  return dict(mappings)"
      ],
      "metadata": {
        "id": "TnDwjTg12nTR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_paper(paper):\n",
        "  section_texts = get_section_texts(paper)\n",
        "  sentences = get_sentence_entries(section_texts)\n",
        "  paper_ID = paper['paper_id']\n",
        "  datapoints = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    grouped_ref_tokens, group_to_refs = group_refs(sentence)\n",
        "    grouped_ref_sent = tokens_to_str(grouped_ref_tokens)\n",
        "    group_ID_to_contexts = make_fine_mappings(grouped_ref_sent)\n",
        "\n",
        "    for group_ID in group_to_refs.keys():\n",
        "      if group_ID not in group_ID_to_contexts.keys():\n",
        "        continue\n",
        "      contexts = group_ID_to_contexts[group_ID]\n",
        "      for context in contexts:\n",
        "        for ref_ID in group_to_refs[group_ID]:\n",
        "          ref_key = 'BIBREF' + str(ref_ID)\n",
        "          if ref_key not in paper['bib_entries']:\n",
        "            continue\n",
        "          if context and len(context) > 1:\n",
        "            datapoints.append([paper_ID, context, ref_ID, paper['bib_entries'][ref_key]])\n",
        "\n",
        "  return datapoints"
      ],
      "metadata": {
        "id": "98bBcIWaStVr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View an example datapoint\n",
        "created_datapoints = process_paper(paper)\n",
        "created_datapoints[15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtfwKStGUstf",
        "outputId": "52a36a3f-daa2-4962-992c-53be263ed0b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['214802675',\n",
              " 'We use the transformer based encoder - decoder architecture by casting data - totext as a seq2seq problem , where the structured data is flattened into a plain string consisting of a series of intents and slot key - value pairs .',\n",
              " 29,\n",
              " {'title': 'Attention is all you need',\n",
              "  'authors': [{'first': 'Ashish',\n",
              "    'middle': [],\n",
              "    'last': 'Vaswani',\n",
              "    'suffix': ''},\n",
              "   {'first': 'Noam', 'middle': [], 'last': 'Shazeer', 'suffix': ''},\n",
              "   {'first': 'Niki', 'middle': [], 'last': 'Parmar', 'suffix': ''},\n",
              "   {'first': 'Jakob', 'middle': [], 'last': 'Uszkoreit', 'suffix': ''},\n",
              "   {'first': 'Llion', 'middle': [], 'last': 'Jones', 'suffix': ''},\n",
              "   {'first': 'Aidan', 'middle': ['N'], 'last': 'Gomez', 'suffix': ''},\n",
              "   {'first': 'Łukasz', 'middle': [], 'last': 'Kaiser', 'suffix': ''},\n",
              "   {'first': 'Illia', 'middle': [], 'last': 'Polosukhin', 'suffix': ''}],\n",
              "  'year': 2017,\n",
              "  'venue': 'Advances in neural information processing systems',\n",
              "  'link': '13756489'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create context to citation mappings for each domain and split type\n",
        "\n",
        "def create_mappings(domain, split_type):\n",
        "  parses = get_parses_from_file(domain, split_type)\n",
        "  mappings = []\n",
        "  for paper in tqdm.tqdm(parses):\n",
        "    datapoints = process_paper(paper)\n",
        "    mappings += datapoints\n",
        "\n",
        "  dump_mappings(domain, split_type, mappings)\n",
        "  return mappings"
      ],
      "metadata": {
        "id": "weX6mBm4Pj_w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Run on a single domain and split type\n",
        "# mappings = create_mappings('ner', 1)"
      ],
      "metadata": {
        "id": "xDHKmMxw51bh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For domains in domain_codes\n",
        "\n",
        "for domain in domain_codes:\n",
        "  for split_type in range(2):\n",
        "    print('Domain: ' + str(domain) + ', Split: ' + str(split_type), flush = True)\n",
        "    mappings = create_mappings(domain, split_type)\n",
        "    print('Size: ' + str(len(mappings)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7IViSpNUbjs",
        "outputId": "be1b0f11-6f20-4b24-93bb-90d26eaab628"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain: mt, Split: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9322/9322 [21:03<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size: 161698\n",
            "Domain: mt, Split: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size: 8648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That's it"
      ],
      "metadata": {
        "id": "umg7Sm_TVthQ"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}